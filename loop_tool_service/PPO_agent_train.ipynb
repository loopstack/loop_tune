{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - RecSys: Recommender System\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This section explores one approach for using *reinforcement learning* with [Ray RLlib](https://rllib.io/) to build a [*recommender system*](https://en.wikipedia.org/wiki/Recommender_system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the GitHub public repo for this code is available at <https://github.com/anyscale/academy/blob/main/ray-rllib/recsys> and full source code for this example recommender system is also in the `recsys.py` script. You can run that with default settings to exercise the code:\n",
    "\n",
    "```shell\n",
    "python recsys.py\n",
    "```\n",
    "\n",
    "To see the available command line options use:\n",
    "\n",
    "```shell\n",
    "python recsys.py --help\n",
    "```\n",
    "\n",
    "A full run takes about 5-10 minutes on a recent model MacBook Pro laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training Data\n",
    "\n",
    "The approach given here for building a recommender system could be applied to just about any dataset where *users* are *rating* a set of *items*. It's bounded in terms of memory requirements as the number of users grows. It could be re-engineered to handle a very large set of items.\n",
    "\n",
    "We'll be using the [Jester collaborative filtering dataset](https://goldberg.berkeley.edu/jester-data/) – which is known for having a high *density*, i.e., where many users have rated many of the items: https://goldberg.berkeley.edu/jester-data/.\n",
    "\n",
    "Jester was an online joke recommender hosted a UC Berkeley which collected data from April 1999 through May 2003. See the discussion of \"universal queries\" in:\n",
    "\n",
    "> \"Eigentaste: A Constant Time Collaborative Filtering Algorithm\"  \n",
    "Ken Goldberg, Theresa Roeder, Dhruv Gupta, Chris Perkins  \n",
    "*Information Retrieval*, 4(2), 133-151 (July 2001)  \n",
    "<https://goldberg.berkeley.edu/pubs/eigentaste.pdf>\n",
    "\n",
    "The data is split into three downloadable files, and the first file contains anonymous ratings from 24,983 users who have rated 36 or more jokes.\n",
    "\n",
    "Ratings data is organized as a matrix with dimensions `24983 X 101`\n",
    "\n",
    "  * one row per user\n",
    "  * first column gives the number of jokes rated by that user\n",
    "  * the next 100 columns give the ratings for jokes `01` through `100`\n",
    "  * ratings are real values ranging from `-10.00` to `+10.00`\n",
    "  * the value `\"99\"` corresponds to `None` = \"not rated\"\n",
    "  \n",
    "Here's a function to load the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the Gym environment. This is one class and should be defined within one cell – albeit this is a long cell to scroll through…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dejang/loop_tool_env'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install compiler_gym 'ray[default,rllib]' &>/dev/null || echo \"Install failed!\"\n",
    "\n",
    "import compiler_gym\n",
    "import ray\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from compiler_gym.wrappers import ConstrainedCommandline, TimeLimit\n",
    "from ray import tune\n",
    "from itertools import islice\n",
    "from compiler_gym.wrappers import CycleOverBenchmarks\n",
    "from compiler_gym.util.registration import register\n",
    "\n",
    "from loop_tool_service.service_py.datasets import loop_tool_dataset\n",
    "from loop_tool_service.service_py.rewards import flops_loop_nest_reward, flops_reward, runtime_reward\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ.setdefault(\"LOOP_TOOL_ROOT\", \"/home/dejang/loop_tool_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loop_tool_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loop_tool_service\n",
    "\n",
    "def register_env():\n",
    "    register(\n",
    "        id=\"loop_tool_env-v0\",\n",
    "        # entry_point=loop_tool_service.LoopToolCompilerEnv,\n",
    "        entry_point=\"compiler_gym.service.client_service_compiler_env:ClientServiceCompilerEnv\",\n",
    "        kwargs={\n",
    "            \"service\": loop_tool_service.paths.LOOP_TOOL_SERVICE_PY,\n",
    "            \"rewards\": [ flops_loop_nest_reward.RewardTensor(),\n",
    "            ],\n",
    "            \"datasets\": [\n",
    "                loop_tool_dataset.Dataset()\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "\n",
    "# register_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env() -> compiler_gym.envs.CompilerEnv:\n",
    "    \"\"\"Make the reinforcement learning environment for this experiment.\"\"\"\n",
    "    \n",
    "    env = loop_tool_service.make(\n",
    "        \"loop_tool_env-v0\",\n",
    "        observation_space=\"ir_tensor\",\n",
    "        reward_space=\"flops_loop_nest_tensor\",\n",
    "        # reward_space=\"runtime\",\n",
    "    )\n",
    "\n",
    "    env = TimeLimit(env, max_episode_steps=10)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: NamedDiscrete([up, down, swap_up, swap_down])\n",
      "Observation space: Box([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], [[256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256.\n",
      "  256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256.\n",
      "  256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256.\n",
      "  256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256. 256.\n",
      "  256. 256. 256. 256.]], (1, 60), float32)\n",
      "Reward space: flops_loop_nest_tensor\n"
     ]
    }
   ],
   "source": [
    "with make_env() as env:\n",
    "    print(\"Action space:\", env.action_space)\n",
    "    print(\"Observation space:\", env.observation_space)\n",
    "    print(\"Reward space:\", env.reward_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benchmarks for training: 1\n",
      "Number of benchmarks for testing: 1\n"
     ]
    }
   ],
   "source": [
    "with make_env() as env:\n",
    "    # The two datasets we will be using:\n",
    "    lt_dataset = env.datasets[\"loop_tool_simple-v0\"]\n",
    "    # train_benchmarks = list(islice(lt_dataset.benchmarks(), 1))\n",
    "    # test_benchmarks = list(islice(lt_dataset.benchmarks(), 2))\n",
    "    \n",
    "    bench = [\"benchmark://loop_tool_simple-v0/simple\"]\n",
    "            #  \"benchmark://loop_tool_simple-v0/mm128\", \n",
    "            #  \"benchmark://loop_tool_simple-v0/mm\"] \n",
    "\n",
    "    train_benchmarks = bench\n",
    "    test_benchmarks = bench\n",
    "\n",
    "print(\"Number of benchmarks for training:\", len(train_benchmarks))\n",
    "print(\"Number of benchmarks for testing:\", len(test_benchmarks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_env(*args) -> compiler_gym.envs.CompilerEnv:\n",
    "    \"\"\"Make a reinforcement learning environment that cycles over the\n",
    "    set of training benchmarks in use.\n",
    "    \"\"\"\n",
    "    del args  # Unused env_config argument passed by ray\n",
    "    return CycleOverBenchmarks(make_env(), train_benchmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0626 13:08:05.574080 140109369882176 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130804-787558-2c1d\n",
      "\n",
      "E0626 13:08:05.682326 140109369882176 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130804-787558-2c1d\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "benchmark://loop_tool_simple-v0/simple\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "benchmark://loop_tool_simple-v0/simple\n"
     ]
    }
   ],
   "source": [
    "with make_training_env() as env:\n",
    "    env.reset()\n",
    "    print(env.benchmark)\n",
    "    env.reset()\n",
    "    print(env.benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "# ray.init(include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `JokeRec.step()` method contains the heart of the simulation logic here. It determines the item to be recommend based on the input `action`, then determines the `reward` and the agent's updated vector distance to the cluster centers as the `observation space`.\n",
    "\n",
    "Overall, this approach is relatively well-behaved and bounded for its memory use as the number of items grows. We're keeping most of the data in memory, but could readily use a distributed key/value store for selecting items, maintaining user history, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Now we'll use the results of K-means clustering on the data sample to prepare a configuration for our custom environment.\n",
    "\n",
    "We'll use [*proximal policy optimization*](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=ppo#proximal-policy-optimization-ppo) (PPO) for training a policy in RLlib, taking the default PPO configuration as the foundation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an instance of our custom Gym environment and call the `reset()` method, which will initialize an episode (simulating one user's ratings), run a \"warm start\", then return the initial observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0626 13:08:33.464604 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   0.,   0., 128.,   0.,   0.,   0.,   0.,   2., 128.,\n",
       "          0.,   0.,   0.,   0.,   1., 128.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_training_env()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly select an action, i.e., the label of an item cluster to recommend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 3\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(\"action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use that action to take one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action = swap_down\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  1\n",
      "obs: [[  0.   0.   0.   2. 128.   0.   1.   0.   0.   0. 128.   0.   0.   0.\n",
      "    0.   1. 128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.]]\n",
      "reward: -0.03406330943607757\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(action)\n",
    "print(\"obs:\", state)\n",
    "print(\"reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `render()` method to describe more about the agent's state within its environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   2. 128.   0.   1.   0.   0.   0. 128.   0.   0.   0.\n",
      "    0.   1. 128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there are already some other \"used\" items, due to the warm start.\n",
    "\n",
    "Next, we'll define a function that runs through one entire episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode (env, naive=False, verbose=False):\n",
    "    \"\"\"\n",
    "    step through one episode, using either a naive strategy or random actions\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "\n",
    "    action = None\n",
    "    avoid_actions = set([])\n",
    "    depleted = 0\n",
    "\n",
    "    for i in range(5):\n",
    "        if not naive or not action:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        sum_reward += reward\n",
    "\n",
    "     \n",
    "    return sum_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, another function which uses `run_one_episode()` to measure the baseline performance of a \"naïve\" strategy, i.e., without use of reinforcement learning to train a policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def measure_baseline (env, n_iter=1, naive=False, verbose=False):\n",
    "    history = []\n",
    "\n",
    "    for episode in tqdm(range(n_iter), ascii=True, desc=\"measure baseline\"):\n",
    "        sum_reward = run_one_episode(env, naive=naive, verbose=verbose)\n",
    "        history.append(sum_reward)\n",
    "\n",
    "    baseline = sum(history) / len(history)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this to measure how well our recommender system runs without leveraging RLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:   0%|          | 0/10 [00:00<?, ?it/s]E0626 13:14:19.277870 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  10%|#         | 1/10 [00:03<00:27,  3.07s/it]E0626 13:14:22.349595 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  20%|##        | 2/10 [00:06<00:24,  3.08s/it]E0626 13:14:25.434799 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "Action = swap_down\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  1\n",
      "Action = swap_down\n",
      "for n_5625 in 128 : L0  \n",
      " for m_5586 in 128 : L1  <<<<<< cursor (line 1 )\n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  30%|###       | 3/10 [00:09<00:23,  3.29s/it]E0626 13:14:28.979576 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "Action = down\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  1\n",
      "Action = down\n",
      "for m_5586 in 128 : L0  \n",
      " for n_5625 in 128 : L1  <<<<<< cursor (line 1 )\n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  40%|####      | 4/10 [00:13<00:19,  3.31s/it]E0626 13:14:32.310877 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  50%|#####     | 5/10 [00:16<00:16,  3.24s/it]E0626 13:14:35.434668 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  60%|######    | 6/10 [00:19<00:12,  3.19s/it]E0626 13:14:38.536162 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  70%|#######   | 7/10 [00:22<00:09,  3.18s/it]E0626 13:14:41.694870 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "Action = swap_down\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  1\n",
      "Action = swap_down\n",
      "for n_5625 in 128 : L0  \n",
      " for m_5586 in 128 : L1  <<<<<< cursor (line 1 )\n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  80%|########  | 8/10 [00:25<00:06,  3.28s/it]E0626 13:14:45.197010 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "Action = down\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  1\n",
      "Action = down\n",
      "for m_5586 in 128 : L0  \n",
      " for n_5625 in 128 : L1  <<<<<< cursor (line 1 )\n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline:  90%|######### | 9/10 [00:29<00:03,  3.28s/it]E0626 13:14:48.452394 140161385207360 example_service.py:243] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0626T130832-679334-be24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      "Action = down\n",
      "for m_5586 in 128 : L0  <<<<<< cursor (line 0 )\n",
      " for n_5625 in 128 : L1  \n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  1\n",
      "Action = down\n",
      "for m_5586 in 128 : L0  \n",
      " for n_5625 in 128 : L1  <<<<<< cursor (line 1 )\n",
      "  for k_5587 in 128 : L2  \n",
      "   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  \n",
      "   %3[m_5586, n_5625] <- add(%2)  \n",
      "  %4[m_5586, n_5625] <- write(%3)  \n",
      "\n",
      ">>> AGENT ITERATION =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "measure baseline: 100%|##########| 10/10 [00:32<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE CUMULATIVE REWARD -0.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = measure_baseline(env, n_iter=10, naive=True)\n",
    "print(\"BASELINE CUMULATIVE REWARD\", round(baseline, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Ray and RLlib\n",
    "\n",
    "At this point we're ready to train a policy with RLlib. First we'll initialize the directory in which to save *checkpoints*, and the directory in which to log results…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_ROOT = \"tmp/rec\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = \"{}/ray_results/\".format(os.getenv(\"HOME\"))\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start Ray, register our custom environment, and create an agent. BTW, if you see lots of \"deprecation\" warnings from [Tensorflow](https://www.tensorflow.org/) just ignore those…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 13:15:22,614\tINFO services.py:1470 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register our environment so we can reference it by name. Then create a `PPOTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 13:15:40,560\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-06-26 13:15:40,562\tWARNING ppo.py:386 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n",
      "2022-06-26 13:15:40,564\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-26 13:15:40,564\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m 2022-06-26 13:15:45,362\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1382142, ip=100.37.253.28, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a464968b0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 506, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/tmp/ipykernel_1378997/3916046868.py\", line 6, in make_training_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/tmp/ipykernel_1378997/2322847629.py\", line 4, in make_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/loop_tool_service-0.2.3-py3.8.egg/loop_tool_service/__init__.py\", line 252, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     return compiler_gym.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/util/registration.py\", line 16, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     return gym.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 200, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 105, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 75, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 313, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     self.reward_space = reward_space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 479, in reward_space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m     raise LookupError(f\"Reward space not found: {reward_space}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382142)\u001b[0m LookupError: Reward space not found: flops_loop_nest_tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m 2022-06-26 13:15:45,363\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1382143, ip=100.37.253.28, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0a17ac9910>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 506, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/tmp/ipykernel_1378997/3916046868.py\", line 6, in make_training_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/tmp/ipykernel_1378997/2322847629.py\", line 4, in make_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/loop_tool_service-0.2.3-py3.8.egg/loop_tool_service/__init__.py\", line 252, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     return compiler_gym.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/util/registration.py\", line 16, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     return gym.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 200, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 105, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 75, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 313, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     self.reward_space = reward_space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 479, in reward_space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m     raise LookupError(f\"Reward space not found: {reward_space}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382143)\u001b[0m LookupError: Reward space not found: flops_loop_nest_tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m 2022-06-26 13:15:45,433\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1382144, ip=100.37.253.28, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb8ef689910>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 506, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/tmp/ipykernel_1378997/3916046868.py\", line 6, in make_training_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/tmp/ipykernel_1378997/2322847629.py\", line 4, in make_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/loop_tool_service-0.2.3-py3.8.egg/loop_tool_service/__init__.py\", line 252, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     return compiler_gym.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/util/registration.py\", line 16, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     return gym.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 200, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 105, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     env = spec.make(**kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 75, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     env = cls(**_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 313, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     self.reward_space = reward_space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 479, in reward_space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m     raise LookupError(f\"Reward space not found: {reward_space}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1382144)\u001b[0m LookupError: Reward space not found: flops_loop_nest_tensor\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1382142, ip=100.37.253.28, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a464968b0>)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 506, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/tmp/ipykernel_1378997/3916046868.py\", line 6, in make_training_env\n  File \"/tmp/ipykernel_1378997/2322847629.py\", line 4, in make_env\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/loop_tool_service-0.2.3-py3.8.egg/loop_tool_service/__init__.py\", line 252, in make\n    return compiler_gym.make(id, **kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/util/registration.py\", line 16, in make\n    return gym.make(id, **kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 200, in make\n    return registry.make(id, **kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 105, in make\n    env = spec.make(**kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 75, in make\n    env = cls(**_kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 313, in __init__\n    self.reward_space = reward_space\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 479, in reward_space\n    raise LookupError(f\"Reward space not found: {reward_space}\")\nLookupError: Reward space not found: flops_loop_nest_tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=933'>934</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=934'>935</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=935'>936</a>\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=936'>937</a>\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=937'>938</a>\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=940'>941</a>\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=941'>942</a>\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1072'>1073</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1073'>1074</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/dejang/loop_tool_env/loop_tool_service/service_py/01-Recsys.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzi.wtf/home/dejang/loop_tool_env/loop_tool_service/service_py/01-Recsys.ipynb#ch0000032vscode-remote?line=10'>11</a>\u001b[0m env_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompiler_gym\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzi.wtf/home/dejang/loop_tool_env/loop_tool_service/service_py/01-Recsys.ipynb#ch0000032vscode-remote?line=11'>12</a>\u001b[0m tune\u001b[39m.\u001b[39mregister_env(\u001b[39m\"\u001b[39m\u001b[39mcompiler_gym\u001b[39m\u001b[39m\"\u001b[39m, make_training_env)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bzi.wtf/home/dejang/loop_tool_env/loop_tool_service/service_py/01-Recsys.ipynb#ch0000032vscode-remote?line=12'>13</a>\u001b[0m AGENT \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39;49mPPOTrainer(CONFIG, env\u001b[39m=\u001b[39;49menv_key)\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=857'>858</a>\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=858'>859</a>\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=859'>860</a>\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=860'>861</a>\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=861'>862</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=862'>863</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=863'>864</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=866'>867</a>\u001b[0m     }\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=867'>868</a>\u001b[0m }\n\u001b[0;32m--> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=869'>870</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=870'>871</a>\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=871'>872</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/trainable.py?line=153'>154</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/trainable.py?line=154'>155</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/trainable.py?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/trainable.py?line=156'>157</a>\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/tune/trainable.py?line=157'>158</a>\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=935'>936</a>\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=936'>937</a>\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=937'>938</a>\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=940'>941</a>\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=941'>942</a>\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=942'>943</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=943'>944</a>\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=944'>945</a>\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=947'>948</a>\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=948'>949</a>\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=949'>950</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=950'>951</a>\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=951'>952</a>\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=952'>953</a>\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=953'>954</a>\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=954'>955</a>\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=955'>956</a>\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=956'>957</a>\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=957'>958</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=958'>959</a>\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=959'>960</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:142\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=128'>129</a>\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=129'>130</a>\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=130'>131</a>\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=131'>132</a>\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=132'>133</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=133'>134</a>\u001b[0m     local_worker\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=134'>135</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=139'>140</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=140'>141</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=141'>142</a>\u001b[0m     remote_spaces \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=142'>143</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremote_workers()[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mforeach_policy\u001b[39m.\u001b[39;49mremote(\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=143'>144</a>\u001b[0m             \u001b[39mlambda\u001b[39;49;00m p, pid: (pid, p\u001b[39m.\u001b[39;49mobservation_space, p\u001b[39m.\u001b[39;49maction_space)\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=144'>145</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=145'>146</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=146'>147</a>\u001b[0m     spaces \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=147'>148</a>\u001b[0m         e[\u001b[39m0\u001b[39m]: (\u001b[39mgetattr\u001b[39m(e[\u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39moriginal_space\u001b[39m\u001b[39m\"\u001b[39m, e[\u001b[39m1\u001b[39m]), e[\u001b[39m2\u001b[39m])\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=148'>149</a>\u001b[0m         \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m remote_spaces\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=149'>150</a>\u001b[0m     }\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=150'>151</a>\u001b[0m     \u001b[39m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=102'>103</a>\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=103'>104</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=104'>105</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py:1833\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py?line=1830'>1831</a>\u001b[0m             \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py?line=1831'>1832</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py?line=1832'>1833</a>\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py?line=1834'>1835</a>\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   <a href='file:///home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/worker.py?line=1835'>1836</a>\u001b[0m     values \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1382142, ip=100.37.253.28, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a464968b0>)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 506, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/tmp/ipykernel_1378997/3916046868.py\", line 6, in make_training_env\n  File \"/tmp/ipykernel_1378997/2322847629.py\", line 4, in make_env\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/loop_tool_service-0.2.3-py3.8.egg/loop_tool_service/__init__.py\", line 252, in make\n    return compiler_gym.make(id, **kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/util/registration.py\", line 16, in make\n    return gym.make(id, **kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 200, in make\n    return registry.make(id, **kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 105, in make\n    env = spec.make(**kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/gym/envs/registration.py\", line 75, in make\n    env = cls(**_kwargs)\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 313, in __init__\n    self.reward_space = reward_space\n  File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/client_service_compiler_env.py\", line 479, in reward_space\n    raise LookupError(f\"Reward space not found: {reward_space}\")\nLookupError: Reward space not found: flops_loop_nest_tensor"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "CONFIG = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "CONFIG[\"log_level\"] = \"WARN\"\n",
    "CONFIG[\"num_workers\"] = 3    # set to `0` for debug\n",
    "CONFIG[\"horizon\"] = 5\n",
    "\n",
    "\n",
    "env_key = \"compiler_gym\"\n",
    "tune.register_env(\"compiler_gym\", make_training_env)\n",
    "AGENT = ppo.PPOTrainer(CONFIG, env=env_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a policy using the PPO optimizer in RLlib. As the following code steps through each training iteration, watch how the measured improvements in the min, mean, and max rewards per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "TRAIN_ITER = 2\n",
    "\n",
    "df = pd.DataFrame(columns=[ \"min_reward\", \"avg_reward\", \"max_reward\", \"steps\", \"checkpoint\"])\n",
    "status = \"reward {:6.2f} {:6.2f} {:6.2f}  len {:4.2f}  saved {}\"\n",
    "\n",
    "for i in tqdm(range(TRAIN_ITER)):\n",
    "    result = AGENT.train()\n",
    "    breakpoint()\n",
    "    checkpoint_file = AGENT.save(CHECKPOINT_ROOT)\n",
    "\n",
    "    row = [\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],\n",
    "        checkpoint_file,\n",
    "        ]\n",
    "\n",
    "    df.loc[len(df)] = row\n",
    "    print(status.format(*row))\n",
    "    \n",
    "BEST_CHECKPOINT = checkpoint_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout to Emulate a Use Case Deployment\n",
    "\n",
    "Now let's define a function to run a *rollout* using a checkpointed policy.\n",
    "Each rollout iteration will emuluate a deployed use of our recommender system for one user, and we'll measure the average rewards across many iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rollout (agent, env, n_iter=1, verbose=False):\n",
    "    \"\"\"\n",
    "    iterate through `n_iter` episodes in a rollout to emulate deployment in a production use case\n",
    "    \"\"\"\n",
    "    for episode in range(n_iter):\n",
    "        state = env.reset()\n",
    "        sum_reward = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            try:\n",
    "                action = agent.compute_action(state)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sum_reward += reward\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"reward {:6.3f}  sum {:6.3f}\".format(reward, sum_reward))\n",
    "                    env.render()\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "\n",
    "            if done:\n",
    "                # report at the end of each episode\n",
    "                print(\"CUMULATIVE REWARD:\", round(sum_reward, 3), \"\\n\")\n",
    "                yield sum_reward\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the best trained policy in a rollout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AGENT.restore(BEST_CHECKPOINT)\n",
    "history = []\n",
    "\n",
    "for episode_reward in run_rollout(AGENT, env, n_iter=500, verbose=False):\n",
    "    history.append(episode_reward)\n",
    "    \n",
    "print(\"average reward:\", round(sum(history) / len(history), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reported *average reward* from many rollouts (using RLlib to train a policy) compare with the *baseline cumulative reward* above based on a naïve strategy and no learning?  How does it compare with the predicted *mean reward per episode* from training? \n",
    "\n",
    "The baseline reward from a naïve strategy should be much lower (worse user ratings) than the other two measures.\n",
    "\n",
    "These measures are an estimate for how a user would rate their recommended items.\n",
    "Of course, not all users will like the jokes, so there will be some rollouts with negative rewards.\n",
    "Overall we want the average reward to be *positive*, with `MAX_STEPS` as an upper bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the trained policy\n",
    "\n",
    "Use the following code to examine the trained policy that was optimized using PPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = AGENT.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "print(\"\\n\", model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate learning with TensorBoard\n",
    "\n",
    "You also can run [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize the RL training metrics from the log files. The results during training were written to a directory under `$HOME/ray_results`\n",
    "\n",
    "If you are viewing this lesson on the Anyscale hosted platform, use the provided link to open TensorBoard.\n",
    "\n",
    "If you are viewing this lesson on a laptop, open a terminal and run the following command, then open the URL shown in the output. (You can open a terminal using the `+` in the upper left-hand corner of Jupyter Lab.)\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=~/ray_results\n",
    "```\n",
    "\n",
    "Open the URL printed to view the TensorBoard GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Compare use of the other datasets `\"jester-data-2.csv\"` and `\"jester-data-3.csv\"` by substituting them during the rollout.\n",
    "\n",
    "How do the mean cumulative reward differ from the metrics in the lesson?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Compare the effect of using a larger `K` value for the number of clusters.\n",
    "\n",
    "Show the difference, if any, by comparing:\n",
    "\n",
    "  * baseline with random actions \n",
    "  * baseline with the naïve strategy\n",
    "  * predicted average reward from training\n",
    "  * stats from the rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions\n",
    "\n",
    "  1. In what ways could the \"warm start\" be improved?\n",
    "  2. How could this code be modified to scale to millions of users?  Or to thousands of items?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Finally, let's shutdown Ray gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e804d18dc74ce1dc9e76e68b7cf0aefb2bc0afdfbb2c1892ec3bac3a66589459"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('compiler_gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
