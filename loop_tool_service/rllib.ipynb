{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdejang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dejang/loop_tool_env/loop_tool_service/wandb/run-20220705_152453-g7fxgziu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dejang/loop_tool/runs/g7fxgziu\" target=\"_blank\">worthy-gorge-38</a></strong> to <a href=\"https://wandb.ai/dejang/loop_tool\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dejang/loop_tool/runs/g7fxgziu?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7889e4f070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install compiler_gym 'ray[default,rllib]' &>/dev/null || echo \"Install failed!\"\n",
    "\n",
    "import compiler_gym\n",
    "import ray\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from compiler_gym.wrappers import ConstrainedCommandline, TimeLimit\n",
    "from ray import tune\n",
    "from itertools import islice\n",
    "from compiler_gym.wrappers import CycleOverBenchmarks\n",
    "from compiler_gym.util.registration import register\n",
    "\n",
    "import loop_tool_service\n",
    "\n",
    "from service_py.datasets import loop_tool_dataset\n",
    "from service_py.rewards import flops_loop_nest_reward, flops_reward, runtime_reward\n",
    "import wandb\n",
    "wandb.init(project=\"loop_tool\", entity=\"dejang\", sync_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env() -> compiler_gym.envs.CompilerEnv:\n",
    "    \"\"\"Make the reinforcement learning environment for this experiment.\"\"\"\n",
    "    \n",
    "    env = loop_tool_service.make(\n",
    "        \"loop_tool_env-v0\",\n",
    "        observation_space=\"5_prev_actions_tensor\",\n",
    "        reward_space=\"flops_loop_nest_tensor\",\n",
    "    )\n",
    "\n",
    "    env = TimeLimit(env, max_episode_steps=10)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: NamedDiscrete([up, down, swap_up, swap_down])\n",
      "Observation space: Box([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (1, 20), float32)\n",
      "Reward space: flops_loop_nest_tensor\n"
     ]
    }
   ],
   "source": [
    "with make_env() as env:\n",
    "    print(\"Action space:\", env.action_space)\n",
    "    print(\"Observation space:\", env.observation_space)\n",
    "    print(\"Reward space:\", env.reward_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benchmarks for training: 1\n",
      "Number of benchmarks for testing: 1\n"
     ]
    }
   ],
   "source": [
    "with make_env() as env:\n",
    "    # The two datasets we will be using:\n",
    "    lt_dataset = env.datasets[\"loop_tool_simple-v0\"]\n",
    "    # train_benchmarks = list(islice(lt_dataset.benchmarks(), 1))\n",
    "    # test_benchmarks = list(islice(lt_dataset.benchmarks(), 2))\n",
    "    \n",
    "    bench = [\"benchmark://loop_tool_simple-v0/simple\"]\n",
    "            #  \"benchmark://loop_tool_simple-v0/mm128\", \n",
    "            #  \"benchmark://loop_tool_simple-v0/mm\"] \n",
    "\n",
    "    train_benchmarks = bench\n",
    "    test_benchmarks = bench\n",
    "\n",
    "print(\"Number of benchmarks for training:\", len(train_benchmarks))\n",
    "print(\"Number of benchmarks for testing:\", len(test_benchmarks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_env(*args) -> compiler_gym.envs.CompilerEnv:\n",
    "    \"\"\"Make a reinforcement learning environment that cycles over the\n",
    "    set of training benchmarks in use.\n",
    "    \"\"\"\n",
    "    del args  # Unused env_config argument passed by ray\n",
    "    return CycleOverBenchmarks(make_env(), train_benchmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0705 15:25:02.042448 140184553244224 example_service.py:263] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152501-249505-dbfd\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_training_env()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ray import tune\n",
    "from ray.tune import Stopper\n",
    "\n",
    "class TimeStopper(Stopper):\n",
    "    def __init__(self):\n",
    "        self._start = time.time()\n",
    "        self._deadline = 30\n",
    "\n",
    "    def __call__(self, trial_id, result):\n",
    "        return False\n",
    "\n",
    "    def stop_all(self):\n",
    "        return time.time() - self._start > self._deadline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_CONFIG = {\n",
    "    \"log_level\": \"ERROR\",\n",
    "    \"seed\": 0xCC,\n",
    "    \"num_workers\": 2,\n",
    "    # Specify the environment to use, where \"compiler_gym\" is the name we \n",
    "    # passed to tune.register_env().\n",
    "    \"env\": \"compiler_gym\",\n",
    "    # Reduce the size of the batch/trajectory lengths to match our short \n",
    "    # training run.\n",
    "    \"rollout_fragment_length\": 5,\n",
    "    \"train_batch_size\": 5,\n",
    "    \"sgd_minibatch_size\": 5,\n",
    "    \"gamma\": 0.8, #tune.grid_search([0.5, 0.8, 0.9]), # def 0.99\n",
    "    \"lr\": 1e-4, #tune.grid_search([0.01, 0.001, 0.0001]), # def 1e-4\n",
    "    \"horizon\": 3, # def None\n",
    "    \"soft_horizon\": True,\n",
    "    \"evaluation_interval\": 5, # def None\n",
    "    \"evaluation_num_episodes\": 1, # def 10\n",
    "    \"model\": {'fcnet_hiddens': [5, 5]}\n",
    "    # \"model\": {                            # The NN model we'll optimize.\n",
    "    #     'fcnet_hiddens': [                # \"Fully-connected network with N hidden layers\".\n",
    "    #         tune.grid_search([20, 40]),   # Try these four values for layer one.\n",
    "    #         tune.grid_search([20, 40])    # Try these four values for layer two.\n",
    "    #     ]\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04/PPOTrainer_compiler_gym_2c379_00000_0_2022-07-05_15-25-05\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:08,982\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:09,198\tWARNING deprecation.py:46 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:09,199\tWARNING ppo.py:386 -- `train_batch_size` (5) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=5)! Auto-adjusting `rollout_fragment_length` to 2.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:09,199\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:09,199\tINFO trainer.py:903 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m E0705 15:25:14.370913 140252437153344 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152513-338012-0236\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m E0705 15:25:14.370043 140432008803904 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152513-337364-6f35\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m E0705 15:25:17.758754 140695939573312 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152516-717902-659b\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:19 (running for 00:00:14.42)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:19,283\tINFO trainable.py:159 -- Trainable.setup took 10.302 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m 2022-07-05 15:25:19,284\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m E0705 15:25:19.297726 140252437153344 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152513-338012-0236\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560394)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m E0705 15:25:19.297726 140432008803904 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152513-337364-6f35\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 8\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8\n",
      "    num_agent_steps_trained: 8\n",
      "    num_env_steps_sampled: 8\n",
      "    num_env_steps_trained: 8\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-25-23\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.016922388863661375\n",
      "  episode_reward_mean: 0.01018992093293436\n",
      "  episode_reward_min: 0.0034574530022073446\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862930536270142\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.5437602769452496e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.49426084756851196\n",
      "          total_loss: -0.49416399002075195\n",
      "          vf_explained_var: 7.748603536583687e-08\n",
      "          vf_loss: 9.656266047386453e-05\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 8\n",
      "    num_agent_steps_trained: 8\n",
      "    num_env_steps_sampled: 8\n",
      "    num_env_steps_trained: 8\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 8\n",
      "  num_agent_steps_trained: 8\n",
      "  num_env_steps_sampled: 8\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 8\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.057142857142857\n",
      "    ram_util_percent: 39.55714285714286\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1645803451538086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.1726303100586\n",
      "    mean_inference_ms: 5.604004859924316\n",
      "    mean_raw_obs_processing_ms: 1.278090476989746\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.016922388863661375\n",
      "    episode_reward_mean: 0.01018992093293436\n",
      "    episode_reward_min: 0.0034574530022073446\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1645803451538086\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.1726303100586\n",
      "      mean_inference_ms: 5.604004859924316\n",
      "      mean_raw_obs_processing_ms: 1.278090476989746\n",
      "  time_since_restore: 4.42792820930481\n",
      "  time_this_iter_s: 4.42792820930481\n",
      "  time_total_s: 4.42792820930481\n",
      "  timers:\n",
      "    learn_throughput: 20.571\n",
      "    learn_time_ms: 388.894\n",
      "    load_throughput: 41221.661\n",
      "    load_time_ms: 0.194\n",
      "    training_iteration_time_ms: 4424.835\n",
      "    update_time_ms: 2.709\n",
      "  timestamp: 1657049123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8\n",
      "  training_iteration: 1\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:23 (running for 00:00:18.90)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.42793</td><td style=\"text-align: right;\">   8</td><td style=\"text-align: right;\">0.0101899</td><td style=\"text-align: right;\">           0.0169224</td><td style=\"text-align: right;\">          0.00345745</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 24\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24\n",
      "    num_agent_steps_trained: 24\n",
      "    num_env_steps_sampled: 24\n",
      "    num_env_steps_trained: 24\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.928253814293377\n",
      "  episode_reward_mean: 6.420532641255615\n",
      "  episode_reward_min: -5.5448363869475905\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 8\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862723112106323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.346512913703918e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.24581602215766907\n",
      "          total_loss: 2.5894381999969482\n",
      "          vf_explained_var: 0.00013541778025683016\n",
      "          vf_loss: 2.835253953933716\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 24\n",
      "    num_agent_steps_trained: 24\n",
      "    num_env_steps_sampled: 24\n",
      "    num_env_steps_trained: 24\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 24\n",
      "  num_agent_steps_trained: 24\n",
      "  num_env_steps_sampled: 24\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 24\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.15\n",
      "    ram_util_percent: 39.475\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15615810695876423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 701.4995519931499\n",
      "    mean_inference_ms: 3.745943970150418\n",
      "    mean_raw_obs_processing_ms: 1.1372354295518663\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.928253814293377\n",
      "    episode_reward_mean: 6.420532641255615\n",
      "    episode_reward_min: -5.5448363869475905\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15615810695876423\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 701.4995519931499\n",
      "      mean_inference_ms: 3.745943970150418\n",
      "      mean_raw_obs_processing_ms: 1.1372354295518663\n",
      "  time_since_restore: 10.716808080673218\n",
      "  time_this_iter_s: 2.9961183071136475\n",
      "  time_total_s: 10.716808080673218\n",
      "  timers:\n",
      "    learn_throughput: 50.432\n",
      "    learn_time_ms: 158.63\n",
      "    load_throughput: 43900.26\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3568.692\n",
      "    update_time_ms: 2.272\n",
      "  timestamp: 1657049130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24\n",
      "  training_iteration: 3\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:30 (running for 00:00:25.23)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         10.7168</td><td style=\"text-align: right;\">  24</td><td style=\"text-align: right;\"> 6.42053</td><td style=\"text-align: right;\">             29.9283</td><td style=\"text-align: right;\">            -5.54484</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Not logging key \"ray/tune/hist_stats/episode_reward\". Histograms must have fewer than 512 bins\n",
      "wandb: WARNING Not logging key \"ray/tune/sampler_results/hist_stats/episode_reward\". Histograms must have fewer than 512 bins\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Current thread 0x00007fb8dffff640 (most recent call first):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/loop_tool_env/loop_tool_service/service_py/env/loop_tool_env.py\", line 74 in get_available_actions\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"./example_service.py\", line 316 in apply_action\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/runtime/compiler_gym_service.py\", line 201 in Step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/grpc/_server.py\", line 443 in _call_behavior\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/grpc/_server.py\", line 560 in _unary_response_in_pool\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/concurrent/futures/thread.py\", line 57 in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/concurrent/futures/thread.py\", line 80 in _worker\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 870 in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Thread 0x00007fb8e4cba640 (most recent call first):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 306 in wait\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 558 in wait\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/grpc/_common.py\", line 106 in _wait_once\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/grpc/_common.py\", line 141 in wait\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/grpc/_server.py\", line 985 in wait_for_termination\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 870 in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Thread 0x00007fb8e54bb640 (most recent call first):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/grpc/_server.py\", line 879 in _serve\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 870 in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Thread 0x00007fb8f0c6d380 (most recent call first):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 302 in wait\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/threading.py\", line 558 in wait\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/runtime/create_and_run_compiler_gym_service.py\", line 129 in main\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/absl/app.py\", line 258 in _run_main\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/absl/app.py\", line 312 in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"/home/dejang/anaconda3/envs/compiler_gym/lib/python3.8/site-packages/compiler_gym/service/runtime/create_and_run_compiler_gym_service.py\", line 143 in create_and_run_compiler_gym_service\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m   File \"./example_service.py\", line 410 in <module>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m E0705 15:25:34.420949 140430960494144 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152533-389257-77ed\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=560393)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:35 (running for 00:00:30.26)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         10.7168</td><td style=\"text-align: right;\">  24</td><td style=\"text-align: right;\"> 6.42053</td><td style=\"text-align: right;\">             29.9283</td><td style=\"text-align: right;\">            -5.54484</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 32\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32\n",
      "    num_agent_steps_trained: 32\n",
      "    num_env_steps_sampled: 32\n",
      "    num_env_steps_trained: 32\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-25-35\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.928253814293377\n",
      "  episode_reward_mean: 2.2581012389619537\n",
      "  episode_reward_min: -29.963682583498628\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 10\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862091302871704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0727613900817232e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.2542511224746704\n",
      "          total_loss: 5.562228679656982\n",
      "          vf_explained_var: -0.00030838249949738383\n",
      "          vf_loss: 5.30797815322876\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 32\n",
      "    num_agent_steps_trained: 32\n",
      "    num_env_steps_sampled: 32\n",
      "    num_env_steps_trained: 32\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 32\n",
      "  num_agent_steps_trained: 32\n",
      "  num_env_steps_sampled: 32\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 32\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.525\n",
      "    ram_util_percent: 39.412499999999994\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15504576441868875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 705.8962068298822\n",
      "    mean_inference_ms: 3.4915452533298064\n",
      "    mean_raw_obs_processing_ms: 7.8148602192697965\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.928253814293377\n",
      "    episode_reward_mean: 2.2581012389619537\n",
      "    episode_reward_min: -29.963682583498628\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15504576441868875\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 705.8962068298822\n",
      "      mean_inference_ms: 3.4915452533298064\n",
      "      mean_raw_obs_processing_ms: 7.8148602192697965\n",
      "  time_since_restore: 16.310240268707275\n",
      "  time_this_iter_s: 5.593432188034058\n",
      "  time_total_s: 16.310240268707275\n",
      "  timers:\n",
      "    learn_throughput: 61.326\n",
      "    learn_time_ms: 130.45\n",
      "    load_throughput: 47110.47\n",
      "    load_time_ms: 0.17\n",
      "    training_iteration_time_ms: 4073.68\n",
      "    update_time_ms: 2.302\n",
      "  timestamp: 1657049135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32\n",
      "  training_iteration: 4\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m E0705 15:25:38.393560 140695939573312 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152516-717902-659b\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=560340)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:40 (running for 00:00:35.87)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         16.3102</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">  2.2581</td><td style=\"text-align: right;\">             29.9283</td><td style=\"text-align: right;\">            -29.9637</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 40\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40\n",
      "    num_agent_steps_trained: 40\n",
      "    num_env_steps_sampled: 40\n",
      "    num_env_steps_trained: 40\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.928253814293377\n",
      "  episode_reward_mean: 4.32898772247297\n",
      "  episode_reward_min: -29.963682583498628\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 12\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.4590602784843412\n",
      "    episode_reward_mean: -1.4590602784843412\n",
      "    episode_reward_min: -1.4590602784843412\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.4590602784843412\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.18727779388427734\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 698.5506415367126\n",
      "      mean_inference_ms: 10.521352291107178\n",
      "      mean_raw_obs_processing_ms: 0.8798837661743164\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862755298614502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7796033919003094e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.3376007080078125\n",
      "          total_loss: 3.697329521179199\n",
      "          vf_explained_var: -0.0005621671443805099\n",
      "          vf_loss: 4.0349297523498535\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 40\n",
      "    num_agent_steps_trained: 40\n",
      "    num_env_steps_sampled: 40\n",
      "    num_env_steps_trained: 40\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 40\n",
      "  num_agent_steps_trained: 40\n",
      "  num_env_steps_sampled: 40\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 40\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.0625\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15451028675854814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 705.5363765001244\n",
      "    mean_inference_ms: 3.2859316578617803\n",
      "    mean_raw_obs_processing_ms: 11.201507411491377\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.928253814293377\n",
      "    episode_reward_mean: 4.32898772247297\n",
      "    episode_reward_min: -29.963682583498628\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15451028675854814\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 705.5363765001244\n",
      "      mean_inference_ms: 3.2859316578617803\n",
      "      mean_raw_obs_processing_ms: 11.201507411491377\n",
      "  time_since_restore: 21.80244779586792\n",
      "  time_this_iter_s: 5.4922075271606445\n",
      "  time_total_s: 21.80244779586792\n",
      "  timers:\n",
      "    learn_throughput: 70.785\n",
      "    learn_time_ms: 113.018\n",
      "    load_throughput: 44127.344\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3784.328\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1657049141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40\n",
      "  training_iteration: 5\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 56\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 56\n",
      "    num_agent_steps_trained: 56\n",
      "    num_env_steps_sampled: 56\n",
      "    num_env_steps_trained: 56\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.928253814293377\n",
      "  episode_reward_mean: 1.4709236924983569\n",
      "  episode_reward_min: -29.963682583498628\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 18\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031250000465661287\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.386264443397522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.5906318822089816e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.4021817445755005\n",
      "          total_loss: 1.6127469539642334\n",
      "          vf_explained_var: 0.0032045284751802683\n",
      "          vf_loss: 2.0149285793304443\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 56\n",
      "    num_agent_steps_trained: 56\n",
      "    num_env_steps_sampled: 56\n",
      "    num_env_steps_trained: 56\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 56\n",
      "  num_agent_steps_trained: 56\n",
      "  num_env_steps_sampled: 56\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 56\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.780000000000001\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15315023126264135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 702.4362902270389\n",
      "    mean_inference_ms: 2.8667644482056804\n",
      "    mean_raw_obs_processing_ms: 15.066485220003257\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.928253814293377\n",
      "    episode_reward_mean: 1.4709236924983569\n",
      "    episode_reward_min: -29.963682583498628\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15315023126264135\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 702.4362902270389\n",
      "      mean_inference_ms: 2.8667644482056804\n",
      "      mean_raw_obs_processing_ms: 15.066485220003257\n",
      "  time_since_restore: 28.501116514205933\n",
      "  time_this_iter_s: 3.5268352031707764\n",
      "  time_total_s: 28.501116514205933\n",
      "  timers:\n",
      "    learn_throughput: 85.74\n",
      "    learn_time_ms: 93.306\n",
      "    load_throughput: 43829.264\n",
      "    load_time_ms: 0.183\n",
      "    training_iteration_time_ms: 3658.866\n",
      "    update_time_ms: 2.242\n",
      "  timestamp: 1657049148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56\n",
      "  training_iteration: 7\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:48 (running for 00:00:43.20)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         28.5011</td><td style=\"text-align: right;\">  56</td><td style=\"text-align: right;\"> 1.47092</td><td style=\"text-align: right;\">             29.9283</td><td style=\"text-align: right;\">            -29.9637</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 72\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 72\n",
      "    num_agent_steps_trained: 72\n",
      "    num_env_steps_sampled: 72\n",
      "    num_env_steps_trained: 72\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-25-56\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.928253814293377\n",
      "  episode_reward_mean: -0.06113682044809162\n",
      "  episode_reward_min: -29.963682583498628\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 24\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812500116415322\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862438201904297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6466299257444916e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.568730354309082\n",
      "          total_loss: -0.5659816861152649\n",
      "          vf_explained_var: 0.23570893704891205\n",
      "          vf_loss: 0.002748743863776326\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 72\n",
      "    num_agent_steps_trained: 72\n",
      "    num_env_steps_sampled: 72\n",
      "    num_env_steps_trained: 72\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 72\n",
      "  num_agent_steps_trained: 72\n",
      "  num_env_steps_sampled: 72\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 72\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.266666666666666\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15144223758344136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 710.213501296674\n",
      "    mean_inference_ms: 2.5980608130087437\n",
      "    mean_raw_obs_processing_ms: 15.568337565374733\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.928253814293377\n",
      "    episode_reward_mean: -0.06113682044809162\n",
      "    episode_reward_min: -29.963682583498628\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15144223758344136\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 710.213501296674\n",
      "      mean_inference_ms: 2.5980608130087437\n",
      "      mean_raw_obs_processing_ms: 15.568337565374733\n",
      "  time_since_restore: 36.41846036911011\n",
      "  time_this_iter_s: 4.274178743362427\n",
      "  time_total_s: 36.41846036911011\n",
      "  timers:\n",
      "    learn_throughput: 95.564\n",
      "    learn_time_ms: 83.714\n",
      "    load_throughput: 44247.603\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3724.581\n",
      "    update_time_ms: 2.267\n",
      "  timestamp: 1657049156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72\n",
      "  training_iteration: 9\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:25:56 (running for 00:00:51.18)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         36.4185</td><td style=\"text-align: right;\">  72</td><td style=\"text-align: right;\">-0.0611368</td><td style=\"text-align: right;\">             29.9283</td><td style=\"text-align: right;\">            -29.9637</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:01 (running for 00:00:56.21)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         36.4185</td><td style=\"text-align: right;\">  72</td><td style=\"text-align: right;\">-0.0611368</td><td style=\"text-align: right;\">             29.9283</td><td style=\"text-align: right;\">            -29.9637</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 80\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 80\n",
      "    num_agent_steps_trained: 80\n",
      "    num_env_steps_sampled: 80\n",
      "    num_env_steps_trained: 80\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 29.928253814293377\n",
      "  episode_reward_mean: -0.0061669844945650976\n",
      "  episode_reward_min: -29.963682583498628\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 26\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.015589807871945971\n",
      "    episode_reward_mean: -0.015589807871945971\n",
      "    episode_reward_min: -0.015589807871945971\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.015589807871945971\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.16832351684570312\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 715.5761037554059\n",
      "      mean_inference_ms: 6.580795560564313\n",
      "      mean_raw_obs_processing_ms: 0.7790837969098773\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906250058207661\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.386276364326477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.1904173738439567e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.392036110162735\n",
      "          total_loss: 0.0013934220187366009\n",
      "          vf_explained_var: -0.008988519199192524\n",
      "          vf_loss: 0.39342957735061646\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 80\n",
      "    num_agent_steps_trained: 80\n",
      "    num_env_steps_sampled: 80\n",
      "    num_env_steps_trained: 80\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 80\n",
      "  num_agent_steps_trained: 80\n",
      "  num_env_steps_sampled: 80\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 80\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.925\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15091307085838274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 713.2850487445047\n",
      "    mean_inference_ms: 2.528962442727718\n",
      "    mean_raw_obs_processing_ms: 15.521935422563795\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 29.928253814293377\n",
      "    episode_reward_mean: -0.0061669844945650976\n",
      "    episode_reward_min: -29.963682583498628\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15091307085838274\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 713.2850487445047\n",
      "      mean_inference_ms: 2.528962442727718\n",
      "      mean_raw_obs_processing_ms: 15.521935422563795\n",
      "  time_since_restore: 42.38565421104431\n",
      "  time_this_iter_s: 5.967193841934204\n",
      "  time_total_s: 42.38565421104431\n",
      "  timers:\n",
      "    learn_throughput: 99.243\n",
      "    learn_time_ms: 80.611\n",
      "    load_throughput: 43879.21\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3726.106\n",
      "    update_time_ms: 2.27\n",
      "  timestamp: 1657049162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80\n",
      "  training_iteration: 10\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 96\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 96\n",
      "    num_agent_steps_trained: 96\n",
      "    num_env_steps_sampled: 96\n",
      "    num_env_steps_trained: 96\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.25356114399076\n",
      "  episode_reward_mean: 1.0673732870209545\n",
      "  episode_reward_min: -29.963682583498628\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 32\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765625145519152e-05\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862522840499878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.30414046504302e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.11840566247701645\n",
      "          total_loss: 2.70650577545166\n",
      "          vf_explained_var: 0.0002753138542175293\n",
      "          vf_loss: 2.824911594390869\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 96\n",
      "    num_agent_steps_trained: 96\n",
      "    num_env_steps_sampled: 96\n",
      "    num_env_steps_trained: 96\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 96\n",
      "  num_agent_steps_trained: 96\n",
      "  num_env_steps_sampled: 96\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 96\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.2\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14963942983416667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.1611873565885\n",
      "    mean_inference_ms: 2.357974161232365\n",
      "    mean_raw_obs_processing_ms: 15.057406801108183\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.25356114399076\n",
      "    episode_reward_mean: 1.0673732870209545\n",
      "    episode_reward_min: -29.963682583498628\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14963942983416667\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.1611873565885\n",
      "      mean_inference_ms: 2.357974161232365\n",
      "      mean_raw_obs_processing_ms: 15.057406801108183\n",
      "  time_since_restore: 49.81961441040039\n",
      "  time_this_iter_s: 3.7430410385131836\n",
      "  time_total_s: 49.81961441040039\n",
      "  timers:\n",
      "    learn_throughput: 169.878\n",
      "    learn_time_ms: 47.092\n",
      "    load_throughput: 44484.2\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3697.295\n",
      "    update_time_ms: 2.208\n",
      "  timestamp: 1657049169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96\n",
      "  training_iteration: 12\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:09 (running for 00:01:04.69)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         49.8196</td><td style=\"text-align: right;\">  96</td><td style=\"text-align: right;\"> 1.06737</td><td style=\"text-align: right;\">             34.2536</td><td style=\"text-align: right;\">            -29.9637</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 112\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 112\n",
      "    num_agent_steps_trained: 112\n",
      "    num_env_steps_sampled: 112\n",
      "    num_env_steps_trained: 112\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.25356114399076\n",
      "  episode_reward_mean: 0.9050328473338679\n",
      "  episode_reward_min: -34.22635813471207\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 36\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.441406286379788e-05\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3861278295516968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.838016255031107e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.7516098618507385\n",
      "          total_loss: 2.7790138721466064\n",
      "          vf_explained_var: -0.0006596406456083059\n",
      "          vf_loss: 2.0274040699005127\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 112\n",
      "    num_agent_steps_trained: 112\n",
      "    num_env_steps_sampled: 112\n",
      "    num_env_steps_trained: 112\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 112\n",
      "  num_agent_steps_trained: 112\n",
      "  num_env_steps_sampled: 112\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 112\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.500000000000004\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14927155260803698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.2874725885387\n",
      "    mean_inference_ms: 2.270303933083116\n",
      "    mean_raw_obs_processing_ms: 14.657645712350343\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.25356114399076\n",
      "    episode_reward_mean: 0.9050328473338679\n",
      "    episode_reward_min: -34.22635813471207\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14927155260803698\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.2874725885387\n",
      "      mean_inference_ms: 2.270303933083116\n",
      "      mean_raw_obs_processing_ms: 14.657645712350343\n",
      "  time_since_restore: 56.46567249298096\n",
      "  time_this_iter_s: 3.2910547256469727\n",
      "  time_total_s: 56.46567249298096\n",
      "  timers:\n",
      "    learn_throughput: 168.243\n",
      "    learn_time_ms: 47.55\n",
      "    load_throughput: 42356.011\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3502.968\n",
      "    update_time_ms: 2.203\n",
      "  timestamp: 1657049176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112\n",
      "  training_iteration: 14\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:16 (running for 00:01:11.42)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         56.4657</td><td style=\"text-align: right;\"> 112</td><td style=\"text-align: right;\">0.905033</td><td style=\"text-align: right;\">             34.2536</td><td style=\"text-align: right;\">            -34.2264</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:21 (running for 00:01:16.42)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         56.4657</td><td style=\"text-align: right;\"> 112</td><td style=\"text-align: right;\">0.905033</td><td style=\"text-align: right;\">             34.2536</td><td style=\"text-align: right;\">            -34.2264</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 120\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 120\n",
      "    num_agent_steps_trained: 120\n",
      "    num_env_steps_sampled: 120\n",
      "    num_env_steps_trained: 120\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.25356114399076\n",
      "  episode_reward_mean: 0.5700955664036595\n",
      "  episode_reward_min: -34.44558506390648\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 40\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.3917012234340183\n",
      "    episode_reward_mean: 1.3917012234340183\n",
      "    episode_reward_min: 1.3917012234340183\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3917012234340183\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1583576202392578\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 826.2465476989746\n",
      "      mean_inference_ms: 4.9401044845581055\n",
      "      mean_raw_obs_processing_ms: 0.7241010665893555\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.220703143189894e-05\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862175941467285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8185866792919114e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.18455982208251953\n",
      "          total_loss: 2.2344064712524414\n",
      "          vf_explained_var: -0.0007277448894456029\n",
      "          vf_loss: 2.0498464107513428\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 120\n",
      "    num_agent_steps_trained: 120\n",
      "    num_env_steps_sampled: 120\n",
      "    num_env_steps_trained: 120\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 120\n",
      "  num_agent_steps_trained: 120\n",
      "  num_env_steps_sampled: 120\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 120\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.61111111111111\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14887425091788997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 728.9483511385257\n",
      "    mean_inference_ms: 2.1957654766929693\n",
      "    mean_raw_obs_processing_ms: 14.23493936452699\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.25356114399076\n",
      "    episode_reward_mean: 0.5700955664036595\n",
      "    episode_reward_min: -34.44558506390648\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14887425091788997\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 728.9483511385257\n",
      "      mean_inference_ms: 2.1957654766929693\n",
      "      mean_raw_obs_processing_ms: 14.23493936452699\n",
      "  time_since_restore: 62.957218408584595\n",
      "  time_this_iter_s: 6.491545915603638\n",
      "  time_total_s: 62.957218408584595\n",
      "  timers:\n",
      "    learn_throughput: 166.249\n",
      "    learn_time_ms: 48.12\n",
      "    load_throughput: 42864.63\n",
      "    load_time_ms: 0.187\n",
      "    training_iteration_time_ms: 3562.917\n",
      "    update_time_ms: 2.241\n",
      "  timestamp: 1657049182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120\n",
      "  training_iteration: 15\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 136\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 136\n",
      "    num_agent_steps_trained: 136\n",
      "    num_env_steps_sampled: 136\n",
      "    num_env_steps_trained: 136\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-29\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: -0.03300034819693532\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 44\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.051757857974735e-06\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862717151641846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0408109119453002e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.1834874302148819\n",
      "          total_loss: 4.184884548187256\n",
      "          vf_explained_var: -6.510417006211355e-05\n",
      "          vf_loss: 4.001397132873535\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 136\n",
      "    num_agent_steps_trained: 136\n",
      "    num_env_steps_sampled: 136\n",
      "    num_env_steps_trained: 136\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 136\n",
      "  num_agent_steps_trained: 136\n",
      "  num_env_steps_sampled: 136\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 136\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.44\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14841957729018926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 730.7038902720334\n",
      "    mean_inference_ms: 2.1312292569103914\n",
      "    mean_raw_obs_processing_ms: 13.814289545547622\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: -0.03300034819693532\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14841957729018926\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 730.7038902720334\n",
      "      mean_inference_ms: 2.1312292569103914\n",
      "      mean_raw_obs_processing_ms: 13.814289545547622\n",
      "  time_since_restore: 69.75031733512878\n",
      "  time_this_iter_s: 3.6319289207458496\n",
      "  time_total_s: 69.75031733512878\n",
      "  timers:\n",
      "    learn_throughput: 163.48\n",
      "    learn_time_ms: 48.936\n",
      "    load_throughput: 43936.666\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3571.743\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1657049189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136\n",
      "  training_iteration: 17\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:29 (running for 00:01:24.84)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         69.7503</td><td style=\"text-align: right;\"> 136</td><td style=\"text-align: right;\">-0.0330003</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 152\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 152\n",
      "    num_agent_steps_trained: 152\n",
      "    num_env_steps_sampled: 152\n",
      "    num_env_steps_trained: 152\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-37\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.6840631515171718\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 50\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.629394644936838e-07\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.386272668838501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.187044851278188e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.46705392003059387\n",
      "          total_loss: 3.9676740169525146\n",
      "          vf_explained_var: 0.0007580498931929469\n",
      "          vf_loss: 4.434728145599365\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 152\n",
      "    num_agent_steps_trained: 152\n",
      "    num_env_steps_sampled: 152\n",
      "    num_env_steps_trained: 152\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 152\n",
      "  num_agent_steps_trained: 152\n",
      "  num_env_steps_sampled: 152\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 152\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.45\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1480579704080146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.0650059938617\n",
      "    mean_inference_ms: 2.051849234325335\n",
      "    mean_raw_obs_processing_ms: 13.209259331219958\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.6840631515171718\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1480579704080146\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.0650059938617\n",
      "      mean_inference_ms: 2.051849234325335\n",
      "      mean_raw_obs_processing_ms: 13.209259331219958\n",
      "  time_since_restore: 77.34691643714905\n",
      "  time_this_iter_s: 3.773761510848999\n",
      "  time_total_s: 77.34691643714905\n",
      "  timers:\n",
      "    learn_throughput: 164.646\n",
      "    learn_time_ms: 48.589\n",
      "    load_throughput: 41854.1\n",
      "    load_time_ms: 0.191\n",
      "    training_iteration_time_ms: 3539.664\n",
      "    update_time_ms: 2.43\n",
      "  timestamp: 1657049197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152\n",
      "  training_iteration: 19\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:37 (running for 00:01:32.58)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         77.3469</td><td style=\"text-align: right;\"> 152</td><td style=\"text-align: right;\">0.684063</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:42 (running for 00:01:37.58)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         77.3469</td><td style=\"text-align: right;\"> 152</td><td style=\"text-align: right;\">0.684063</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 160\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 160\n",
      "    num_agent_steps_trained: 160\n",
      "    num_env_steps_sampled: 160\n",
      "    num_env_steps_trained: 160\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: -0.03010272064518691\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 52\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.011528375400177815\n",
      "    episode_reward_mean: -0.011528375400177815\n",
      "    episode_reward_min: -0.011528375400177815\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.011528375400177815\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15319310701810396\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 841.3687485914963\n",
      "      mean_inference_ms: 4.0680628556471605\n",
      "      mean_raw_obs_processing_ms: 0.674596199622521\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.814697322468419e-07\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3861702680587769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.9745901226997375e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.3565310835838318\n",
      "          total_loss: 1.0478371381759644\n",
      "          vf_explained_var: 0.04290208965539932\n",
      "          vf_loss: 0.691305935382843\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 160\n",
      "    num_agent_steps_trained: 160\n",
      "    num_env_steps_sampled: 160\n",
      "    num_env_steps_trained: 160\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 160\n",
      "  num_agent_steps_trained: 160\n",
      "  num_env_steps_sampled: 160\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 160\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.433333333333334\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1479795151701329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 735.1587915208444\n",
      "    mean_inference_ms: 2.0287863196276725\n",
      "    mean_raw_obs_processing_ms: 13.013973957361967\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: -0.03010272064518691\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1479795151701329\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 735.1587915208444\n",
      "      mean_inference_ms: 2.0287863196276725\n",
      "      mean_raw_obs_processing_ms: 13.013973957361967\n",
      "  time_since_restore: 83.61721563339233\n",
      "  time_this_iter_s: 6.270299196243286\n",
      "  time_total_s: 83.61721563339233\n",
      "  timers:\n",
      "    learn_throughput: 165.508\n",
      "    learn_time_ms: 48.336\n",
      "    load_throughput: 42908.481\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3523.963\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1657049203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160\n",
      "  training_iteration: 20\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 176\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 176\n",
      "    num_agent_steps_trained: 176\n",
      "    num_env_steps_sampled: 176\n",
      "    num_env_steps_trained: 176\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-50\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.4557252066085174\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 58\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.536743306171047e-08\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862026929855347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3582211067841854e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.27387845516204834\n",
      "          total_loss: -0.26593300700187683\n",
      "          vf_explained_var: -0.9157342910766602\n",
      "          vf_loss: 0.00794544443488121\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 176\n",
      "    num_agent_steps_trained: 176\n",
      "    num_env_steps_sampled: 176\n",
      "    num_env_steps_trained: 176\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 176\n",
      "  num_agent_steps_trained: 176\n",
      "  num_env_steps_sampled: 176\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 176\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.500000000000004\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14768114338137908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.8404616091348\n",
      "    mean_inference_ms: 1.9667465178649488\n",
      "    mean_raw_obs_processing_ms: 12.465101872057568\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.4557252066085174\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14768114338137908\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.8404616091348\n",
      "      mean_inference_ms: 1.9667465178649488\n",
      "      mean_raw_obs_processing_ms: 12.465101872057568\n",
      "  time_since_restore: 90.26654124259949\n",
      "  time_this_iter_s: 3.6052801609039307\n",
      "  time_total_s: 90.26654124259949\n",
      "  timers:\n",
      "    learn_throughput: 165.468\n",
      "    learn_time_ms: 48.348\n",
      "    load_throughput: 42409.545\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3445.441\n",
      "    update_time_ms: 2.461\n",
      "  timestamp: 1657049210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176\n",
      "  training_iteration: 22\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:50 (running for 00:01:45.58)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         90.2665</td><td style=\"text-align: right;\"> 176</td><td style=\"text-align: right;\">0.455725</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 192\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 192\n",
      "    num_agent_steps_trained: 192\n",
      "    num_env_steps_sampled: 192\n",
      "    num_env_steps_trained: 192\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.3420870546187124\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 64\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3841858265427618e-08\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3862097263336182\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.180185204764712e-07\n",
      "          model: {}\n",
      "          policy_loss: 0.2958478033542633\n",
      "          total_loss: 0.2968146502971649\n",
      "          vf_explained_var: 0.21754883229732513\n",
      "          vf_loss: 0.0009668538114055991\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 192\n",
      "    num_agent_steps_trained: 192\n",
      "    num_env_steps_sampled: 192\n",
      "    num_env_steps_trained: 192\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 192\n",
      "  num_agent_steps_trained: 192\n",
      "  num_env_steps_sampled: 192\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 192\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.86\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14727107988044386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 739.811021387485\n",
      "    mean_inference_ms: 1.9132362042812099\n",
      "    mean_raw_obs_processing_ms: 11.958720584335481\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.3420870546187124\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14727107988044386\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 739.811021387485\n",
      "      mean_inference_ms: 1.9132362042812099\n",
      "      mean_raw_obs_processing_ms: 11.958720584335481\n",
      "  time_since_restore: 97.09436774253845\n",
      "  time_this_iter_s: 3.5870413780212402\n",
      "  time_total_s: 97.09436774253845\n",
      "  timers:\n",
      "    learn_throughput: 165.1\n",
      "    learn_time_ms: 48.455\n",
      "    load_throughput: 43001.963\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3463.572\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1657049217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192\n",
      "  training_iteration: 24\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:26:57 (running for 00:01:52.45)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         97.0944</td><td style=\"text-align: right;\"> 192</td><td style=\"text-align: right;\">0.342087</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:02 (running for 00:01:57.48)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         97.0944</td><td style=\"text-align: right;\"> 192</td><td style=\"text-align: right;\">0.342087</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 200\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 200\n",
      "    num_agent_steps_trained: 200\n",
      "    num_env_steps_sampled: 200\n",
      "    num_env_steps_trained: 200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.34204484870071267\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 66\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.3812582286654451\n",
      "    episode_reward_mean: -1.3812582286654451\n",
      "    episode_reward_min: -1.3812582286654451\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3812582286654451\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15090405941009521\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 853.1218767166138\n",
      "      mean_inference_ms: 3.5303086042404175\n",
      "      mean_raw_obs_processing_ms: 0.701218843460083\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1920929132713809e-08\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3861321210861206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.792245143406035e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.1988820880651474\n",
      "          total_loss: 3.8065099716186523\n",
      "          vf_explained_var: 0.00824605394154787\n",
      "          vf_loss: 4.005391597747803\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 200\n",
      "    num_agent_steps_trained: 200\n",
      "    num_env_steps_sampled: 200\n",
      "    num_env_steps_trained: 200\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 200\n",
      "  num_agent_steps_trained: 200\n",
      "  num_env_steps_sampled: 200\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 200\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.45\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14711019078694684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.2009530814784\n",
      "    mean_inference_ms: 1.89689429379731\n",
      "    mean_raw_obs_processing_ms: 11.801196508657272\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.34204484870071267\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14711019078694684\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.2009530814784\n",
      "      mean_inference_ms: 1.89689429379731\n",
      "      mean_raw_obs_processing_ms: 11.801196508657272\n",
      "  time_since_restore: 102.77374386787415\n",
      "  time_this_iter_s: 5.679376125335693\n",
      "  time_total_s: 102.77374386787415\n",
      "  timers:\n",
      "    learn_throughput: 164.526\n",
      "    learn_time_ms: 48.624\n",
      "    load_throughput: 44075.177\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3436.3\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1657049223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 25\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 216\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 216\n",
      "    num_agent_steps_trained: 216\n",
      "    num_env_steps_sampled: 216\n",
      "    num_env_steps_trained: 216\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.3977431286155161\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 72\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9802322831784522e-09\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3861994743347168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0223939120332943e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.03520425781607628\n",
      "          total_loss: 2.037780523300171\n",
      "          vf_explained_var: 0.008770668879151344\n",
      "          vf_loss: 2.0025763511657715\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 216\n",
      "    num_agent_steps_trained: 216\n",
      "    num_env_steps_sampled: 216\n",
      "    num_env_steps_trained: 216\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 216\n",
      "  num_agent_steps_trained: 216\n",
      "  num_env_steps_sampled: 216\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 216\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.959999999999999\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14661401047339814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.6984414770766\n",
      "    mean_inference_ms: 1.8517544500471042\n",
      "    mean_raw_obs_processing_ms: 11.351689290964329\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.3977431286155161\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14661401047339814\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.6984414770766\n",
      "      mean_inference_ms: 1.8517544500471042\n",
      "      mean_raw_obs_processing_ms: 11.351689290964329\n",
      "  time_since_restore: 109.19279837608337\n",
      "  time_this_iter_s: 3.114487409591675\n",
      "  time_total_s: 109.19279837608337\n",
      "  timers:\n",
      "    learn_throughput: 164.768\n",
      "    learn_time_ms: 48.553\n",
      "    load_throughput: 42233.395\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3399.421\n",
      "    update_time_ms: 2.21\n",
      "  timestamp: 1657049229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216\n",
      "  training_iteration: 27\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:09 (running for 00:02:04.72)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         109.193</td><td style=\"text-align: right;\"> 216</td><td style=\"text-align: right;\">0.397743</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 232\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 232\n",
      "    num_agent_steps_trained: 232\n",
      "    num_env_steps_sampled: 232\n",
      "    num_env_steps_trained: 232\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: -0.018300560154286306\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 76\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.450580707946131e-10\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3860942125320435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.276671567524318e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3426475524902344\n",
      "          total_loss: 4.345736980438232\n",
      "          vf_explained_var: -5.410115045378916e-05\n",
      "          vf_loss: 4.003089427947998\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 232\n",
      "    num_agent_steps_trained: 232\n",
      "    num_env_steps_sampled: 232\n",
      "    num_env_steps_trained: 232\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 232\n",
      "  num_agent_steps_trained: 232\n",
      "  num_env_steps_sampled: 232\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 232\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.84\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.146335928250683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.6647553245076\n",
      "    mean_inference_ms: 1.8248798708505682\n",
      "    mean_raw_obs_processing_ms: 11.073774854425597\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: -0.018300560154286306\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.146335928250683\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.6647553245076\n",
      "      mean_inference_ms: 1.8248798708505682\n",
      "      mean_raw_obs_processing_ms: 11.073774854425597\n",
      "  time_since_restore: 115.36781978607178\n",
      "  time_this_iter_s: 3.038029670715332\n",
      "  time_total_s: 115.36781978607178\n",
      "  timers:\n",
      "    learn_throughput: 166.832\n",
      "    learn_time_ms: 47.952\n",
      "    load_throughput: 44005.812\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3257.113\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1657049235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232\n",
      "  training_iteration: 29\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:15 (running for 00:02:10.98)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         115.368</td><td style=\"text-align: right;\"> 232</td><td style=\"text-align: right;\">-0.0183006</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:20 (running for 00:02:15.99)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         115.368</td><td style=\"text-align: right;\"> 232</td><td style=\"text-align: right;\">-0.0183006</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 240\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 240\n",
      "    num_agent_steps_trained: 240\n",
      "    num_env_steps_sampled: 240\n",
      "    num_env_steps_trained: 240\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.39073733317345666\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 80\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.3880605763294587\n",
      "    episode_reward_mean: 1.3880605763294587\n",
      "    episode_reward_min: 1.3880605763294587\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3880605763294587\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.148773193359375\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 833.7044213947496\n",
      "      mean_inference_ms: 3.171192972283614\n",
      "      mean_raw_obs_processing_ms: 0.6940113870721114\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7252903539730653e-10\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3860702514648438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.793665200646501e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5573578476905823\n",
      "          total_loss: 0.7750367522239685\n",
      "          vf_explained_var: 0.03141912445425987\n",
      "          vf_loss: 0.21767891943454742\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 240\n",
      "    num_agent_steps_trained: 240\n",
      "    num_env_steps_sampled: 240\n",
      "    num_env_steps_trained: 240\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 240\n",
      "  num_agent_steps_trained: 240\n",
      "  num_env_steps_sampled: 240\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 240\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.350000000000001\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14614149280851202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.6338302519964\n",
      "    mean_inference_ms: 1.8004276600024547\n",
      "    mean_raw_obs_processing_ms: 10.811235737797782\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.39073733317345666\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14614149280851202\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.6338302519964\n",
      "      mean_inference_ms: 1.8004276600024547\n",
      "      mean_raw_obs_processing_ms: 10.811235737797782\n",
      "  time_since_restore: 120.96299910545349\n",
      "  time_this_iter_s: 5.595179319381714\n",
      "  time_total_s: 120.96299910545349\n",
      "  timers:\n",
      "    learn_throughput: 167.21\n",
      "    learn_time_ms: 47.844\n",
      "    load_throughput: 43856.27\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3238.041\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1657049241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240\n",
      "  training_iteration: 30\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 256\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 256\n",
      "    num_agent_steps_trained: 256\n",
      "    num_env_steps_sampled: 256\n",
      "    num_env_steps_trained: 256\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.7644230636001307\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 84\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.313225884932663e-11\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3861104249954224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.6224777229799656e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.3576037287712097\n",
      "          total_loss: 9.642396926879883\n",
      "          vf_explained_var: 0.00012489159416873008\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 256\n",
      "    num_agent_steps_trained: 256\n",
      "    num_env_steps_sampled: 256\n",
      "    num_env_steps_trained: 256\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 256\n",
      "  num_agent_steps_trained: 256\n",
      "  num_env_steps_sampled: 256\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 256\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.333333333333336\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14596389443287336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.3637115586657\n",
      "    mean_inference_ms: 1.777884799031417\n",
      "    mean_raw_obs_processing_ms: 10.563240597983796\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.7644230636001307\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14596389443287336\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.3637115586657\n",
      "      mean_inference_ms: 1.777884799031417\n",
      "      mean_raw_obs_processing_ms: 10.563240597983796\n",
      "  time_since_restore: 126.97713971138\n",
      "  time_this_iter_s: 2.525343894958496\n",
      "  time_total_s: 126.97713971138\n",
      "  timers:\n",
      "    learn_throughput: 159.2\n",
      "    learn_time_ms: 50.251\n",
      "    load_throughput: 38555.018\n",
      "    load_time_ms: 0.207\n",
      "    training_iteration_time_ms: 3173.944\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1657049247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256\n",
      "  training_iteration: 32\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:27 (running for 00:02:22.75)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         126.977</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.764423</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 272\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 272\n",
      "    num_agent_steps_trained: 272\n",
      "    num_env_steps_sampled: 272\n",
      "    num_env_steps_trained: 272\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-34\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.33746636267994073\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 90\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3283064712331658e-11\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3859444856643677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2294666450761724e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.4746439754962921\n",
      "          total_loss: 3.5254623889923096\n",
      "          vf_explained_var: -0.011364527978003025\n",
      "          vf_loss: 4.000106334686279\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 272\n",
      "    num_agent_steps_trained: 272\n",
      "    num_env_steps_sampled: 272\n",
      "    num_env_steps_trained: 272\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 272\n",
      "  num_agent_steps_trained: 272\n",
      "  num_env_steps_sampled: 272\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 272\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.683333333333332\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14579853673264276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 739.3687613160399\n",
      "    mean_inference_ms: 1.7473075856797353\n",
      "    mean_raw_obs_processing_ms: 10.216279833934017\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.33746636267994073\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14579853673264276\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 739.3687613160399\n",
      "      mean_inference_ms: 1.7473075856797353\n",
      "      mean_raw_obs_processing_ms: 10.216279833934017\n",
      "  time_since_restore: 133.37820100784302\n",
      "  time_this_iter_s: 3.9179887771606445\n",
      "  time_total_s: 133.37820100784302\n",
      "  timers:\n",
      "    learn_throughput: 160.437\n",
      "    learn_time_ms: 49.864\n",
      "    load_throughput: 40480.676\n",
      "    load_time_ms: 0.198\n",
      "    training_iteration_time_ms: 3131.19\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1657049254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272\n",
      "  training_iteration: 34\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:34 (running for 00:02:29.25)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         133.378</td><td style=\"text-align: right;\"> 272</td><td style=\"text-align: right;\">0.337466</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:39 (running for 00:02:34.28)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         133.378</td><td style=\"text-align: right;\"> 272</td><td style=\"text-align: right;\">0.337466</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 280\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 280\n",
      "    num_agent_steps_trained: 280\n",
      "    num_env_steps_sampled: 280\n",
      "    num_env_steps_trained: 280\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-40\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: 0.2805456278363427\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 92\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.388075453575006\n",
      "    episode_reward_mean: -1.388075453575006\n",
      "    episode_reward_min: -1.388075453575006\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.388075453575006\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.15114654194224963\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 869.9924512342973\n",
      "      mean_inference_ms: 2.8997876427390357\n",
      "      mean_raw_obs_processing_ms: 0.6902434609153054\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1641532356165829e-11\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3859981298446655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.787529865599936e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.004737970884889364\n",
      "          total_loss: 1.7012290954589844\n",
      "          vf_explained_var: -0.007560090161859989\n",
      "          vf_loss: 1.6964912414550781\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 280\n",
      "    num_agent_steps_trained: 280\n",
      "    num_env_steps_sampled: 280\n",
      "    num_env_steps_trained: 280\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 280\n",
      "  num_agent_steps_trained: 280\n",
      "  num_env_steps_sampled: 280\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 280\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.944444444444445\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14577998090798708\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 739.056491285137\n",
      "    mean_inference_ms: 1.7380577010667404\n",
      "    mean_raw_obs_processing_ms: 10.106178518032529\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: 0.2805456278363427\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14577998090798708\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 739.056491285137\n",
      "      mean_inference_ms: 1.7380577010667404\n",
      "      mean_raw_obs_processing_ms: 10.106178518032529\n",
      "  time_since_restore: 140.01642417907715\n",
      "  time_this_iter_s: 6.638223171234131\n",
      "  time_total_s: 140.01642417907715\n",
      "  timers:\n",
      "    learn_throughput: 160.864\n",
      "    learn_time_ms: 49.731\n",
      "    load_throughput: 39373.893\n",
      "    load_time_ms: 0.203\n",
      "    training_iteration_time_ms: 3168.336\n",
      "    update_time_ms: 2.415\n",
      "  timestamp: 1657049260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280\n",
      "  training_iteration: 35\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 296\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 296\n",
      "    num_agent_steps_trained: 296\n",
      "    num_env_steps_sampled: 296\n",
      "    num_env_steps_trained: 296\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.490288723481925\n",
      "  episode_reward_mean: -0.014869449253587658\n",
      "  episode_reward_min: -34.489986266107266\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 98\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9103830890414573e-12\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.38593327999115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.316212433266628e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.45593899488449097\n",
      "          total_loss: 4.802432060241699\n",
      "          vf_explained_var: -0.0003170967102050781\n",
      "          vf_loss: 4.346494197845459\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 296\n",
      "    num_agent_steps_trained: 296\n",
      "    num_env_steps_sampled: 296\n",
      "    num_env_steps_trained: 296\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 296\n",
      "  num_agent_steps_trained: 296\n",
      "  num_env_steps_sampled: 296\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 296\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.41666666666667\n",
      "    ram_util_percent: 39.483333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14572517370285565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.0906436593602\n",
      "    mean_inference_ms: 1.7122702476992964\n",
      "    mean_raw_obs_processing_ms: 9.79441302193741\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.490288723481925\n",
      "    episode_reward_mean: -0.014869449253587658\n",
      "    episode_reward_min: -34.489986266107266\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016922388863661375\n",
      "      - 0.0034574530022073446\n",
      "      - 0.02113257580721095\n",
      "      - 28.70269990020786\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14572517370285565\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.0906436593602\n",
      "      mean_inference_ms: 1.7122702476992964\n",
      "      mean_raw_obs_processing_ms: 9.79441302193741\n",
      "  time_since_restore: 147.13504076004028\n",
      "  time_this_iter_s: 3.8558156490325928\n",
      "  time_total_s: 147.13504076004028\n",
      "  timers:\n",
      "    learn_throughput: 162.609\n",
      "    learn_time_ms: 49.198\n",
      "    load_throughput: 42265.313\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3238.365\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1657049267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296\n",
      "  training_iteration: 37\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:48 (running for 00:02:43.15)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         147.135</td><td style=\"text-align: right;\"> 296</td><td style=\"text-align: right;\">-0.0148694</td><td style=\"text-align: right;\">             34.4903</td><td style=\"text-align: right;\">              -34.49</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 312\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 312\n",
      "    num_agent_steps_trained: 312\n",
      "    num_env_steps_sampled: 312\n",
      "    num_env_steps_trained: 312\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-27-55\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: -0.30161280756486086\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 104\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.275957722603643e-13\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385935664176941\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.6449085908097913e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5318413972854614\n",
      "          total_loss: 1.4687069654464722\n",
      "          vf_explained_var: 0.00035844644298776984\n",
      "          vf_loss: 2.0005478858947754\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 312\n",
      "    num_agent_steps_trained: 312\n",
      "    num_env_steps_sampled: 312\n",
      "    num_env_steps_trained: 312\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 312\n",
      "  num_agent_steps_trained: 312\n",
      "  num_env_steps_sampled: 312\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 312\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.933333333333332\n",
      "    ram_util_percent: 39.41666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1451224544679763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.9536886601741\n",
      "    mean_inference_ms: 1.5715230529622772\n",
      "    mean_raw_obs_processing_ms: 9.83833043833542\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: -0.30161280756486086\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0026261954656212616\n",
      "      - 29.928253814293377\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1451224544679763\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.9536886601741\n",
      "      mean_inference_ms: 1.5715230529622772\n",
      "      mean_raw_obs_processing_ms: 9.83833043833542\n",
      "  time_since_restore: 154.74520444869995\n",
      "  time_this_iter_s: 3.9331908226013184\n",
      "  time_total_s: 154.74520444869995\n",
      "  timers:\n",
      "    learn_throughput: 160.752\n",
      "    learn_time_ms: 49.766\n",
      "    load_throughput: 42979.931\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3381.865\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1657049275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312\n",
      "  training_iteration: 39\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:27:55 (running for 00:02:50.85)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         154.745</td><td style=\"text-align: right;\"> 312</td><td style=\"text-align: right;\">-0.301613</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:00 (running for 00:02:55.85)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         154.745</td><td style=\"text-align: right;\"> 312</td><td style=\"text-align: right;\">-0.301613</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 320\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 320\n",
      "    num_agent_steps_trained: 320\n",
      "    num_env_steps_sampled: 320\n",
      "    num_env_steps_trained: 320\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: -0.26363971903339284\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 106\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.00011138290769419079\n",
      "    episode_reward_mean: 0.00011138290769419079\n",
      "    episode_reward_min: 0.00011138290769419079\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.00011138290769419079\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14827728271484375\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 854.8087882995605\n",
      "      mean_inference_ms: 2.701730728149414\n",
      "      mean_raw_obs_processing_ms: 0.6849002838134766\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.6379788613018216e-13\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3858797550201416\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1132459576401743e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.29412975907325745\n",
      "          total_loss: 0.2960060238838196\n",
      "          vf_explained_var: 0.17187660932540894\n",
      "          vf_loss: 0.0018762556137517095\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 320\n",
      "    num_agent_steps_trained: 320\n",
      "    num_env_steps_sampled: 320\n",
      "    num_env_steps_trained: 320\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 320\n",
      "  num_agent_steps_trained: 320\n",
      "  num_env_steps_sampled: 320\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 320\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.037500000000001\n",
      "    ram_util_percent: 39.4375\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.144813335040664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.1013916345654\n",
      "    mean_inference_ms: 1.5392567149984568\n",
      "    mean_raw_obs_processing_ms: 9.909932979233277\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: -0.26363971903339284\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.7607424197161876\n",
      "      - -5.5448363869475905\n",
      "      - -29.963682583498628\n",
      "      - 1.1804338430732422\n",
      "      - 29.130934000743203\n",
      "      - 0.23590627931289987\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.144813335040664\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.1013916345654\n",
      "      mean_inference_ms: 1.5392567149984568\n",
      "      mean_raw_obs_processing_ms: 9.909932979233277\n",
      "  time_since_restore: 160.55968832969666\n",
      "  time_this_iter_s: 5.814483880996704\n",
      "  time_total_s: 160.55968832969666\n",
      "  timers:\n",
      "    learn_throughput: 160.147\n",
      "    learn_time_ms: 49.954\n",
      "    load_throughput: 41553.476\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3399.484\n",
      "    update_time_ms: 2.562\n",
      "  timestamp: 1657049281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320\n",
      "  training_iteration: 40\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 336\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 336\n",
      "    num_agent_steps_trained: 336\n",
      "    num_env_steps_sampled: 336\n",
      "    num_env_steps_trained: 336\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: -0.534409008256883\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 112\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.094947153254554e-14\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3858736753463745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.972227437363472e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.21308118104934692\n",
      "          total_loss: 0.5765048265457153\n",
      "          vf_explained_var: -0.0027459741104394197\n",
      "          vf_loss: 0.3634236752986908\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 336\n",
      "    num_agent_steps_trained: 336\n",
      "    num_env_steps_sampled: 336\n",
      "    num_env_steps_trained: 336\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 336\n",
      "  num_agent_steps_trained: 336\n",
      "  num_env_steps_sampled: 336\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 336\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.166666666666664\n",
      "    ram_util_percent: 39.483333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14443606241556556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.5211680000198\n",
      "    mean_inference_ms: 1.465807142322938\n",
      "    mean_raw_obs_processing_ms: 8.905918010914444\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: -0.534409008256883\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -29.126614583118595\n",
      "      - -0.07756459163930574\n",
      "      - 0.21946060817301571\n",
      "      - -1.9987212854384104\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14443606241556556\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.5211680000198\n",
      "      mean_inference_ms: 1.465807142322938\n",
      "      mean_raw_obs_processing_ms: 8.905918010914444\n",
      "  time_since_restore: 168.10139989852905\n",
      "  time_this_iter_s: 4.237908840179443\n",
      "  time_total_s: 168.10139989852905\n",
      "  timers:\n",
      "    learn_throughput: 169.076\n",
      "    learn_time_ms: 47.316\n",
      "    load_throughput: 45908.376\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3552.381\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1657049289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336\n",
      "  training_iteration: 42\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:09 (running for 00:03:04.36)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         168.101</td><td style=\"text-align: right;\"> 336</td><td style=\"text-align: right;\">-0.534409</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 352\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 352\n",
      "    num_agent_steps_trained: 352\n",
      "    num_env_steps_sampled: 352\n",
      "    num_env_steps_trained: 352\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-17\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: -0.21125787459346784\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 116\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2737367883136385e-14\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3861244916915894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5635310976213077e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.10793797671794891\n",
      "          total_loss: -0.10580019652843475\n",
      "          vf_explained_var: -0.0940871611237526\n",
      "          vf_loss: 0.0021377645898610353\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 352\n",
      "    num_agent_steps_trained: 352\n",
      "    num_env_steps_sampled: 352\n",
      "    num_env_steps_trained: 352\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 352\n",
      "  num_agent_steps_trained: 352\n",
      "  num_env_steps_sampled: 352\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 352\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.7\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14416732387301207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 742.1757597264927\n",
      "    mean_inference_ms: 1.4346565890146348\n",
      "    mean_raw_obs_processing_ms: 8.127842429080461\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: -0.21125787459346784\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.04783815721452944\n",
      "      - 5.464375490103556\n",
      "      - -0.00390525207960124\n",
      "      - -27.965183809855084\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14416732387301207\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 742.1757597264927\n",
      "      mean_inference_ms: 1.4346565890146348\n",
      "      mean_raw_obs_processing_ms: 8.127842429080461\n",
      "  time_since_restore: 176.3471405506134\n",
      "  time_this_iter_s: 4.384092092514038\n",
      "  time_total_s: 176.3471405506134\n",
      "  timers:\n",
      "    learn_throughput: 166.812\n",
      "    learn_time_ms: 47.958\n",
      "    load_throughput: 40588.402\n",
      "    load_time_ms: 0.197\n",
      "    training_iteration_time_ms: 3736.848\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1657049297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352\n",
      "  training_iteration: 44\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:17 (running for 00:03:12.72)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         176.347</td><td style=\"text-align: right;\"> 352</td><td style=\"text-align: right;\">-0.211258</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:22 (running for 00:03:17.72)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         176.347</td><td style=\"text-align: right;\"> 352</td><td style=\"text-align: right;\">-0.211258</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 360\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 360\n",
      "    num_agent_steps_trained: 360\n",
      "    num_env_steps_sampled: 360\n",
      "    num_env_steps_trained: 360\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: 0.014454484847235922\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 120\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.006045475704037906\n",
      "    episode_reward_mean: 0.006045475704037906\n",
      "    episode_reward_min: 0.006045475704037906\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.006045475704037906\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.146763665335519\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 879.7915663037982\n",
      "      mean_inference_ms: 2.5385277611868724\n",
      "      mean_raw_obs_processing_ms: 0.6845508302961077\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1368683941568192e-14\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3860503435134888\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0999022379110102e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.30773600935935974\n",
      "          total_loss: 0.31378716230392456\n",
      "          vf_explained_var: -0.12201846390962601\n",
      "          vf_loss: 0.006051181349903345\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 360\n",
      "    num_agent_steps_trained: 360\n",
      "    num_env_steps_sampled: 360\n",
      "    num_env_steps_trained: 360\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 360\n",
      "  num_agent_steps_trained: 360\n",
      "  num_env_steps_sampled: 360\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 360\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.430000000000001\n",
      "    ram_util_percent: 39.47\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1440856760426901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 743.5877197967773\n",
      "    mean_inference_ms: 1.4110799532299696\n",
      "    mean_raw_obs_processing_ms: 7.518424812842343\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: 0.014454484847235922\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0005151563862972885\n",
      "      - 0.03600588252643844\n",
      "      - 1.379402618140553\n",
      "      - -1.390744750843227\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1440856760426901\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 743.5877197967773\n",
      "      mean_inference_ms: 1.4110799532299696\n",
      "      mean_raw_obs_processing_ms: 7.518424812842343\n",
      "  time_since_restore: 183.24706149101257\n",
      "  time_this_iter_s: 6.89992094039917\n",
      "  time_total_s: 183.24706149101257\n",
      "  timers:\n",
      "    learn_throughput: 167.033\n",
      "    learn_time_ms: 47.895\n",
      "    load_throughput: 41755.142\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 3766.604\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1657049304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360\n",
      "  training_iteration: 45\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:27 (running for 00:03:22.86)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         186.375</td><td style=\"text-align: right;\"> 368</td><td style=\"text-align: right;\">0.246841</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 376\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 376\n",
      "    num_agent_steps_trained: 376\n",
      "    num_env_steps_sampled: 376\n",
      "    num_env_steps_trained: 376\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-31\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: 0.279763930005682\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 124\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.842170985392048e-15\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3860105276107788\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.47606453235494e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.43855544924736023\n",
      "          total_loss: 4.439111232757568\n",
      "          vf_explained_var: -0.0007113814353942871\n",
      "          vf_loss: 4.000555515289307\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 376\n",
      "    num_agent_steps_trained: 376\n",
      "    num_env_steps_sampled: 376\n",
      "    num_env_steps_trained: 376\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 376\n",
      "  num_agent_steps_trained: 376\n",
      "  num_env_steps_sampled: 376\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 376\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.850000000000001\n",
      "    ram_util_percent: 39.46666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14402750613703177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 743.4630015820353\n",
      "    mean_inference_ms: 1.391785747347954\n",
      "    mean_raw_obs_processing_ms: 7.0271951122467975\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: 0.279763930005682\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.09606016098359227\n",
      "      - 1.4030022548790986\n",
      "      - 0.013915415044734036\n",
      "      - -0.024917754666857306\n",
      "      - 0.04297921480132638\n",
      "      - 34.25356114399076\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14402750613703177\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 743.4630015820353\n",
      "      mean_inference_ms: 1.391785747347954\n",
      "      mean_raw_obs_processing_ms: 7.0271951122467975\n",
      "  time_since_restore: 190.34795141220093\n",
      "  time_this_iter_s: 3.972536087036133\n",
      "  time_total_s: 190.34795141220093\n",
      "  timers:\n",
      "    learn_throughput: 163.038\n",
      "    learn_time_ms: 49.068\n",
      "    load_throughput: 39030.397\n",
      "    load_time_ms: 0.205\n",
      "    training_iteration_time_ms: 3764.681\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1657049311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376\n",
      "  training_iteration: 47\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:34 (running for 00:03:30.05)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">          193.52</td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">0.334771</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 392\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 392\n",
      "    num_agent_steps_trained: 392\n",
      "    num_env_steps_sampled: 392\n",
      "    num_env_steps_trained: 392\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-38\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: -0.07416442378132787\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 130\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.10542746348012e-16\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3858932256698608\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.741040685184998e-07\n",
      "          model: {}\n",
      "          policy_loss: 0.2999935746192932\n",
      "          total_loss: 4.300436019897461\n",
      "          vf_explained_var: -0.0016021688934415579\n",
      "          vf_loss: 4.000442028045654\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 392\n",
      "    num_agent_steps_trained: 392\n",
      "    num_env_steps_sampled: 392\n",
      "    num_env_steps_trained: 392\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 392\n",
      "  num_agent_steps_trained: 392\n",
      "  num_env_steps_sampled: 392\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 392\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.459999999999999\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1439887421359491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 741.2277499146093\n",
      "    mean_inference_ms: 1.3689563581797837\n",
      "    mean_raw_obs_processing_ms: 6.211826983394422\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: -0.07416442378132787\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.008985049021333236\n",
      "      - 0.03973381138060583\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1439887421359491\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 741.2277499146093\n",
      "      mean_inference_ms: 1.3689563581797837\n",
      "      mean_raw_obs_processing_ms: 6.211826983394422\n",
      "  time_since_restore: 197.3366687297821\n",
      "  time_this_iter_s: 3.8162927627563477\n",
      "  time_total_s: 197.3366687297821\n",
      "  timers:\n",
      "    learn_throughput: 161.764\n",
      "    learn_time_ms: 49.455\n",
      "    load_throughput: 37031.71\n",
      "    load_time_ms: 0.216\n",
      "    training_iteration_time_ms: 3702.582\n",
      "    update_time_ms: 2.515\n",
      "  timestamp: 1657049318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392\n",
      "  training_iteration: 49\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:43 (running for 00:03:38.94)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         197.337</td><td style=\"text-align: right;\"> 392</td><td style=\"text-align: right;\">-0.0741644</td><td style=\"text-align: right;\">             35.1169</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 400\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 400\n",
      "    num_agent_steps_trained: 400\n",
      "    num_env_steps_sampled: 400\n",
      "    num_env_steps_trained: 400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-44\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.11691174532054\n",
      "  episode_reward_mean: -0.008053343287597325\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 132\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.08074798660062621\n",
      "    episode_reward_mean: 0.08074798660062621\n",
      "    episode_reward_min: 0.08074798660062621\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.08074798660062621\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1464966804750504\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 891.6965300037015\n",
      "      mean_inference_ms: 2.4095581423851753\n",
      "      mean_raw_obs_processing_ms: 0.682938483453566\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.55271373174006e-16\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3857163190841675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.656191206071526e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5532369613647461\n",
      "          total_loss: 5.44878625869751\n",
      "          vf_explained_var: 0.000563933455850929\n",
      "          vf_loss: 6.002022743225098\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 400\n",
      "    num_agent_steps_trained: 400\n",
      "    num_env_steps_sampled: 400\n",
      "    num_env_steps_trained: 400\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 400\n",
      "  num_agent_steps_trained: 400\n",
      "  num_env_steps_sampled: 400\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 400\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.225\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14405979784424955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 741.6599359563504\n",
      "    mean_inference_ms: 1.3632281413336165\n",
      "    mean_raw_obs_processing_ms: 6.268252216319938\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.11691174532054\n",
      "    episode_reward_mean: -0.008053343287597325\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -34.22635813471207\n",
      "      - -1.3968892587378938\n",
      "      - 33.96422918147703\n",
      "      - 0.0842555313216361\n",
      "      - 0.48338983381292877\n",
      "      - -34.44558506390648\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14405979784424955\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 741.6599359563504\n",
      "      mean_inference_ms: 1.3632281413336165\n",
      "      mean_raw_obs_processing_ms: 6.268252216319938\n",
      "  time_since_restore: 202.9594542980194\n",
      "  time_this_iter_s: 5.622785568237305\n",
      "  time_total_s: 202.9594542980194\n",
      "  timers:\n",
      "    learn_throughput: 162.762\n",
      "    learn_time_ms: 49.152\n",
      "    load_throughput: 36165.587\n",
      "    load_time_ms: 0.221\n",
      "    training_iteration_time_ms: 3605.86\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1657049324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400\n",
      "  training_iteration: 50\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 416\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 416\n",
      "    num_agent_steps_trained: 416\n",
      "    num_env_steps_sampled: 416\n",
      "    num_env_steps_trained: 416\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.7943673803390766\n",
      "  episode_reward_min: -35.118821415117345\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 138\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.88178432935015e-17\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385759711265564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.992830104631139e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.4229654371738434\n",
      "          total_loss: 5.636115074157715\n",
      "          vf_explained_var: 0.00013722380390390754\n",
      "          vf_loss: 6.059079647064209\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 416\n",
      "    num_agent_steps_trained: 416\n",
      "    num_env_steps_sampled: 416\n",
      "    num_env_steps_trained: 416\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 416\n",
      "  num_agent_steps_trained: 416\n",
      "  num_env_steps_sampled: 416\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 416\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.866666666666664\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14388510927446208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 739.22508897817\n",
      "    mean_inference_ms: 1.346030350288982\n",
      "    mean_raw_obs_processing_ms: 5.648765103012974\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.7943673803390766\n",
      "    episode_reward_min: -35.118821415117345\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 28.128012480546857\n",
      "      - -3.9431770983261742\n",
      "      - 34.490288723481925\n",
      "      - 3.8188990932552542\n",
      "      - -34.489986266107266\n",
      "      - -28.075039527441447\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14388510927446208\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 739.22508897817\n",
      "      mean_inference_ms: 1.346030350288982\n",
      "      mean_raw_obs_processing_ms: 5.648765103012974\n",
      "  time_since_restore: 207.87408018112183\n",
      "  time_this_iter_s: 2.4390130043029785\n",
      "  time_total_s: 207.87408018112183\n",
      "  timers:\n",
      "    learn_throughput: 162.199\n",
      "    learn_time_ms: 49.322\n",
      "    load_throughput: 36812.323\n",
      "    load_time_ms: 0.217\n",
      "    training_iteration_time_ms: 3343.066\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1657049329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416\n",
      "  training_iteration: 52\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:49 (running for 00:03:44.67)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         207.874</td><td style=\"text-align: right;\"> 416</td><td style=\"text-align: right;\">0.794367</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -35.1188</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 432\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 432\n",
      "    num_agent_steps_trained: 432\n",
      "    num_env_steps_sampled: 432\n",
      "    num_env_steps_trained: 432\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-28-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.2904721791812971\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 144\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2204460823375376e-17\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854669332504272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.511838596954476e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.18849046528339386\n",
      "          total_loss: 3.8125030994415283\n",
      "          vf_explained_var: -0.0013168136356398463\n",
      "          vf_loss: 4.000993728637695\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 432\n",
      "    num_agent_steps_trained: 432\n",
      "    num_env_steps_sampled: 432\n",
      "    num_env_steps_trained: 432\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 432\n",
      "  num_agent_steps_trained: 432\n",
      "  num_env_steps_sampled: 432\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 432\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.08333333333333\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14401004952451488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.710918793986\n",
      "    mean_inference_ms: 1.3335441787967743\n",
      "    mean_raw_obs_processing_ms: 5.468215418874729\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.2904721791812971\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.06687246759790133\n",
      "      - 0.031406681965215455\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14401004952451488\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.710918793986\n",
      "      mean_inference_ms: 1.3335441787967743\n",
      "      mean_raw_obs_processing_ms: 5.468215418874729\n",
      "  time_since_restore: 215.8039689064026\n",
      "  time_this_iter_s: 4.2758948802948\n",
      "  time_total_s: 215.8039689064026\n",
      "  timers:\n",
      "    learn_throughput: 163.951\n",
      "    learn_time_ms: 48.795\n",
      "    load_throughput: 38047.888\n",
      "    load_time_ms: 0.21\n",
      "    training_iteration_time_ms: 3311.453\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1657049337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432\n",
      "  training_iteration: 54\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:28:57 (running for 00:03:52.63)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         215.804</td><td style=\"text-align: right;\"> 432</td><td style=\"text-align: right;\">0.290472</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:02 (running for 00:03:57.66)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         215.804</td><td style=\"text-align: right;\"> 432</td><td style=\"text-align: right;\">0.290472</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 440\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 440\n",
      "    num_agent_steps_trained: 440\n",
      "    num_env_steps_sampled: 440\n",
      "    num_env_steps_trained: 440\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-03\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.014896529851941658\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 146\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 28.88762923253877\n",
      "    episode_reward_mean: 28.88762923253877\n",
      "    episode_reward_min: 28.88762923253877\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 28.88762923253877\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1465012045467601\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 875.8323332842658\n",
      "      mean_inference_ms: 2.310058649848489\n",
      "      mean_raw_obs_processing_ms: 0.6832164876601275\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1102230411687688e-17\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3856257200241089\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.7159153331886046e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3368909955024719\n",
      "          total_loss: 4.3369975090026855\n",
      "          vf_explained_var: 0.0010886689415201545\n",
      "          vf_loss: 4.000106334686279\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 440\n",
      "    num_agent_steps_trained: 440\n",
      "    num_env_steps_sampled: 440\n",
      "    num_env_steps_trained: 440\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 440\n",
      "  num_agent_steps_trained: 440\n",
      "  num_env_steps_sampled: 440\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 440\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.244444444444444\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14390943534718914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.5176405529186\n",
      "    mean_inference_ms: 1.328873357699288\n",
      "    mean_raw_obs_processing_ms: 5.209033383502218\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.014896529851941658\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03065100982679647\n",
      "      - 0.006176508065745767\n",
      "      - 34.25948751158662\n",
      "      - 1.4556256723308638\n",
      "      - -34.307743415929494\n",
      "      - -1.4607556334788157\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14390943534718914\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.5176405529186\n",
      "      mean_inference_ms: 1.328873357699288\n",
      "      mean_raw_obs_processing_ms: 5.209033383502218\n",
      "  time_since_restore: 222.1110394001007\n",
      "  time_this_iter_s: 6.30707049369812\n",
      "  time_total_s: 222.1110394001007\n",
      "  timers:\n",
      "    learn_throughput: 164.87\n",
      "    learn_time_ms: 48.523\n",
      "    load_throughput: 34020.513\n",
      "    load_time_ms: 0.235\n",
      "    training_iteration_time_ms: 3364.985\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1657049343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440\n",
      "  training_iteration: 55\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 456\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 456\n",
      "    num_agent_steps_trained: 456\n",
      "    num_env_steps_sampled: 456\n",
      "    num_env_steps_trained: 456\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.34714103713619127\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 152\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.775557602921922e-18\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854999542236328\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.136327894841088e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.19490623474121094\n",
      "          total_loss: 3.8054723739624023\n",
      "          vf_explained_var: 1.068115216185106e-05\n",
      "          vf_loss: 4.0003790855407715\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 456\n",
      "    num_agent_steps_trained: 456\n",
      "    num_env_steps_sampled: 456\n",
      "    num_env_steps_trained: 456\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 456\n",
      "  num_agent_steps_trained: 456\n",
      "  num_env_steps_sampled: 456\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 456\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.883333333333333\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1439700368768149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 736.5922578811494\n",
      "    mean_inference_ms: 1.3179716154034313\n",
      "    mean_raw_obs_processing_ms: 5.071700075338682\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.34714103713619127\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03531119322298304\n",
      "      - 0.007898888079250543\n",
      "      - 0.004423153221276843\n",
      "      - 27.92241230000608\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1439700368768149\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 736.5922578811494\n",
      "      mean_inference_ms: 1.3179716154034313\n",
      "      mean_raw_obs_processing_ms: 5.071700075338682\n",
      "  time_since_restore: 229.9710419178009\n",
      "  time_this_iter_s: 4.246910810470581\n",
      "  time_total_s: 229.9710419178009\n",
      "  timers:\n",
      "    learn_throughput: 165.546\n",
      "    learn_time_ms: 48.325\n",
      "    load_throughput: 35025.503\n",
      "    load_time_ms: 0.228\n",
      "    training_iteration_time_ms: 3440.875\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1657049351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456\n",
      "  training_iteration: 57\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:11 (running for 00:04:06.96)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         229.971</td><td style=\"text-align: right;\"> 456</td><td style=\"text-align: right;\">0.347141</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 472\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 472\n",
      "    num_agent_steps_trained: 472\n",
      "    num_env_steps_sampled: 472\n",
      "    num_env_steps_trained: 472\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-20\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.002389849332891254\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 156\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.938894007304805e-19\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3851624727249146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.332072073040763e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5276077389717102\n",
      "          total_loss: 0.587431013584137\n",
      "          vf_explained_var: -0.02383425645530224\n",
      "          vf_loss: 0.059823229908943176\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 472\n",
      "    num_agent_steps_trained: 472\n",
      "    num_env_steps_sampled: 472\n",
      "    num_env_steps_trained: 472\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 472\n",
      "  num_agent_steps_trained: 472\n",
      "  num_env_steps_sampled: 472\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 472\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.75\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14396156063748822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 735.5403049944947\n",
      "    mean_inference_ms: 1.311266920433203\n",
      "    mean_raw_obs_processing_ms: 4.901785929085779\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.002389849332891254\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.016399707208365033\n",
      "      - 0.08158060155173175\n",
      "      - 0.1055787939720152\n",
      "      - 0.34606359618797455\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14396156063748822\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 735.5403049944947\n",
      "      mean_inference_ms: 1.311266920433203\n",
      "      mean_raw_obs_processing_ms: 4.901785929085779\n",
      "  time_since_restore: 238.11899304389954\n",
      "  time_this_iter_s: 4.415424823760986\n",
      "  time_total_s: 238.11899304389954\n",
      "  timers:\n",
      "    learn_throughput: 167.263\n",
      "    learn_time_ms: 47.829\n",
      "    load_throughput: 36535.749\n",
      "    load_time_ms: 0.219\n",
      "    training_iteration_time_ms: 3556.773\n",
      "    update_time_ms: 2.345\n",
      "  timestamp: 1657049360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472\n",
      "  training_iteration: 59\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:20 (running for 00:04:15.17)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         238.119</td><td style=\"text-align: right;\"> 472</td><td style=\"text-align: right;\">0.00238985</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:25 (running for 00:04:20.18)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         238.119</td><td style=\"text-align: right;\"> 472</td><td style=\"text-align: right;\">0.00238985</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 480\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 480\n",
      "    num_agent_steps_trained: 480\n",
      "    num_env_steps_sampled: 480\n",
      "    num_env_steps_trained: 480\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.06571429820922296\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 160\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -5.706423848562427\n",
      "    episode_reward_mean: -5.706423848562427\n",
      "    episode_reward_min: -5.706423848562427\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -5.706423848562427\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14647277625831398\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 855.0760746002197\n",
      "      mean_inference_ms: 2.235193510313292\n",
      "      mean_raw_obs_processing_ms: 0.6915685292836783\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4694470036524025e-19\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3855265378952026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.758441602665698e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.4909226596355438\n",
      "          total_loss: 2.4916892051696777\n",
      "          vf_explained_var: 0.0004890839336439967\n",
      "          vf_loss: 2.0007665157318115\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 480\n",
      "    num_agent_steps_trained: 480\n",
      "    num_env_steps_sampled: 480\n",
      "    num_env_steps_trained: 480\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 480\n",
      "  num_agent_steps_trained: 480\n",
      "  num_env_steps_sampled: 480\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 480\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.88888888888889\n",
      "    ram_util_percent: 39.477777777777774\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1439728768214382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.6101300385533\n",
      "    mean_inference_ms: 1.3052837008093499\n",
      "    mean_raw_obs_processing_ms: 4.747143011771114\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.06571429820922296\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.11394880136623042\n",
      "      - 0.11914441821097332\n",
      "      - -28.272537407217325\n",
      "      - 23.277208912516176\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1439728768214382\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.6101300385533\n",
      "      mean_inference_ms: 1.3052837008093499\n",
      "      mean_raw_obs_processing_ms: 4.747143011771114\n",
      "  time_since_restore: 244.4212682247162\n",
      "  time_this_iter_s: 6.30227518081665\n",
      "  time_total_s: 244.4212682247162\n",
      "  timers:\n",
      "    learn_throughput: 165.916\n",
      "    learn_time_ms: 48.217\n",
      "    load_throughput: 37714.322\n",
      "    load_time_ms: 0.212\n",
      "    training_iteration_time_ms: 3739.532\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1657049366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480\n",
      "  training_iteration: 60\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:30 (running for 00:04:25.32)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         248.177</td><td style=\"text-align: right;\"> 488</td><td style=\"text-align: right;\">-0.0472042</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 496\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 496\n",
      "    num_agent_steps_trained: 496\n",
      "    num_env_steps_sampled: 496\n",
      "    num_env_steps_trained: 496\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-33\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.0505697688704388\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 164\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.673617509131006e-20\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3856850862503052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.346278382807213e-07\n",
      "          model: {}\n",
      "          policy_loss: -0.5738440752029419\n",
      "          total_loss: -0.5069561004638672\n",
      "          vf_explained_var: 0.019644299522042274\n",
      "          vf_loss: 0.06688805669546127\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 496\n",
      "    num_agent_steps_trained: 496\n",
      "    num_env_steps_sampled: 496\n",
      "    num_env_steps_trained: 496\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 496\n",
      "  num_agent_steps_trained: 496\n",
      "  num_env_steps_sampled: 496\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 496\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.78\n",
      "    ram_util_percent: 39.480000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1440416723655811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 733.7522406301485\n",
      "    mean_inference_ms: 1.3000044568146714\n",
      "    mean_raw_obs_processing_ms: 4.605488047712907\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.0505697688704388\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.04006212480391724\n",
      "      - 0.721450643453359\n",
      "      - -0.05102863617712439\n",
      "      - 3.846233653508083\n",
      "      - -0.020096841857822678\n",
      "      - 0.028912730107843565\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1440416723655811\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 733.7522406301485\n",
      "      mean_inference_ms: 1.3000044568146714\n",
      "      mean_raw_obs_processing_ms: 4.605488047712907\n",
      "  time_since_restore: 251.4854757785797\n",
      "  time_this_iter_s: 3.308030128479004\n",
      "  time_total_s: 251.4854757785797\n",
      "  timers:\n",
      "    learn_throughput: 167.287\n",
      "    learn_time_ms: 47.822\n",
      "    load_throughput: 37617.076\n",
      "    load_time_ms: 0.213\n",
      "    training_iteration_time_ms: 3955.031\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1657049373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496\n",
      "  training_iteration: 62\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:37 (running for 00:04:32.18)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         254.926</td><td style=\"text-align: right;\"> 504</td><td style=\"text-align: right;\">-0.263601</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 512\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 512\n",
      "    num_agent_steps_trained: 512\n",
      "    num_env_steps_sampled: 512\n",
      "    num_env_steps_trained: 512\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-40\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.12286497028483315\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 170\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1684043772827515e-20\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3855855464935303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.205234735214617e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.2962568402290344\n",
      "          total_loss: 3.703948497772217\n",
      "          vf_explained_var: -0.001406852388754487\n",
      "          vf_loss: 4.0002055168151855\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 512\n",
      "    num_agent_steps_trained: 512\n",
      "    num_env_steps_sampled: 512\n",
      "    num_env_steps_trained: 512\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 512\n",
      "  num_agent_steps_trained: 512\n",
      "  num_env_steps_sampled: 512\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 512\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.04\n",
      "    ram_util_percent: 39.46\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14412202210722516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.1421944700355\n",
      "    mean_inference_ms: 1.2928179411343286\n",
      "    mean_raw_obs_processing_ms: 4.311443487264667\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.12286497028483315\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -3.7234960780126407\n",
      "      - 5.982020418501772\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14412202210722516\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.1421944700355\n",
      "      mean_inference_ms: 1.2928179411343286\n",
      "      mean_raw_obs_processing_ms: 4.311443487264667\n",
      "  time_since_restore: 258.1110243797302\n",
      "  time_this_iter_s: 3.1847846508026123\n",
      "  time_total_s: 258.1110243797302\n",
      "  timers:\n",
      "    learn_throughput: 166.103\n",
      "    learn_time_ms: 48.163\n",
      "    load_throughput: 40184.949\n",
      "    load_time_ms: 0.199\n",
      "    training_iteration_time_ms: 3824.652\n",
      "    update_time_ms: 2.359\n",
      "  timestamp: 1657049380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512\n",
      "  training_iteration: 64\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:45 (running for 00:04:40.39)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         258.111</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.122865</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 520\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 520\n",
      "    num_agent_steps_trained: 520\n",
      "    num_env_steps_sampled: 520\n",
      "    num_env_steps_trained: 520\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-46\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.10074491704155036\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 172\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -23.267039715355118\n",
      "    episode_reward_mean: -23.267039715355118\n",
      "    episode_reward_min: -23.267039715355118\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -23.267039715355118\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1458883285522461\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 844.7897672653198\n",
      "      mean_inference_ms: 2.1577298641204834\n",
      "      mean_raw_obs_processing_ms: 0.6892740726470947\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0842021886413758e-20\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854961395263672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.25293479161337e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.42253103852272034\n",
      "          total_loss: 2.752582550048828\n",
      "          vf_explained_var: -0.001615675282664597\n",
      "          vf_loss: 3.1751134395599365\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 520\n",
      "    num_agent_steps_trained: 520\n",
      "    num_env_steps_sampled: 520\n",
      "    num_env_steps_trained: 520\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 520\n",
      "  num_agent_steps_trained: 520\n",
      "  num_env_steps_sampled: 520\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 520\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0625\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1442757896456126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.8553620931079\n",
      "    mean_inference_ms: 1.291511532945461\n",
      "    mean_raw_obs_processing_ms: 4.354778773786961\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.10074491704155036\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.005650662312011612\n",
      "      - 0.06357833947737035\n",
      "      - 0.07649771094748781\n",
      "      - -30.162773220155767\n",
      "      - -0.019076574574572458\n",
      "      - -1.3955989674200229\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1442757896456126\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.8553620931079\n",
      "      mean_inference_ms: 1.291511532945461\n",
      "      mean_raw_obs_processing_ms: 4.354778773786961\n",
      "  time_since_restore: 263.84901666641235\n",
      "  time_this_iter_s: 5.737992286682129\n",
      "  time_total_s: 263.84901666641235\n",
      "  timers:\n",
      "    learn_throughput: 164.891\n",
      "    learn_time_ms: 48.517\n",
      "    load_throughput: 44949.005\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3765.891\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1657049386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520\n",
      "  training_iteration: 65\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 536\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 536\n",
      "    num_agent_steps_trained: 536\n",
      "    num_env_steps_sampled: 536\n",
      "    num_env_steps_trained: 536\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-52\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.6442291053232733\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 178\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7105054716034394e-21\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3855584859848022\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3042729733570013e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.16649919748306274\n",
      "          total_loss: 9.833499908447266\n",
      "          vf_explained_var: -0.0005919853574596345\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 536\n",
      "    num_agent_steps_trained: 536\n",
      "    num_env_steps_sampled: 536\n",
      "    num_env_steps_trained: 536\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 536\n",
      "  num_agent_steps_trained: 536\n",
      "  num_env_steps_sampled: 536\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 536\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.35\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14433105011058742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 731.6130397003985\n",
      "    mean_inference_ms: 1.2857971621434248\n",
      "    mean_raw_obs_processing_ms: 4.098461202967241\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.6442291053232733\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.025577378457993905\n",
      "      - 34.038927389138905\n",
      "      - 0.04285461857084094\n",
      "      - -2.4197715188978393\n",
      "      - 27.0160949734971\n",
      "      - 8.313372615364344\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14433105011058742\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 731.6130397003985\n",
      "      mean_inference_ms: 1.2857971621434248\n",
      "      mean_raw_obs_processing_ms: 4.098461202967241\n",
      "  time_since_restore: 269.92507004737854\n",
      "  time_this_iter_s: 2.3852620124816895\n",
      "  time_total_s: 269.92507004737854\n",
      "  timers:\n",
      "    learn_throughput: 165.09\n",
      "    learn_time_ms: 48.458\n",
      "    load_throughput: 43884.949\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3587.647\n",
      "    update_time_ms: 2.212\n",
      "  timestamp: 1657049392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536\n",
      "  training_iteration: 67\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:52 (running for 00:04:47.36)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         269.925</td><td style=\"text-align: right;\"> 536</td><td style=\"text-align: right;\">0.644229</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 552\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 552\n",
      "    num_agent_steps_trained: 552\n",
      "    num_env_steps_sampled: 552\n",
      "    num_env_steps_trained: 552\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-29-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.42325612644007665\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 184\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.776263679008599e-22\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3855211734771729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0570239282969851e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.523387610912323\n",
      "          total_loss: 5.097594738006592\n",
      "          vf_explained_var: -0.0013224601279944181\n",
      "          vf_loss: 5.6209821701049805\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 552\n",
      "    num_agent_steps_trained: 552\n",
      "    num_env_steps_sampled: 552\n",
      "    num_env_steps_trained: 552\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 552\n",
      "  num_agent_steps_trained: 552\n",
      "  num_env_steps_sampled: 552\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 552\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.0\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14454129001177585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.0862699220131\n",
      "    mean_inference_ms: 1.2821250681109504\n",
      "    mean_raw_obs_processing_ms: 4.04399446317509\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.42325612644007665\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -4.873409291399419\n",
      "      - -20.83624312114945\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14454129001177585\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.0862699220131\n",
      "      mean_inference_ms: 1.2821250681109504\n",
      "      mean_raw_obs_processing_ms: 4.04399446317509\n",
      "  time_since_restore: 275.2434997558594\n",
      "  time_this_iter_s: 2.5055041313171387\n",
      "  time_total_s: 275.2434997558594\n",
      "  timers:\n",
      "    learn_throughput: 163.937\n",
      "    learn_time_ms: 48.799\n",
      "    load_throughput: 42979.931\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3304.632\n",
      "    update_time_ms: 2.175\n",
      "  timestamp: 1657049397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552\n",
      "  training_iteration: 69\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:29:57 (running for 00:04:52.78)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         275.243</td><td style=\"text-align: right;\"> 552</td><td style=\"text-align: right;\">-0.423256</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:02 (running for 00:04:57.81)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         275.243</td><td style=\"text-align: right;\"> 552</td><td style=\"text-align: right;\">-0.423256</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 560\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 560\n",
      "    num_agent_steps_trained: 560\n",
      "    num_env_steps_sampled: 560\n",
      "    num_env_steps_trained: 560\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.22500711983016558\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 186\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.09089218064208293\n",
      "    episode_reward_mean: 0.09089218064208293\n",
      "    episode_reward_min: 0.09089218064208293\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.09089218064208293\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1452301823815634\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 850.6944678550542\n",
      "      mean_inference_ms: 2.101232839185138\n",
      "      mean_raw_obs_processing_ms: 0.6881480993226516\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3881318395042993e-22\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385513424873352\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4276010915637016e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.45486146211624146\n",
      "          total_loss: 8.342306137084961\n",
      "          vf_explained_var: -0.0009448250057175756\n",
      "          vf_loss: 8.797167778015137\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 560\n",
      "    num_agent_steps_trained: 560\n",
      "    num_env_steps_sampled: 560\n",
      "    num_env_steps_trained: 560\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 560\n",
      "  num_agent_steps_trained: 560\n",
      "  num_env_steps_sampled: 560\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 560\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.975\n",
      "    ram_util_percent: 39.4875\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14447737350770054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 731.0707015914164\n",
      "    mean_inference_ms: 1.2803333683418954\n",
      "    mean_raw_obs_processing_ms: 3.9142994426034905\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.22500711983016558\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -12.637904325730315\n",
      "      - -0.5143242748529353\n",
      "      - -0.00985072644131968\n",
      "      - 5.03216703835713\n",
      "      - 0.08658124330277595\n",
      "      - -4.6483561235539135\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14447737350770054\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 731.0707015914164\n",
      "      mean_inference_ms: 1.2803333683418954\n",
      "      mean_raw_obs_processing_ms: 3.9142994426034905\n",
      "  time_since_restore: 280.9625418186188\n",
      "  time_this_iter_s: 5.719042062759399\n",
      "  time_total_s: 280.9625418186188\n",
      "  timers:\n",
      "    learn_throughput: 164.165\n",
      "    learn_time_ms: 48.732\n",
      "    load_throughput: 44442.956\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3153.542\n",
      "    update_time_ms: 2.198\n",
      "  timestamp: 1657049403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560\n",
      "  training_iteration: 70\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 576\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 576\n",
      "    num_agent_steps_trained: 576\n",
      "    num_env_steps_sampled: 576\n",
      "    num_env_steps_trained: 576\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.28615109848837833\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 192\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.470329598760748e-23\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3853241205215454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.69548842072254e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.131069153547287\n",
      "          total_loss: 4.384771823883057\n",
      "          vf_explained_var: -0.00048029422760009766\n",
      "          vf_loss: 4.2537031173706055\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 576\n",
      "    num_agent_steps_trained: 576\n",
      "    num_env_steps_sampled: 576\n",
      "    num_env_steps_trained: 576\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 576\n",
      "  num_agent_steps_trained: 576\n",
      "  num_env_steps_sampled: 576\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 576\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.516666666666666\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14461614185721292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.0377877752462\n",
      "    mean_inference_ms: 1.2771318569638515\n",
      "    mean_raw_obs_processing_ms: 3.87046283831253\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.28615109848837833\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0882018147842305\n",
      "      - -1.3256973322665133\n",
      "      - 4.879602265021713\n",
      "      - -9.576016549394552\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14461614185721292\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.0377877752462\n",
      "      mean_inference_ms: 1.2771318569638515\n",
      "      mean_raw_obs_processing_ms: 3.87046283831253\n",
      "  time_since_restore: 287.2944209575653\n",
      "  time_this_iter_s: 4.027751684188843\n",
      "  time_total_s: 287.2944209575653\n",
      "  timers:\n",
      "    learn_throughput: 162.252\n",
      "    learn_time_ms: 49.306\n",
      "    load_throughput: 44979.131\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3080.27\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1657049409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576\n",
      "  training_iteration: 72\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:09 (running for 00:05:04.99)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         287.294</td><td style=\"text-align: right;\"> 576</td><td style=\"text-align: right;\">-0.286151</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 592\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 592\n",
      "    num_agent_steps_trained: 592\n",
      "    num_env_steps_sampled: 592\n",
      "    num_env_steps_trained: 592\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.2116147690392779\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 196\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.117582399690187e-23\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3843411207199097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.884315553179476e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.09582927823066711\n",
      "          total_loss: 0.09906155616044998\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0032322602346539497\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 592\n",
      "    num_agent_steps_trained: 592\n",
      "    num_env_steps_sampled: 592\n",
      "    num_env_steps_trained: 592\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 592\n",
      "  num_agent_steps_trained: 592\n",
      "  num_env_steps_sampled: 592\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 592\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.085714285714285\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14461032957649617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.1698223384711\n",
      "    mean_inference_ms: 1.2746466899627942\n",
      "    mean_raw_obs_processing_ms: 3.7915918886698257\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.2116147690392779\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.307688264799185\n",
      "      - -22.464778621170733\n",
      "      - 0.02443670542074239\n",
      "      - 35.11691174532054\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14461032957649617\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.1698223384711\n",
      "      mean_inference_ms: 1.2746466899627942\n",
      "      mean_raw_obs_processing_ms: 3.7915918886698257\n",
      "  time_since_restore: 294.68870544433594\n",
      "  time_this_iter_s: 4.292093992233276\n",
      "  time_total_s: 294.68870544433594\n",
      "  timers:\n",
      "    learn_throughput: 162.282\n",
      "    learn_time_ms: 49.297\n",
      "    load_throughput: 44086.759\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3157.198\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1657049417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592\n",
      "  training_iteration: 74\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:17 (running for 00:05:12.47)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         294.689</td><td style=\"text-align: right;\"> 592</td><td style=\"text-align: right;\">-0.211615</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:22 (running for 00:05:17.48)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         294.689</td><td style=\"text-align: right;\"> 592</td><td style=\"text-align: right;\">-0.211615</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 600\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 600\n",
      "    num_agent_steps_trained: 600\n",
      "    num_env_steps_sampled: 600\n",
      "    num_env_steps_trained: 600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-24\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.3515701978210245\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 200\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.0005383608557222308\n",
      "    episode_reward_mean: 0.0005383608557222308\n",
      "    episode_reward_min: 0.0005383608557222308\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0005383608557222308\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14578259509542715\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 854.852355044821\n",
      "      mean_inference_ms: 2.0526388417119565\n",
      "      mean_raw_obs_processing_ms: 0.684059184530507\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0587911998450935e-23\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3847399950027466\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0723749710450647e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.34692513942718506\n",
      "          total_loss: 0.34812185168266296\n",
      "          vf_explained_var: -0.08462338149547577\n",
      "          vf_loss: 0.0011967484606429935\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 600\n",
      "    num_agent_steps_trained: 600\n",
      "    num_env_steps_sampled: 600\n",
      "    num_env_steps_trained: 600\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 600\n",
      "  num_agent_steps_trained: 600\n",
      "  num_env_steps_sampled: 600\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 600\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.511111111111111\n",
      "    ram_util_percent: 39.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14463965973666926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.407618708416\n",
      "    mean_inference_ms: 1.2723723709054036\n",
      "    mean_raw_obs_processing_ms: 3.7173880119468663\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.3515701978210245\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03133330679973367\n",
      "      - 0.03922954323283001\n",
      "      - -35.118821415117345\n",
      "      - 0.009714316189409544\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14463965973666926\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.407618708416\n",
      "      mean_inference_ms: 1.2723723709054036\n",
      "      mean_raw_obs_processing_ms: 3.7173880119468663\n",
      "  time_since_restore: 301.45099925994873\n",
      "  time_this_iter_s: 6.762293815612793\n",
      "  time_total_s: 301.45099925994873\n",
      "  timers:\n",
      "    learn_throughput: 162.042\n",
      "    learn_time_ms: 49.37\n",
      "    load_throughput: 44202.914\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3200.695\n",
      "    update_time_ms: 2.201\n",
      "  timestamp: 1657049424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600\n",
      "  training_iteration: 75\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:27 (running for 00:05:22.84)<br>Memory usage on this node: 12.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         304.966</td><td style=\"text-align: right;\"> 608</td><td style=\"text-align: right;\">-0.350926</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 616\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 616\n",
      "    num_agent_steps_trained: 616\n",
      "    num_env_steps_sampled: 616\n",
      "    num_env_steps_trained: 616\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-31\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.000347765342747145\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 204\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.646977999612734e-24\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.384734869003296\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0346581802878063e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.22108988463878632\n",
      "          total_loss: 2.2246930599212646\n",
      "          vf_explained_var: 0.0033480802085250616\n",
      "          vf_loss: 2.003603219985962\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 616\n",
      "    num_agent_steps_trained: 616\n",
      "    num_env_steps_sampled: 616\n",
      "    num_env_steps_trained: 616\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 616\n",
      "  num_agent_steps_trained: 616\n",
      "  num_env_steps_sampled: 616\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 616\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.366666666666667\n",
      "    ram_util_percent: 39.46666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1446927863780768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.6270311070149\n",
      "    mean_inference_ms: 1.2704292237661183\n",
      "    mean_raw_obs_processing_ms: 3.647451371340752\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.000347765342747145\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.025251698286016477\n",
      "      - 33.7481881702606\n",
      "      - 0.01976895129699141\n",
      "      - -33.76273838691315\n",
      "      - 0.05168938718175875\n",
      "      - -1.3969779289555369\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1446927863780768\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.6270311070149\n",
      "      mean_inference_ms: 1.2704292237661183\n",
      "      mean_raw_obs_processing_ms: 3.647451371340752\n",
      "  time_since_restore: 308.6933171749115\n",
      "  time_this_iter_s: 3.727752685546875\n",
      "  time_total_s: 308.6933171749115\n",
      "  timers:\n",
      "    learn_throughput: 163.354\n",
      "    learn_time_ms: 48.973\n",
      "    load_throughput: 43759.04\n",
      "    load_time_ms: 0.183\n",
      "    training_iteration_time_ms: 3317.18\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1657049431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616\n",
      "  training_iteration: 77\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:35 (running for 00:05:30.31)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         312.317</td><td style=\"text-align: right;\"> 624</td><td style=\"text-align: right;\">-0.000481844</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 632\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 632\n",
      "    num_agent_steps_trained: 632\n",
      "    num_env_steps_sampled: 632\n",
      "    num_env_steps_trained: 632\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-38\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.0004548826650939308\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 210\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.617444999031835e-25\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385851502418518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.388401981079369e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.18791532516479492\n",
      "          total_loss: 0.5785229206085205\n",
      "          vf_explained_var: -0.01729259081184864\n",
      "          vf_loss: 0.39060765504837036\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 632\n",
      "    num_agent_steps_trained: 632\n",
      "    num_env_steps_sampled: 632\n",
      "    num_env_steps_trained: 632\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 632\n",
      "  num_agent_steps_trained: 632\n",
      "  num_env_steps_sampled: 632\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 632\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.733333333333334\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1446733360165381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 731.5404890224918\n",
      "    mean_inference_ms: 1.2668245930423359\n",
      "    mean_raw_obs_processing_ms: 3.4832350393453773\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.0004548826650939308\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.282223155158165\n",
      "      - 0.0071186328497065965\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1446733360165381\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 731.5404890224918\n",
      "      mean_inference_ms: 1.2668245930423359\n",
      "      mean_raw_obs_processing_ms: 3.4832350393453773\n",
      "  time_since_restore: 316.13948702812195\n",
      "  time_this_iter_s: 3.822178840637207\n",
      "  time_total_s: 316.13948702812195\n",
      "  timers:\n",
      "    learn_throughput: 166.321\n",
      "    learn_time_ms: 48.1\n",
      "    load_throughput: 46558.113\n",
      "    load_time_ms: 0.172\n",
      "    training_iteration_time_ms: 3530.038\n",
      "    update_time_ms: 2.231\n",
      "  timestamp: 1657049438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632\n",
      "  training_iteration: 79\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:44 (running for 00:05:39.16)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         316.139</td><td style=\"text-align: right;\"> 632</td><td style=\"text-align: right;\">-0.000454883</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 640\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 640\n",
      "    num_agent_steps_trained: 640\n",
      "    num_env_steps_sampled: 640\n",
      "    num_env_steps_trained: 640\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-45\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.01325255530051971\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 212\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.000972798086700366\n",
      "    episode_reward_mean: 0.000972798086700366\n",
      "    episode_reward_min: 0.000972798086700366\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.000972798086700366\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14473954025579958\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 862.3861779971999\n",
      "      mean_inference_ms: 2.0003318786621094\n",
      "      mean_raw_obs_processing_ms: 0.688085750657685\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3087224995159173e-25\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3843975067138672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.321451110125054e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.21037855744361877\n",
      "          total_loss: 0.8281955122947693\n",
      "          vf_explained_var: -0.03300447016954422\n",
      "          vf_loss: 0.6178170442581177\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 640\n",
      "    num_agent_steps_trained: 640\n",
      "    num_env_steps_sampled: 640\n",
      "    num_env_steps_trained: 640\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 640\n",
      "  num_agent_steps_trained: 640\n",
      "  num_env_steps_sampled: 640\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 640\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.200000000000001\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14476049501763513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.9764233489763\n",
      "    mean_inference_ms: 1.266271733806934\n",
      "    mean_raw_obs_processing_ms: 3.518599009399797\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.01325255530051971\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.298704390998008\n",
      "      - -0.0009022264073110797\n",
      "      - -0.009724386911038163\n",
      "      - 0.04359573663854288\n",
      "      - 0.01824231096558382\n",
      "      - 0.04683907868775705\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14476049501763513\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.9764233489763\n",
      "      mean_inference_ms: 1.266271733806934\n",
      "      mean_raw_obs_processing_ms: 3.518599009399797\n",
      "  time_since_restore: 322.82065892219543\n",
      "  time_this_iter_s: 6.681171894073486\n",
      "  time_total_s: 322.82065892219543\n",
      "  timers:\n",
      "    learn_throughput: 167.927\n",
      "    learn_time_ms: 47.64\n",
      "    load_throughput: 45337.7\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3611.684\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1657049445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640\n",
      "  training_iteration: 80\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:50 (running for 00:05:45.53)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         327.405</td><td style=\"text-align: right;\"> 648</td><td style=\"text-align: right;\">0.280072</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 656\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 656\n",
      "    num_agent_steps_trained: 656\n",
      "    num_env_steps_sampled: 656\n",
      "    num_env_steps_trained: 656\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-30-53\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.3409824761954974\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 218\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.271806248789793e-26\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3851786851882935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.761790736054536e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.4380095303058624\n",
      "          total_loss: 4.149560928344727\n",
      "          vf_explained_var: 0.013327368535101414\n",
      "          vf_loss: 4.5875701904296875\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 656\n",
      "    num_agent_steps_trained: 656\n",
      "    num_env_steps_sampled: 656\n",
      "    num_env_steps_trained: 656\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 656\n",
      "  num_agent_steps_trained: 656\n",
      "  num_env_steps_sampled: 656\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 656\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.919999999999998\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1446790547553787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 731.642735228846\n",
      "    mean_inference_ms: 1.262632069072355\n",
      "    mean_raw_obs_processing_ms: 3.3687378820077147\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.3409824761954974\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.008423566346478673\n",
      "      - 0.05770270614690043\n",
      "      - -1.366397842081726\n",
      "      - 24.64153891665569\n",
      "      - 0.012725134249780057\n",
      "      - 3.268257213230921\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1446790547553787\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 731.642735228846\n",
      "      mean_inference_ms: 1.262632069072355\n",
      "      mean_raw_obs_processing_ms: 3.3687378820077147\n",
      "  time_since_restore: 330.79353070259094\n",
      "  time_this_iter_s: 3.3884379863739014\n",
      "  time_total_s: 330.79353070259094\n",
      "  timers:\n",
      "    learn_throughput: 167.808\n",
      "    learn_time_ms: 47.673\n",
      "    load_throughput: 44484.2\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3775.808\n",
      "    update_time_ms: 2.204\n",
      "  timestamp: 1657049453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656\n",
      "  training_iteration: 82\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:30:57 (running for 00:05:52.73)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         334.477</td><td style=\"text-align: right;\"> 664</td><td style=\"text-align: right;\">-0.000143498</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 672\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 672\n",
      "    num_agent_steps_trained: 672\n",
      "    num_env_steps_sampled: 672\n",
      "    num_env_steps_trained: 672\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.27944580508311256\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 224\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0679515621974483e-26\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.384314775466919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.213568975799717e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.2244318425655365\n",
      "          total_loss: 2.239877939224243\n",
      "          vf_explained_var: 0.0008020102977752686\n",
      "          vf_loss: 2.015446424484253\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 672\n",
      "    num_agent_steps_trained: 672\n",
      "    num_env_steps_sampled: 672\n",
      "    num_env_steps_trained: 672\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 672\n",
      "  num_agent_steps_trained: 672\n",
      "  num_env_steps_sampled: 672\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 672\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.860000000000003\n",
      "    ram_util_percent: 39.559999999999995\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14472455577634372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.9277414283407\n",
      "    mean_inference_ms: 1.2600640969998753\n",
      "    mean_raw_obs_processing_ms: 3.349302200972253\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.27944580508311256\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.013258455710666528\n",
      "      - 1.3590098637518278\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14472455577634372\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.9277414283407\n",
      "      mean_inference_ms: 1.2600640969998753\n",
      "      mean_raw_obs_processing_ms: 3.349302200972253\n",
      "  time_since_restore: 338.1587595939636\n",
      "  time_this_iter_s: 3.682253360748291\n",
      "  time_total_s: 338.1587595939636\n",
      "  timers:\n",
      "    learn_throughput: 167.304\n",
      "    learn_time_ms: 47.817\n",
      "    load_throughput: 44632.126\n",
      "    load_time_ms: 0.179\n",
      "    training_iteration_time_ms: 3772.486\n",
      "    update_time_ms: 2.287\n",
      "  timestamp: 1657049461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672\n",
      "  training_iteration: 84\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:06 (running for 00:06:01.42)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         338.159</td><td style=\"text-align: right;\"> 672</td><td style=\"text-align: right;\">-0.279446</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 680\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 680\n",
      "    num_agent_steps_trained: 680\n",
      "    num_env_steps_sampled: 680\n",
      "    num_env_steps_trained: 680\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-08\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: -0.29337707715728806\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 226\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.00040541113245573523\n",
      "    episode_reward_mean: 0.00040541113245573523\n",
      "    episode_reward_min: 0.00040541113245573523\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.00040541113245573523\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14418363571166992\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 868.3553062952482\n",
      "      mean_inference_ms: 1.9565912393423226\n",
      "      mean_raw_obs_processing_ms: 0.6881035291231595\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0339757810987241e-26\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3856691122055054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.388624008948682e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.4781613051891327\n",
      "          total_loss: 0.1871228963136673\n",
      "          vf_explained_var: -0.12270383536815643\n",
      "          vf_loss: 0.6652842164039612\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 680\n",
      "    num_agent_steps_trained: 680\n",
      "    num_env_steps_sampled: 680\n",
      "    num_env_steps_trained: 680\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 680\n",
      "  num_agent_steps_trained: 680\n",
      "  num_env_steps_sampled: 680\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 680\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.129999999999999\n",
      "    ram_util_percent: 39.510000000000005\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14463881435038045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 731.5920122458322\n",
      "    mean_inference_ms: 1.2586335974800666\n",
      "    mean_raw_obs_processing_ms: 3.265308588128093\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: -0.29337707715728806\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -5.748472140520505\n",
      "      - 11.199410640730324\n",
      "      - -0.04553442119903117\n",
      "      - -6.551510752687459\n",
      "      - 0.1323463118462449\n",
      "      - 6.509510499886094\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14463881435038045\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 731.5920122458322\n",
      "      mean_inference_ms: 1.2586335974800666\n",
      "      mean_raw_obs_processing_ms: 3.265308588128093\n",
      "  time_since_restore: 345.2101447582245\n",
      "  time_this_iter_s: 7.051385164260864\n",
      "  time_total_s: 345.2101447582245\n",
      "  timers:\n",
      "    learn_throughput: 169.014\n",
      "    learn_time_ms: 47.333\n",
      "    load_throughput: 45142.516\n",
      "    load_time_ms: 0.177\n",
      "    training_iteration_time_ms: 3785.977\n",
      "    update_time_ms: 2.224\n",
      "  timestamp: 1657049468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680\n",
      "  training_iteration: 85\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:12 (running for 00:06:07.16)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         348.816</td><td style=\"text-align: right;\"> 688</td><td style=\"text-align: right;\">-0.348602</td><td style=\"text-align: right;\">             39.4073</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 696\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 696\n",
      "    num_agent_steps_trained: 696\n",
      "    num_env_steps_sampled: 696\n",
      "    num_env_steps_trained: 696\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-15\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.4073055421964\n",
      "  episode_reward_mean: 0.03244444666341829\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 232\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5849394527468104e-27\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3857172727584839\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7314083606834174e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.16481517255306244\n",
      "          total_loss: 2.167149305343628\n",
      "          vf_explained_var: -0.0013069987762719393\n",
      "          vf_loss: 2.0023343563079834\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 696\n",
      "    num_agent_steps_trained: 696\n",
      "    num_env_steps_sampled: 696\n",
      "    num_env_steps_trained: 696\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 696\n",
      "  num_agent_steps_trained: 696\n",
      "  num_env_steps_sampled: 696\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 696\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.424999999999997\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14469836287496116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 733.4724028475548\n",
      "    mean_inference_ms: 1.2562973233737444\n",
      "    mean_raw_obs_processing_ms: 3.2497716613991297\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.4073055421964\n",
      "    episode_reward_mean: 0.03244444666341829\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 39.4073055421964\n",
      "      - -4.397808899064273\n",
      "      - -2.652508074509349\n",
      "      - -3.273504982373435\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14469836287496116\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 733.4724028475548\n",
      "      mean_inference_ms: 1.2562973233737444\n",
      "      mean_raw_obs_processing_ms: 3.2497716613991297\n",
      "  time_since_restore: 352.06867957115173\n",
      "  time_this_iter_s: 3.2521886825561523\n",
      "  time_total_s: 352.06867957115173\n",
      "  timers:\n",
      "    learn_throughput: 172.217\n",
      "    learn_time_ms: 46.453\n",
      "    load_throughput: 45795.594\n",
      "    load_time_ms: 0.175\n",
      "    training_iteration_time_ms: 3747.749\n",
      "    update_time_ms: 2.183\n",
      "  timestamp: 1657049475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696\n",
      "  training_iteration: 87\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:18 (running for 00:06:13.74)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">           355.3</td><td style=\"text-align: right;\"> 704</td><td style=\"text-align: right;\">-0.31354</td><td style=\"text-align: right;\">             38.6885</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 712\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 712\n",
      "    num_agent_steps_trained: 712\n",
      "    num_env_steps_sampled: 712\n",
      "    num_env_steps_trained: 712\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-21\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.688538726889576\n",
      "  episode_reward_mean: -0.2516088373843116\n",
      "  episode_reward_min: -39.56517672311935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 236\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.462348631867026e-28\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3847732543945312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.2936711654183455e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5026589035987854\n",
      "          total_loss: 3.843308448791504\n",
      "          vf_explained_var: 0.0008262813207693398\n",
      "          vf_loss: 4.345967769622803\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 712\n",
      "    num_agent_steps_trained: 712\n",
      "    num_env_steps_sampled: 712\n",
      "    num_env_steps_trained: 712\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 712\n",
      "  num_agent_steps_trained: 712\n",
      "  num_env_steps_sampled: 712\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 712\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.6\n",
      "    ram_util_percent: 39.525\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14467065903974471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 733.9287997965055\n",
      "    mean_inference_ms: 1.2542241694979337\n",
      "    mean_raw_obs_processing_ms: 3.203497886058155\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.688538726889576\n",
      "    episode_reward_mean: -0.2516088373843116\n",
      "    episode_reward_min: -39.56517672311935\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 4.542029300368995\n",
      "      - 11.079601565304202\n",
      "      - -39.56517672311935\n",
      "      - -0.00039309462024306185\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14467065903974471\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 733.9287997965055\n",
      "      mean_inference_ms: 1.2542241694979337\n",
      "      mean_raw_obs_processing_ms: 3.203497886058155\n",
      "  time_since_restore: 358.3664937019348\n",
      "  time_this_iter_s: 3.0663187503814697\n",
      "  time_total_s: 358.3664937019348\n",
      "  timers:\n",
      "    learn_throughput: 169.83\n",
      "    learn_time_ms: 47.106\n",
      "    load_throughput: 43509.378\n",
      "    load_time_ms: 0.184\n",
      "    training_iteration_time_ms: 3632.94\n",
      "    update_time_ms: 2.226\n",
      "  timestamp: 1657049481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712\n",
      "  training_iteration: 89\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:26 (running for 00:06:21.84)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         358.366</td><td style=\"text-align: right;\"> 712</td><td style=\"text-align: right;\">-0.251609</td><td style=\"text-align: right;\">             38.6885</td><td style=\"text-align: right;\">            -39.5652</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 720\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 720\n",
      "    num_agent_steps_trained: 720\n",
      "    num_env_steps_sampled: 720\n",
      "    num_env_steps_trained: 720\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-28\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.688538726889576\n",
      "  episode_reward_mean: -0.3860259636646004\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 240\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.0021375574480978488\n",
      "    episode_reward_mean: -0.0021375574480978488\n",
      "    episode_reward_min: -0.0021375574480978488\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0021375574480978488\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14391378922895953\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 876.0547637939454\n",
      "      mean_inference_ms: 1.9167336550625889\n",
      "      mean_raw_obs_processing_ms: 0.6899270144375889\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.231174315933513e-28\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385614037513733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.7076509923062986e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.008160632103681564\n",
      "          total_loss: 0.7449018359184265\n",
      "          vf_explained_var: -0.01954755000770092\n",
      "          vf_loss: 0.7530624866485596\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 720\n",
      "    num_agent_steps_trained: 720\n",
      "    num_env_steps_sampled: 720\n",
      "    num_env_steps_trained: 720\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 720\n",
      "  num_agent_steps_trained: 720\n",
      "  num_env_steps_sampled: 720\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 720\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.579999999999998\n",
      "    ram_util_percent: 39.53\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14464740114489316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.4966187985312\n",
      "    mean_inference_ms: 1.2521683710789824\n",
      "    mean_raw_obs_processing_ms: 3.1592659990731096\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.688538726889576\n",
      "    episode_reward_mean: -0.3860259636646004\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.05074144665081759\n",
      "      - 0.060001671540470536\n",
      "      - -10.904002802365202\n",
      "      - -0.00021031515363389985\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14464740114489316\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.4966187985312\n",
      "      mean_inference_ms: 1.2521683710789824\n",
      "      mean_raw_obs_processing_ms: 3.1592659990731096\n",
      "  time_since_restore: 364.81879925727844\n",
      "  time_this_iter_s: 6.452305555343628\n",
      "  time_total_s: 364.81879925727844\n",
      "  timers:\n",
      "    learn_throughput: 170.93\n",
      "    learn_time_ms: 46.803\n",
      "    load_throughput: 43312.808\n",
      "    load_time_ms: 0.185\n",
      "    training_iteration_time_ms: 3600.517\n",
      "    update_time_ms: 2.202\n",
      "  timestamp: 1657049488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720\n",
      "  training_iteration: 90\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:32 (running for 00:06:27.63)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         369.041</td><td style=\"text-align: right;\"> 728</td><td style=\"text-align: right;\">-0.400408</td><td style=\"text-align: right;\">             38.6885</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 736\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 736\n",
      "    num_agent_steps_trained: 736\n",
      "    num_env_steps_sampled: 736\n",
      "    num_env_steps_trained: 736\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.688538726889576\n",
      "  episode_reward_mean: -0.29077606908860465\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 244\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.077935789833782e-29\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.38528573513031\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0939055502822157e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.1629982888698578\n",
      "          total_loss: 2.1659553050994873\n",
      "          vf_explained_var: 0.0031259774696081877\n",
      "          vf_loss: 2.0029568672180176\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 736\n",
      "    num_agent_steps_trained: 736\n",
      "    num_env_steps_sampled: 736\n",
      "    num_env_steps_trained: 736\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 736\n",
      "  num_agent_steps_trained: 736\n",
      "  num_env_steps_sampled: 736\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 736\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.82\n",
      "    ram_util_percent: 39.58\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14461222174202104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 735.0882464371105\n",
      "    mean_inference_ms: 1.25012971744605\n",
      "    mean_raw_obs_processing_ms: 3.1169400865543597\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.688538726889576\n",
      "    episode_reward_mean: -0.29077606908860465\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.022665721307608244\n",
      "      - -27.570364997260626\n",
      "      - 0.029451546646872107\n",
      "      - -0.02338879415880113\n",
      "      - -0.10696556669209034\n",
      "      - 0.027830617391428447\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14461222174202104\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 735.0882464371105\n",
      "      mean_inference_ms: 1.25012971744605\n",
      "      mean_raw_obs_processing_ms: 3.1169400865543597\n",
      "  time_since_restore: 372.8308343887329\n",
      "  time_this_iter_s: 3.7898032665252686\n",
      "  time_total_s: 372.8308343887329\n",
      "  timers:\n",
      "    learn_throughput: 170.494\n",
      "    learn_time_ms: 46.923\n",
      "    load_throughput: 44691.572\n",
      "    load_time_ms: 0.179\n",
      "    training_iteration_time_ms: 3604.043\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1657049496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736\n",
      "  training_iteration: 92\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:41 (running for 00:06:36.15)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         377.475</td><td style=\"text-align: right;\"> 744</td><td style=\"text-align: right;\">0.375651</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 752\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 752\n",
      "    num_agent_steps_trained: 752\n",
      "    num_env_steps_sampled: 752\n",
      "    num_env_steps_trained: 752\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-44\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.3809297712576688\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 250\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0194839474584456e-29\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3856669664382935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.782254553807434e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.2550975978374481\n",
      "          total_loss: 6.359776020050049\n",
      "          vf_explained_var: 0.0017148733604699373\n",
      "          vf_loss: 6.614873886108398\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 752\n",
      "    num_agent_steps_trained: 752\n",
      "    num_env_steps_sampled: 752\n",
      "    num_env_steps_trained: 752\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 752\n",
      "  num_agent_steps_trained: 752\n",
      "  num_env_steps_sampled: 752\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 752\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.380000000000003\n",
      "    ram_util_percent: 39.54\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14446161181044254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.2423704953369\n",
      "    mean_inference_ms: 1.2469209135485109\n",
      "    mean_raw_obs_processing_ms: 3.0077169362919953\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.3809297712576688\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 38.470861748515844\n",
      "      - -5.2511991905301585\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14446161181044254\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.2423704953369\n",
      "      mean_inference_ms: 1.2469209135485109\n",
      "      mean_raw_obs_processing_ms: 3.0077169362919953\n",
      "  time_since_restore: 380.6764488220215\n",
      "  time_this_iter_s: 3.201733350753784\n",
      "  time_total_s: 380.6764488220215\n",
      "  timers:\n",
      "    learn_throughput: 172.651\n",
      "    learn_time_ms: 46.336\n",
      "    load_throughput: 42881.063\n",
      "    load_time_ms: 0.187\n",
      "    training_iteration_time_ms: 3652.475\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1657049504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752\n",
      "  training_iteration: 94\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:49 (running for 00:06:44.44)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         380.676</td><td style=\"text-align: right;\"> 752</td><td style=\"text-align: right;\"> 0.38093</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 760\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 760\n",
      "    num_agent_steps_trained: 760\n",
      "    num_env_steps_sampled: 760\n",
      "    num_env_steps_trained: 760\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-51\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.34586721235130624\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 252\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.003743676432123255\n",
      "    episode_reward_mean: 0.003743676432123255\n",
      "    episode_reward_min: 0.003743676432123255\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.003743676432123255\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14304292613062367\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 882.7430503121739\n",
      "      mean_inference_ms: 1.8698182599297888\n",
      "      mean_raw_obs_processing_ms: 0.6882733312146417\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0097419737292228e-29\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3856929540634155\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2751087524520699e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.33529022336006165\n",
      "          total_loss: 4.337952613830566\n",
      "          vf_explained_var: 0.005712101701647043\n",
      "          vf_loss: 4.002662181854248\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 760\n",
      "    num_agent_steps_trained: 760\n",
      "    num_env_steps_sampled: 760\n",
      "    num_env_steps_trained: 760\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 760\n",
      "  num_agent_steps_trained: 760\n",
      "  num_env_steps_sampled: 760\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 760\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.969999999999999\n",
      "    ram_util_percent: 39.529999999999994\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14451991140937206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 736.059568513274\n",
      "    mean_inference_ms: 1.2464619610765504\n",
      "    mean_raw_obs_processing_ms: 3.037599148236993\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.34586721235130624\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.025243359498570594\n",
      "      - -8.176872765197459\n",
      "      - 0.027758230602860756\n",
      "      - 1.598662261846787\n",
      "      - -0.020588497941270045\n",
      "      - 0.009851404799794405\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14451991140937206\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 736.059568513274\n",
      "      mean_inference_ms: 1.2464619610765504\n",
      "      mean_raw_obs_processing_ms: 3.037599148236993\n",
      "  time_since_restore: 388.1666181087494\n",
      "  time_this_iter_s: 7.490169286727905\n",
      "  time_total_s: 388.1666181087494\n",
      "  timers:\n",
      "    learn_throughput: 170.552\n",
      "    learn_time_ms: 46.906\n",
      "    load_throughput: 43040.575\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3684.65\n",
      "    update_time_ms: 2.254\n",
      "  timestamp: 1657049511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760\n",
      "  training_iteration: 95\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:31:55 (running for 00:06:50.76)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         391.907</td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">0.098136</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 776\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 776\n",
      "    num_agent_steps_trained: 776\n",
      "    num_env_steps_sampled: 776\n",
      "    num_env_steps_trained: 776\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-31-59\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.0114861959176692\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 258\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.524354934323057e-30\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385690689086914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2765560768457362e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.16235855221748352\n",
      "          total_loss: -0.1217077448964119\n",
      "          vf_explained_var: 1.7881394143159923e-08\n",
      "          vf_loss: 0.04065079614520073\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 776\n",
      "    num_agent_steps_trained: 776\n",
      "    num_env_steps_sampled: 776\n",
      "    num_env_steps_trained: 776\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 776\n",
      "  num_agent_steps_trained: 776\n",
      "  num_env_steps_sampled: 776\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 776\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.6\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14437969963283293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 735.0125412704407\n",
      "    mean_inference_ms: 1.2434966248860797\n",
      "    mean_raw_obs_processing_ms: 2.9357894609346986\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.0114861959176692\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.781420977969841\n",
      "      - 8.674225657664579\n",
      "      - 0.06024211816012803\n",
      "      - -11.346895627121622\n",
      "      - 0.022098292189425273\n",
      "      - 4.7599694050372285\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14437969963283293\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 735.0125412704407\n",
      "      mean_inference_ms: 1.2434966248860797\n",
      "      mean_raw_obs_processing_ms: 2.9357894609346986\n",
      "  time_since_restore: 395.9747049808502\n",
      "  time_this_iter_s: 4.06739068031311\n",
      "  time_total_s: 395.9747049808502\n",
      "  timers:\n",
      "    learn_throughput: 167.987\n",
      "    learn_time_ms: 47.623\n",
      "    load_throughput: 43690.667\n",
      "    load_time_ms: 0.183\n",
      "    training_iteration_time_ms: 3779.556\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1657049519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 776\n",
      "  training_iteration: 97\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:02 (running for 00:06:58.09)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         399.181</td><td style=\"text-align: right;\"> 784</td><td style=\"text-align: right;\">-0.0483815</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 792\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 792\n",
      "    num_agent_steps_trained: 792\n",
      "    num_env_steps_sampled: 792\n",
      "    num_env_steps_trained: 792\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-06\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.04627884414191613\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 264\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.3108873358076425e-31\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3853449821472168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.185408220611862e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.38183578848838806\n",
      "          total_loss: 4.382559776306152\n",
      "          vf_explained_var: -0.00421886844560504\n",
      "          vf_loss: 4.000723838806152\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 792\n",
      "    num_agent_steps_trained: 792\n",
      "    num_env_steps_sampled: 792\n",
      "    num_env_steps_trained: 792\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 792\n",
      "  num_agent_steps_trained: 792\n",
      "  num_env_steps_sampled: 792\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 792\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.833333333333334\n",
      "    ram_util_percent: 39.55\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14442283070872278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.3247303143196\n",
      "    mean_inference_ms: 1.2416607254268683\n",
      "    mean_raw_obs_processing_ms: 2.9304080288035963\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.04627884414191613\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0471420760005441\n",
      "      - 0.05107726316972583\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14442283070872278\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.3247303143196\n",
      "      mean_inference_ms: 1.2416607254268683\n",
      "      mean_raw_obs_processing_ms: 2.9304080288035963\n",
      "  time_since_restore: 402.86305594444275\n",
      "  time_this_iter_s: 3.682551383972168\n",
      "  time_total_s: 402.86305594444275\n",
      "  timers:\n",
      "    learn_throughput: 169.586\n",
      "    learn_time_ms: 47.174\n",
      "    load_throughput: 46002.786\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3838.57\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1657049526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792\n",
      "  training_iteration: 99\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:11 (running for 00:07:06.87)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         402.863</td><td style=\"text-align: right;\"> 792</td><td style=\"text-align: right;\">0.0462788</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 800\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 800\n",
      "    num_agent_steps_trained: 800\n",
      "    num_env_steps_sampled: 800\n",
      "    num_env_steps_trained: 800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.04683016735673588\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 266\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.00032014742736463653\n",
      "    episode_reward_mean: 0.00032014742736463653\n",
      "    episode_reward_min: 0.00032014742736463653\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.00032014742736463653\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14315276849465292\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 883.4267796063032\n",
      "      mean_inference_ms: 1.8368783544321527\n",
      "      mean_raw_obs_processing_ms: 0.6843355835461226\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1554436679038213e-31\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3850284814834595\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.5283867368416395e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5450056195259094\n",
      "          total_loss: 3.458409070968628\n",
      "          vf_explained_var: 0.025586076080799103\n",
      "          vf_loss: 4.003414630889893\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 800\n",
      "    num_agent_steps_trained: 800\n",
      "    num_env_steps_sampled: 800\n",
      "    num_env_steps_trained: 800\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 800\n",
      "  num_agent_steps_trained: 800\n",
      "  num_env_steps_sampled: 800\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 800\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.177777777777777\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1443217098393532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 735.7455710060235\n",
      "    mean_inference_ms: 1.2405529446476746\n",
      "    mean_raw_obs_processing_ms: 2.8693673826023196\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.04683016735673588\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -26.93401744766001\n",
      "      - -0.01035233277275216\n",
      "      - -0.033174567956145484\n",
      "      - 38.688538726889576\n",
      "      - -0.03659094262630891\n",
      "      - 0.08310995878716199\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1443217098393532\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 735.7455710060235\n",
      "      mean_inference_ms: 1.2405529446476746\n",
      "      mean_raw_obs_processing_ms: 2.8693673826023196\n",
      "  time_since_restore: 409.46875977516174\n",
      "  time_this_iter_s: 6.605703830718994\n",
      "  time_total_s: 409.46875977516174\n",
      "  timers:\n",
      "    learn_throughput: 167.5\n",
      "    learn_time_ms: 47.761\n",
      "    load_throughput: 46687.675\n",
      "    load_time_ms: 0.171\n",
      "    training_iteration_time_ms: 3887.844\n",
      "    update_time_ms: 2.246\n",
      "  timestamp: 1657049533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800\n",
      "  training_iteration: 100\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 816\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 816\n",
      "    num_agent_steps_trained: 816\n",
      "    num_env_steps_sampled: 816\n",
      "    num_env_steps_trained: 816\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.2593789109232135\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 272\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.888609169759553e-32\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3849509954452515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2120070095988922e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.5000454187393188\n",
      "          total_loss: 5.170994758605957\n",
      "          vf_explained_var: 0.006531560327857733\n",
      "          vf_loss: 4.670949459075928\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 816\n",
      "    num_agent_steps_trained: 816\n",
      "    num_env_steps_sampled: 816\n",
      "    num_env_steps_trained: 816\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 816\n",
      "  num_agent_steps_trained: 816\n",
      "  num_env_steps_sampled: 816\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 816\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.35\n",
      "    ram_util_percent: 39.574999999999996\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14440333669956654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.9983352243315\n",
      "    mean_inference_ms: 1.2390056381608163\n",
      "    mean_raw_obs_processing_ms: 2.8658605429938206\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.2593789109232135\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.05624998606459242\n",
      "      - 34.99899288604231\n",
      "      - -10.884111939654534\n",
      "      - 0.07527646052224313\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14440333669956654\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.9983352243315\n",
      "      mean_inference_ms: 1.2390056381608163\n",
      "      mean_raw_obs_processing_ms: 2.8658605429938206\n",
      "  time_since_restore: 414.86360812187195\n",
      "  time_this_iter_s: 2.3973217010498047\n",
      "  time_total_s: 414.86360812187195\n",
      "  timers:\n",
      "    learn_throughput: 168.932\n",
      "    learn_time_ms: 47.356\n",
      "    load_throughput: 44942.984\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3626.566\n",
      "    update_time_ms: 2.205\n",
      "  timestamp: 1657049538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 816\n",
      "  training_iteration: 102\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:18 (running for 00:07:14.00)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         414.864</td><td style=\"text-align: right;\"> 816</td><td style=\"text-align: right;\">0.259379</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 832\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 832\n",
      "    num_agent_steps_trained: 832\n",
      "    num_env_steps_sampled: 832\n",
      "    num_env_steps_trained: 832\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-25\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.2696640962908772\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 276\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9721522924398883e-32\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385023593902588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.228035666325013e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.6011038422584534\n",
      "          total_loss: 5.3993754386901855\n",
      "          vf_explained_var: -0.007451708894222975\n",
      "          vf_loss: 6.0004801750183105\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 832\n",
      "    num_agent_steps_trained: 832\n",
      "    num_env_steps_sampled: 832\n",
      "    num_env_steps_trained: 832\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 832\n",
      "  num_agent_steps_trained: 832\n",
      "  num_env_steps_sampled: 832\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 832\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.420000000000002\n",
      "    ram_util_percent: 39.54\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14440530950096325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.2494741825785\n",
      "    mean_inference_ms: 1.2377142017935707\n",
      "    mean_raw_obs_processing_ms: 2.835473408841864\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.2696640962908772\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -5.7631134002278905\n",
      "      - 4.422101461388063\n",
      "      - -3.4181567714490235\n",
      "      - -2.345488027112154\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14440530950096325\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.2494741825785\n",
      "      mean_inference_ms: 1.2377142017935707\n",
      "      mean_raw_obs_processing_ms: 2.835473408841864\n",
      "  time_since_restore: 421.24844765663147\n",
      "  time_this_iter_s: 3.672170639038086\n",
      "  time_total_s: 421.24844765663147\n",
      "  timers:\n",
      "    learn_throughput: 170.274\n",
      "    learn_time_ms: 46.983\n",
      "    load_throughput: 43018.503\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3480.523\n",
      "    update_time_ms: 2.141\n",
      "  timestamp: 1657049545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 832\n",
      "  training_iteration: 104\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:25 (running for 00:07:20.45)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         421.248</td><td style=\"text-align: right;\"> 832</td><td style=\"text-align: right;\">-0.269664</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:30 (running for 00:07:25.49)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         421.248</td><td style=\"text-align: right;\"> 832</td><td style=\"text-align: right;\">-0.269664</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 840\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 840\n",
      "    num_agent_steps_trained: 840\n",
      "    num_env_steps_sampled: 840\n",
      "    num_env_steps_trained: 840\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.2487974399230081\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 280\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.007466559791082572\n",
      "    episode_reward_mean: -0.007466559791082572\n",
      "    episode_reward_min: -0.007466559791082572\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.007466559791082572\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1432672142982483\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 886.2363547086716\n",
      "      mean_inference_ms: 1.8194913864135742\n",
      "      mean_raw_obs_processing_ms: 0.6873682141304016\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.860761462199441e-33\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3855174779891968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.650998562283348e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.1792127788066864\n",
      "          total_loss: 8.930448532104492\n",
      "          vf_explained_var: 0.00048100153799168766\n",
      "          vf_loss: 9.109663009643555\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 840\n",
      "    num_agent_steps_trained: 840\n",
      "    num_env_steps_sampled: 840\n",
      "    num_env_steps_trained: 840\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 840\n",
      "  num_agent_steps_trained: 840\n",
      "  num_env_steps_sampled: 840\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 840\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.675\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1444002239796108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.5771563809017\n",
      "    mean_inference_ms: 1.2363160931203472\n",
      "    mean_raw_obs_processing_ms: 2.8060750831718075\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.2487974399230081\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 12.811995708844286\n",
      "      - -15.377388792412258\n",
      "      - -1.0970018935481605\n",
      "      - -30.305427944526343\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1444002239796108\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.5771563809017\n",
      "      mean_inference_ms: 1.2363160931203472\n",
      "      mean_raw_obs_processing_ms: 2.8060750831718075\n",
      "  time_since_restore: 426.54028272628784\n",
      "  time_this_iter_s: 5.291835069656372\n",
      "  time_total_s: 426.54028272628784\n",
      "  timers:\n",
      "    learn_throughput: 170.908\n",
      "    learn_time_ms: 46.809\n",
      "    load_throughput: 41384.351\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3279.113\n",
      "    update_time_ms: 2.131\n",
      "  timestamp: 1657049550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840\n",
      "  training_iteration: 105\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 856\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 856\n",
      "    num_agent_steps_trained: 856\n",
      "    num_env_steps_sampled: 856\n",
      "    num_env_steps_trained: 856\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.44204843906870833\n",
      "  episode_reward_min: -39.50620098288872\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 284\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4651903655498604e-33\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854691982269287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.29488682129886e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.23525093495845795\n",
      "          total_loss: 3.0015578269958496\n",
      "          vf_explained_var: -0.003358304500579834\n",
      "          vf_loss: 3.2368085384368896\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 856\n",
      "    num_agent_steps_trained: 856\n",
      "    num_env_steps_sampled: 856\n",
      "    num_env_steps_trained: 856\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 856\n",
      "  num_agent_steps_trained: 856\n",
      "  num_env_steps_sampled: 856\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 856\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.799999999999997\n",
      "    ram_util_percent: 39.575\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14436678857084406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.9936775547372\n",
      "    mean_inference_ms: 1.2346811741180455\n",
      "    mean_raw_obs_processing_ms: 2.777527065452217\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.44204843906870833\n",
      "    episode_reward_min: -39.50620098288872\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 6.181487388110881\n",
      "      - 32.93518482636447\n",
      "      - 9.985690351014387\n",
      "      - 2.8701031311516445\n",
      "      - -39.50620098288872\n",
      "      - -1.3449055953668099\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14436678857084406\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.9936775547372\n",
      "      mean_inference_ms: 1.2346811741180455\n",
      "      mean_raw_obs_processing_ms: 2.777527065452217\n",
      "  time_since_restore: 432.54640674591064\n",
      "  time_this_iter_s: 2.7402503490448\n",
      "  time_total_s: 432.54640674591064\n",
      "  timers:\n",
      "    learn_throughput: 169.356\n",
      "    learn_time_ms: 47.238\n",
      "    load_throughput: 40850.295\n",
      "    load_time_ms: 0.196\n",
      "    training_iteration_time_ms: 3098.778\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1657049556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 856\n",
      "  training_iteration: 107\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:36 (running for 00:07:31.91)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         432.546</td><td style=\"text-align: right;\"> 856</td><td style=\"text-align: right;\">0.442048</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.5062</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 872\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 872\n",
      "    num_agent_steps_trained: 872\n",
      "    num_env_steps_sampled: 872\n",
      "    num_env_steps_trained: 872\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.05379647445831992\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 290\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.162975913874651e-34\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.384265661239624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.254780146264238e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.2936437427997589\n",
      "          total_loss: 5.310583114624023\n",
      "          vf_explained_var: 0.008448241278529167\n",
      "          vf_loss: 5.016939163208008\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 872\n",
      "    num_agent_steps_trained: 872\n",
      "    num_env_steps_sampled: 872\n",
      "    num_env_steps_trained: 872\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 872\n",
      "  num_agent_steps_trained: 872\n",
      "  num_env_steps_sampled: 872\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 872\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.919999999999998\n",
      "    ram_util_percent: 39.559999999999995\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14426469185703206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.999413549599\n",
      "    mean_inference_ms: 1.232054104069193\n",
      "    mean_raw_obs_processing_ms: 2.697809571245314\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.05379647445831992\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -3.169541741091251\n",
      "      - -32.642654163592205\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14426469185703206\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.999413549599\n",
      "      mean_inference_ms: 1.232054104069193\n",
      "      mean_raw_obs_processing_ms: 2.697809571245314\n",
      "  time_since_restore: 438.3674352169037\n",
      "  time_this_iter_s: 3.185408115386963\n",
      "  time_total_s: 438.3674352169037\n",
      "  timers:\n",
      "    learn_throughput: 167.639\n",
      "    learn_time_ms: 47.722\n",
      "    load_throughput: 35142.891\n",
      "    load_time_ms: 0.228\n",
      "    training_iteration_time_ms: 2992.136\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1657049562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 872\n",
      "  training_iteration: 109\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:42 (running for 00:07:37.86)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         438.367</td><td style=\"text-align: right;\"> 872</td><td style=\"text-align: right;\">-0.0537965</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:47 (running for 00:07:42.87)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         438.367</td><td style=\"text-align: right;\"> 872</td><td style=\"text-align: right;\">-0.0537965</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 880\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 880\n",
      "    num_agent_steps_trained: 880\n",
      "    num_env_steps_sampled: 880\n",
      "    num_env_steps_trained: 880\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.675677348959817\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 292\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.006710444063893872\n",
      "    episode_reward_mean: 0.006710444063893872\n",
      "    episode_reward_min: 0.006710444063893872\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.006710444063893872\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14548159357327134\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 898.8445623597103\n",
      "      mean_inference_ms: 1.8284214076711172\n",
      "      mean_raw_obs_processing_ms: 0.689164916081215\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.0814879569373254e-34\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3841865062713623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.031318783541792e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.08343223482370377\n",
      "          total_loss: 4.378530502319336\n",
      "          vf_explained_var: -0.0034437933936715126\n",
      "          vf_loss: 4.4619622230529785\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 880\n",
      "    num_agent_steps_trained: 880\n",
      "    num_env_steps_sampled: 880\n",
      "    num_env_steps_trained: 880\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 880\n",
      "  num_agent_steps_trained: 880\n",
      "  num_env_steps_sampled: 880\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 880\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.033333333333335\n",
      "    ram_util_percent: 39.57777777777778\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14434049532405713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 739.9025997036022\n",
      "    mean_inference_ms: 1.2318142895565969\n",
      "    mean_raw_obs_processing_ms: 2.7235764319761\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.675677348959817\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3445535782890357\n",
      "      - -0.014938583809950945\n",
      "      - 0.024231723540299566\n",
      "      - -0.010527204532947754\n",
      "      - -0.024371491137193058\n",
      "      - -0.008463074100732992\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14434049532405713\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 739.9025997036022\n",
      "      mean_inference_ms: 1.2318142895565969\n",
      "      mean_raw_obs_processing_ms: 2.7235764319761\n",
      "  time_since_restore: 444.68026661872864\n",
      "  time_this_iter_s: 6.312831401824951\n",
      "  time_total_s: 444.68026661872864\n",
      "  timers:\n",
      "    learn_throughput: 167.683\n",
      "    learn_time_ms: 47.709\n",
      "    load_throughput: 35268.48\n",
      "    load_time_ms: 0.227\n",
      "    training_iteration_time_ms: 2880.854\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1657049569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 880\n",
      "  training_iteration: 110\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 896\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 896\n",
      "    num_agent_steps_trained: 896\n",
      "    num_env_steps_sampled: 896\n",
      "    num_env_steps_trained: 896\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.5258534706462266\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 298\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.703719892343314e-35\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3855743408203125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.551328048473806e-07\n",
      "          model: {}\n",
      "          policy_loss: 0.06445129215717316\n",
      "          total_loss: 9.208685874938965\n",
      "          vf_explained_var: -0.00589991407468915\n",
      "          vf_loss: 9.144234657287598\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 896\n",
      "    num_agent_steps_trained: 896\n",
      "    num_env_steps_sampled: 896\n",
      "    num_env_steps_trained: 896\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 896\n",
      "  num_agent_steps_trained: 896\n",
      "  num_env_steps_sampled: 896\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 896\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.1\n",
      "    ram_util_percent: 39.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14423581039186442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.5448491114654\n",
      "    mean_inference_ms: 1.22959188422415\n",
      "    mean_raw_obs_processing_ms: 2.6483979809200484\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.5258534706462266\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.010544742465338541\n",
      "      - 0.011005038967677572\n",
      "      - 0.08109577444518745\n",
      "      - -0.008819957504358822\n",
      "      - 0.020520899720275043\n",
      "      - -0.0022112627787610473\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14423581039186442\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.5448491114654\n",
      "      mean_inference_ms: 1.22959188422415\n",
      "      mean_raw_obs_processing_ms: 2.6483979809200484\n",
      "  time_since_restore: 449.8836364746094\n",
      "  time_this_iter_s: 2.7722744941711426\n",
      "  time_total_s: 449.8836364746094\n",
      "  timers:\n",
      "    learn_throughput: 169.077\n",
      "    learn_time_ms: 47.316\n",
      "    load_throughput: 30766.947\n",
      "    load_time_ms: 0.26\n",
      "    training_iteration_time_ms: 2861.519\n",
      "    update_time_ms: 2.098\n",
      "  timestamp: 1657049574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 896\n",
      "  training_iteration: 112\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:32:54 (running for 00:07:49.72)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         449.884</td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">0.525853</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 912\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 912\n",
      "    num_agent_steps_trained: 912\n",
      "    num_env_steps_sampled: 912\n",
      "    num_env_steps_trained: 912\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-00\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.0003007447146730635\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 304\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9259299730858284e-35\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3844172954559326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0178458978771232e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.05820029601454735\n",
      "          total_loss: 6.0608439445495605\n",
      "          vf_explained_var: 0.0016584635013714433\n",
      "          vf_loss: 6.0026421546936035\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 912\n",
      "    num_agent_steps_trained: 912\n",
      "    num_env_steps_sampled: 912\n",
      "    num_env_steps_trained: 912\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 912\n",
      "  num_agent_steps_trained: 912\n",
      "  num_env_steps_sampled: 912\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 912\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.740000000000002\n",
      "    ram_util_percent: 39.56\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14430874885614609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 740.1643015413334\n",
      "    mean_inference_ms: 1.228590602007438\n",
      "    mean_raw_obs_processing_ms: 2.6497409911754635\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.0003007447146730635\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.4430269558938584\n",
      "      - 1.3440612362635416\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14430874885614609\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 740.1643015413334\n",
      "      mean_inference_ms: 1.228590602007438\n",
      "      mean_raw_obs_processing_ms: 2.6497409911754635\n",
      "  time_since_restore: 455.6459126472473\n",
      "  time_this_iter_s: 3.4130120277404785\n",
      "  time_total_s: 455.6459126472473\n",
      "  timers:\n",
      "    learn_throughput: 167.697\n",
      "    learn_time_ms: 47.705\n",
      "    load_throughput: 33301.342\n",
      "    load_time_ms: 0.24\n",
      "    training_iteration_time_ms: 2799.167\n",
      "    update_time_ms: 2.208\n",
      "  timestamp: 1657049580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 912\n",
      "  training_iteration: 114\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:00 (running for 00:07:55.54)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         455.646</td><td style=\"text-align: right;\"> 912</td><td style=\"text-align: right;\">-0.000300745</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:05 (running for 00:08:00.58)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         455.646</td><td style=\"text-align: right;\"> 912</td><td style=\"text-align: right;\">-0.000300745</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 920\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 920\n",
      "    num_agent_steps_trained: 920\n",
      "    num_env_steps_sampled: 920\n",
      "    num_env_steps_trained: 920\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-05\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.4859580234017989\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 306\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.2984587937108258\n",
      "    episode_reward_mean: 1.2984587937108258\n",
      "    episode_reward_min: 1.2984587937108258\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.2984587937108258\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1469135284423828\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 900.0141961233957\n",
      "      mean_inference_ms: 1.7992394311087474\n",
      "      mean_raw_obs_processing_ms: 0.6847347531999861\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.629649865429142e-36\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3845477104187012\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.025686055480037e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.47275641560554504\n",
      "          total_loss: 5.557100296020508\n",
      "          vf_explained_var: 0.0004997153882868588\n",
      "          vf_loss: 6.029856204986572\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 920\n",
      "    num_agent_steps_trained: 920\n",
      "    num_env_steps_sampled: 920\n",
      "    num_env_steps_trained: 920\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 920\n",
      "  num_agent_steps_trained: 920\n",
      "  num_env_steps_sampled: 920\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 920\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.5625\n",
      "    ram_util_percent: 39.575\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14422909542512852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.4855345739421\n",
      "    mean_inference_ms: 1.2277967379827042\n",
      "    mean_raw_obs_processing_ms: 2.602274349280996\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.4859580234017989\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0069920770845747215\n",
      "      - 0.002963896063060023\n",
      "      - -1.3432851688557643\n",
      "      - 0.0006927633081285078\n",
      "      - 0.0004630740586952786\n",
      "      - 0.009111450406611099\n",
      "      - 1.29368567150272\n",
      "      - 29.37801342631505\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14422909542512852\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.4855345739421\n",
      "      mean_inference_ms: 1.2277967379827042\n",
      "      mean_raw_obs_processing_ms: 2.602274349280996\n",
      "  time_since_restore: 461.2286117076874\n",
      "  time_this_iter_s: 5.5826990604400635\n",
      "  time_total_s: 461.2286117076874\n",
      "  timers:\n",
      "    learn_throughput: 167.799\n",
      "    learn_time_ms: 47.676\n",
      "    load_throughput: 33951.666\n",
      "    load_time_ms: 0.236\n",
      "    training_iteration_time_ms: 2833.493\n",
      "    update_time_ms: 2.188\n",
      "  timestamp: 1657049585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 920\n",
      "  training_iteration: 115\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:11 (running for 00:08:06.15)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         466.127</td><td style=\"text-align: right;\"> 936</td><td style=\"text-align: right;\">0.613346</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 944\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 944\n",
      "    num_agent_steps_trained: 944\n",
      "    num_env_steps_sampled: 944\n",
      "    num_env_steps_trained: 944\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.2930233218861914\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 314\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2037062331786428e-36\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3842031955718994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7990909327636473e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.3397553861141205\n",
      "          total_loss: 4.3583197593688965\n",
      "          vf_explained_var: 0.0043681105598807335\n",
      "          vf_loss: 4.018564224243164\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 944\n",
      "    num_agent_steps_trained: 944\n",
      "    num_env_steps_sampled: 944\n",
      "    num_env_steps_trained: 944\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 944\n",
      "  num_agent_steps_trained: 944\n",
      "  num_env_steps_sampled: 944\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 944\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.416666666666664\n",
      "    ram_util_percent: 39.58333333333333\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14422391835864726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 737.814509342069\n",
      "    mean_inference_ms: 1.2262917941288918\n",
      "    mean_raw_obs_processing_ms: 2.559178751711002\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.2930233218861914\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.005557379456999523\n",
      "      - -0.0019728665718654614\n",
      "      - 4.800799166528957\n",
      "      - 1.3552900352553938\n",
      "      - -34.05002564660673\n",
      "      - -0.013292683072359868\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14422391835864726\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 737.814509342069\n",
      "      mean_inference_ms: 1.2262917941288918\n",
      "      mean_raw_obs_processing_ms: 2.559178751711002\n",
      "  time_since_restore: 469.84895968437195\n",
      "  time_this_iter_s: 3.721919298171997\n",
      "  time_total_s: 469.84895968437195\n",
      "  timers:\n",
      "    learn_throughput: 160.998\n",
      "    learn_time_ms: 49.69\n",
      "    load_throughput: 31926.196\n",
      "    load_time_ms: 0.251\n",
      "    training_iteration_time_ms: 2831.418\n",
      "    update_time_ms: 2.321\n",
      "  timestamp: 1657049594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 944\n",
      "  training_iteration: 118\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:18 (running for 00:08:13.62)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         473.474</td><td style=\"text-align: right;\"> 952</td><td style=\"text-align: right;\">-0.292635</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:23 (running for 00:08:18.62)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         473.474</td><td style=\"text-align: right;\"> 952</td><td style=\"text-align: right;\">-0.292635</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 960\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 960\n",
      "    num_agent_steps_trained: 960\n",
      "    num_env_steps_sampled: 960\n",
      "    num_env_steps_trained: 960\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.013660952852258133\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 320\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.004445921062639524\n",
      "    episode_reward_mean: -0.004445921062639524\n",
      "    episode_reward_min: -0.004445921062639524\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.004445921062639524\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14852171074854184\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 903.2061655227452\n",
      "      mean_inference_ms: 1.7690821869732583\n",
      "      mean_raw_obs_processing_ms: 0.68186733820667\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.009265582946607e-37\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385109543800354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.2607632571598515e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.008095446974039078\n",
      "          total_loss: 3.9925270080566406\n",
      "          vf_explained_var: -0.0024827043525874615\n",
      "          vf_loss: 4.000622749328613\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 960\n",
      "    num_agent_steps_trained: 960\n",
      "    num_env_steps_sampled: 960\n",
      "    num_env_steps_trained: 960\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 960\n",
      "  num_agent_steps_trained: 960\n",
      "  num_env_steps_sampled: 960\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 960\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.855555555555554\n",
      "    ram_util_percent: 39.53333333333333\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1443037070123651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.5850539131559\n",
      "    mean_inference_ms: 1.2256465272284458\n",
      "    mean_raw_obs_processing_ms: 2.5622823148608784\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.013660952852258133\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 34.29158972933906\n",
      "      - -34.28205144561599\n",
      "      - -1.3884746653397024\n",
      "      - 0.004829145289795833\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1443037070123651\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.5850539131559\n",
      "      mean_inference_ms: 1.2256465272284458\n",
      "      mean_raw_obs_processing_ms: 2.5622823148608784\n",
      "  time_since_restore: 479.8323519229889\n",
      "  time_this_iter_s: 6.358512878417969\n",
      "  time_total_s: 479.8323519229889\n",
      "  timers:\n",
      "    learn_throughput: 163.775\n",
      "    learn_time_ms: 48.847\n",
      "    load_throughput: 35150.254\n",
      "    load_time_ms: 0.228\n",
      "    training_iteration_time_ms: 2937.474\n",
      "    update_time_ms: 2.299\n",
      "  timestamp: 1657049604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 960\n",
      "  training_iteration: 120\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:29 (running for 00:08:24.71)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         484.479</td><td style=\"text-align: right;\"> 968</td><td style=\"text-align: right;\">-0.013947</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 976\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 976\n",
      "    num_agent_steps_trained: 976\n",
      "    num_env_steps_sampled: 976\n",
      "    num_env_steps_trained: 976\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.0003869437157687605\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 324\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.523163957366517e-38\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3850499391555786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.630405965144746e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.24368511140346527\n",
      "          total_loss: 1.7639400959014893\n",
      "          vf_explained_var: 0.0030248602852225304\n",
      "          vf_loss: 2.007625102996826\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 976\n",
      "    num_agent_steps_trained: 976\n",
      "    num_env_steps_sampled: 976\n",
      "    num_env_steps_trained: 976\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 976\n",
      "  num_agent_steps_trained: 976\n",
      "  num_env_steps_sampled: 976\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 976\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.98\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.144315065931944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 738.1276054263976\n",
      "    mean_inference_ms: 1.2250584996182732\n",
      "    mean_raw_obs_processing_ms: 2.542271728831179\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.0003869437157687605\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03977561890055892\n",
      "      - -0.007600180475828999\n",
      "      - -1.4343510524779641\n",
      "      - 1.3628115286135565\n",
      "      - -0.011134359841272312\n",
      "      - 38.04364730257988\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.144315065931944\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 738.1276054263976\n",
      "      mean_inference_ms: 1.2250584996182732\n",
      "      mean_raw_obs_processing_ms: 2.542271728831179\n",
      "  time_since_restore: 487.84200739860535\n",
      "  time_this_iter_s: 3.3632142543792725\n",
      "  time_total_s: 487.84200739860535\n",
      "  timers:\n",
      "    learn_throughput: 161.212\n",
      "    learn_time_ms: 49.624\n",
      "    load_throughput: 41517.486\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3217.697\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1657049612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 976\n",
      "  training_iteration: 122\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:36 (running for 00:08:31.17)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         490.887</td><td style=\"text-align: right;\"> 984</td><td style=\"text-align: right;\">0.269511</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 992\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 992\n",
      "    num_agent_steps_trained: 992\n",
      "    num_env_steps_sampled: 992\n",
      "    num_env_steps_trained: 992\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-38\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.25673613690846886\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 330\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8807909893416293e-38\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3836473226547241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.486261322104838e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.38144752383232117\n",
      "          total_loss: 7.44150972366333\n",
      "          vf_explained_var: 0.005577270407229662\n",
      "          vf_loss: 7.060061454772949\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 992\n",
      "    num_agent_steps_trained: 992\n",
      "    num_env_steps_sampled: 992\n",
      "    num_env_steps_trained: 992\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 992\n",
      "  num_agent_steps_trained: 992\n",
      "  num_env_steps_sampled: 992\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 992\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.65\n",
      "    ram_util_percent: 39.925\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14426437826905525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 735.8450190645988\n",
      "    mean_inference_ms: 1.223961284638697\n",
      "    mean_raw_obs_processing_ms: 2.481194893418809\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.25673613690846886\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.02170894878815366\n",
      "      - 0.09522015246395865\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14426437826905525\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 735.8450190645988\n",
      "      mean_inference_ms: 1.223961284638697\n",
      "      mean_raw_obs_processing_ms: 2.481194893418809\n",
      "  time_since_restore: 493.6507124900818\n",
      "  time_this_iter_s: 2.7634401321411133\n",
      "  time_total_s: 493.6507124900818\n",
      "  timers:\n",
      "    learn_throughput: 163.313\n",
      "    learn_time_ms: 48.986\n",
      "    load_throughput: 41943.04\n",
      "    load_time_ms: 0.191\n",
      "    training_iteration_time_ms: 3222.305\n",
      "    update_time_ms: 2.287\n",
      "  timestamp: 1657049618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 992\n",
      "  training_iteration: 124\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_env_steps_sampled: 1000\n",
      "    num_env_steps_trained: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-43\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.2622720015897964\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 332\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.0002814061622675723\n",
      "    episode_reward_mean: 0.0002814061622675723\n",
      "    episode_reward_min: 0.0002814061622675723\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0002814061622675723\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14781010778326736\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 900.4403478220889\n",
      "      mean_inference_ms: 1.7419112356085524\n",
      "      mean_raw_obs_processing_ms: 0.682084183943899\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3840261697769165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0619567041867413e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.15475991368293762\n",
      "          total_loss: 1.8529651165008545\n",
      "          vf_explained_var: 0.001138498424552381\n",
      "          vf_loss: 2.007725238800049\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_env_steps_sampled: 1000\n",
      "    num_env_steps_trained: 1000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1000\n",
      "  num_agent_steps_trained: 1000\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1000\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.185714285714287\n",
      "    ram_util_percent: 40.300000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14433672333378572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 736.8470399762487\n",
      "    mean_inference_ms: 1.2241682113727463\n",
      "    mean_raw_obs_processing_ms: 2.5040587829953527\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.2622720015897964\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -37.93069081274812\n",
      "      - 38.341749567642886\n",
      "      - -0.06321386201668933\n",
      "      - 0.3303102885982625\n",
      "      - -0.02630584258448354\n",
      "      - 1.4340542515717394\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14433672333378572\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 736.8470399762487\n",
      "      mean_inference_ms: 1.2241682113727463\n",
      "      mean_raw_obs_processing_ms: 2.5040587829953527\n",
      "  time_since_restore: 498.63107895851135\n",
      "  time_this_iter_s: 4.980366468429565\n",
      "  time_total_s: 498.63107895851135\n",
      "  timers:\n",
      "    learn_throughput: 162.184\n",
      "    learn_time_ms: 49.327\n",
      "    load_throughput: 41020.088\n",
      "    load_time_ms: 0.195\n",
      "    training_iteration_time_ms: 3190.008\n",
      "    update_time_ms: 2.386\n",
      "  timestamp: 1657049623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 125\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:43 (running for 00:08:39.04)<br>Memory usage on this node: 12.4/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         498.631</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.262272</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1016\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1016\n",
      "    num_agent_steps_trained: 1016\n",
      "    num_env_steps_sampled: 1016\n",
      "    num_env_steps_trained: 1016\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: -0.11020564649302596\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 338\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3842623233795166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.1413165894919075e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5494164228439331\n",
      "          total_loss: 4.550925254821777\n",
      "          vf_explained_var: -0.010136513039469719\n",
      "          vf_loss: 4.001509189605713\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1016\n",
      "    num_agent_steps_trained: 1016\n",
      "    num_env_steps_sampled: 1016\n",
      "    num_env_steps_trained: 1016\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1016\n",
      "  num_agent_steps_trained: 1016\n",
      "  num_env_steps_sampled: 1016\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1016\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.12\n",
      "    ram_util_percent: 40.08\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14430760621225644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.4860110887711\n",
      "    mean_inference_ms: 1.223416876525021\n",
      "    mean_raw_obs_processing_ms: 2.445968203264056\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: -0.11020564649302596\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -38.72868414232357\n",
      "      - -0.06471584675896036\n",
      "      - -0.07142536653463294\n",
      "      - -1.3575602425743405\n",
      "      - -0.00840022884857028\n",
      "      - 0.06742240292792467\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14430760621225644\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.4860110887711\n",
      "      mean_inference_ms: 1.223416876525021\n",
      "      mean_raw_obs_processing_ms: 2.445968203264056\n",
      "  time_since_restore: 505.3939528465271\n",
      "  time_this_iter_s: 3.775973081588745\n",
      "  time_total_s: 505.3939528465271\n",
      "  timers:\n",
      "    learn_throughput: 168.704\n",
      "    learn_time_ms: 47.42\n",
      "    load_throughput: 42436.363\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3376.423\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1657049630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1016\n",
      "  training_iteration: 127\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:50 (running for 00:08:45.85)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         505.394</td><td style=\"text-align: right;\">1016</td><td style=\"text-align: right;\">-0.110206</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1032\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1032\n",
      "    num_agent_steps_trained: 1032\n",
      "    num_env_steps_sampled: 1032\n",
      "    num_env_steps_trained: 1032\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-33-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.10377045214865\n",
      "  episode_reward_mean: 0.3077563734926253\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 344\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3848457336425781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3260533933134866e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.17058314383029938\n",
      "          total_loss: 0.21442262828350067\n",
      "          vf_explained_var: 0.07248704880475998\n",
      "          vf_loss: 0.38500574231147766\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1032\n",
      "    num_agent_steps_trained: 1032\n",
      "    num_env_steps_sampled: 1032\n",
      "    num_env_steps_trained: 1032\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1032\n",
      "  num_agent_steps_trained: 1032\n",
      "  num_env_steps_sampled: 1032\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1032\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.419999999999998\n",
      "    ram_util_percent: 39.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14441357711060768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 734.6059574260468\n",
      "    mean_inference_ms: 1.2235647743487925\n",
      "    mean_raw_obs_processing_ms: 2.451557321260477\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.10377045214865\n",
      "    episode_reward_mean: 0.3077563734926253\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 39.10377045214865\n",
      "      - 0.00040339252212362453\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14441357711060768\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 734.6059574260468\n",
      "      mean_inference_ms: 1.2235647743487925\n",
      "      mean_raw_obs_processing_ms: 2.451557321260477\n",
      "  time_since_restore: 512.008763551712\n",
      "  time_this_iter_s: 3.385637044906616\n",
      "  time_total_s: 512.008763551712\n",
      "  timers:\n",
      "    learn_throughput: 165.97\n",
      "    learn_time_ms: 48.202\n",
      "    load_throughput: 41922.079\n",
      "    load_time_ms: 0.191\n",
      "    training_iteration_time_ms: 3303.427\n",
      "    update_time_ms: 2.345\n",
      "  timestamp: 1657049637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1032\n",
      "  training_iteration: 129\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:33:57 (running for 00:08:52.54)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         512.009</td><td style=\"text-align: right;\">1032</td><td style=\"text-align: right;\">0.307756</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:02 (running for 00:08:57.55)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         512.009</td><td style=\"text-align: right;\">1032</td><td style=\"text-align: right;\">0.307756</td><td style=\"text-align: right;\">             39.1038</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1040\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1040\n",
      "    num_agent_steps_trained: 1040\n",
      "    num_env_steps_sampled: 1040\n",
      "    num_env_steps_trained: 1040\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: -0.12860003103405807\n",
      "  episode_reward_min: -39.465538815414156\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 346\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.06230683048111341\n",
      "    episode_reward_mean: -0.06230683048111341\n",
      "    episode_reward_min: -0.06230683048111341\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.06230683048111341\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14705597599850426\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 899.6483494963827\n",
      "      mean_inference_ms: 1.7183732382858856\n",
      "      mean_raw_obs_processing_ms: 0.679257549817049\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854862451553345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.244401684947661e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.03233659639954567\n",
      "          total_loss: 0.9049160480499268\n",
      "          vf_explained_var: -0.0034142215736210346\n",
      "          vf_loss: 0.9372524619102478\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1040\n",
      "    num_agent_steps_trained: 1040\n",
      "    num_env_steps_sampled: 1040\n",
      "    num_env_steps_trained: 1040\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1040\n",
      "  num_agent_steps_trained: 1040\n",
      "  num_env_steps_sampled: 1040\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1040\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.190000000000001\n",
      "    ram_util_percent: 39.510000000000005\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14436622226576817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 733.0190412854986\n",
      "    mean_inference_ms: 1.2231250011685069\n",
      "    mean_raw_obs_processing_ms: 2.4131163096438173\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: -0.12860003103405807\n",
      "    episode_reward_min: -39.465538815414156\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.057711435321622706\n",
      "      - 0.00923268006583089\n",
      "      - 0.45558452908601055\n",
      "      - -0.006798499254475843\n",
      "      - -39.465538815414156\n",
      "      - 0.00550301250234464\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14436622226576817\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 733.0190412854986\n",
      "      mean_inference_ms: 1.2231250011685069\n",
      "      mean_raw_obs_processing_ms: 2.4131163096438173\n",
      "  time_since_restore: 519.0778832435608\n",
      "  time_this_iter_s: 7.069119691848755\n",
      "  time_total_s: 519.0778832435608\n",
      "  timers:\n",
      "    learn_throughput: 164.066\n",
      "    learn_time_ms: 48.761\n",
      "    load_throughput: 42074.523\n",
      "    load_time_ms: 0.19\n",
      "    training_iteration_time_ms: 3404.067\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1657049644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1040\n",
      "  training_iteration: 130\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:08 (running for 00:09:03.59)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         522.981</td><td style=\"text-align: right;\">1048</td><td style=\"text-align: right;\">-0.0893415</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.4655</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1056\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1056\n",
      "    num_agent_steps_trained: 1056\n",
      "    num_env_steps_sampled: 1056\n",
      "    num_env_steps_trained: 1056\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: 0.6427911323660048\n",
      "  episode_reward_min: -38.43775633427545\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 352\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3847283124923706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.304989256022964e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.15893785655498505\n",
      "          total_loss: 4.336456298828125\n",
      "          vf_explained_var: 0.002164218807592988\n",
      "          vf_loss: 4.495393753051758\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1056\n",
      "    num_agent_steps_trained: 1056\n",
      "    num_env_steps_sampled: 1056\n",
      "    num_env_steps_trained: 1056\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1056\n",
      "  num_agent_steps_trained: 1056\n",
      "  num_env_steps_sampled: 1056\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1056\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.25\n",
      "    ram_util_percent: 40.725\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14447481317496716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 732.8541664092915\n",
      "    mean_inference_ms: 1.223313690423689\n",
      "    mean_raw_obs_processing_ms: 2.419179547933902\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: 0.6427911323660048\n",
      "    episode_reward_min: -38.43775633427545\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.01890264080590942\n",
      "      - -1.4209290126594731\n",
      "      - 1.357110910243617\n",
      "      - 37.907347052062754\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14447481317496716\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 732.8541664092915\n",
      "      mean_inference_ms: 1.223313690423689\n",
      "      mean_raw_obs_processing_ms: 2.419179547933902\n",
      "  time_since_restore: 525.851007938385\n",
      "  time_this_iter_s: 2.8699347972869873\n",
      "  time_total_s: 525.851007938385\n",
      "  timers:\n",
      "    learn_throughput: 168.88\n",
      "    learn_time_ms: 47.371\n",
      "    load_throughput: 45356.085\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3280.926\n",
      "    update_time_ms: 2.235\n",
      "  timestamp: 1657049651\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1056\n",
      "  training_iteration: 132\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:14 (running for 00:09:09.45)<br>Memory usage on this node: 12.4/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         528.748</td><td style=\"text-align: right;\">1064</td><td style=\"text-align: right;\">0.239307</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1072\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1072\n",
      "    num_agent_steps_trained: 1072\n",
      "    num_env_steps_sampled: 1072\n",
      "    num_env_steps_trained: 1072\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: 0.2867637835475283\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 356\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3851529359817505\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.477209121338092e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.1315872073173523\n",
      "          total_loss: 2.0154361724853516\n",
      "          vf_explained_var: 0.003321524476632476\n",
      "          vf_loss: 2.1470234394073486\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1072\n",
      "    num_agent_steps_trained: 1072\n",
      "    num_env_steps_sampled: 1072\n",
      "    num_env_steps_trained: 1072\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1072\n",
      "  num_agent_steps_trained: 1072\n",
      "  num_env_steps_sampled: 1072\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1072\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.400000000000002\n",
      "    ram_util_percent: 40.2\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14450781956575437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 731.8111575021106\n",
      "    mean_inference_ms: 1.2231878959563354\n",
      "    mean_raw_obs_processing_ms: 2.403683524994658\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: 0.2867637835475283\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0024688661097365117\n",
      "      - -8.678187073441023\n",
      "      - 1.403011504384857\n",
      "      - -0.49697762474182383\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14450781956575437\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 731.8111575021106\n",
      "      mean_inference_ms: 1.2231878959563354\n",
      "      mean_raw_obs_processing_ms: 2.403683524994658\n",
      "  time_since_restore: 530.9286599159241\n",
      "  time_this_iter_s: 2.1802127361297607\n",
      "  time_total_s: 530.9286599159241\n",
      "  timers:\n",
      "    learn_throughput: 163.695\n",
      "    learn_time_ms: 48.871\n",
      "    load_throughput: 41755.142\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 3207.643\n",
      "    update_time_ms: 2.445\n",
      "  timestamp: 1657049656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1072\n",
      "  training_iteration: 134\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:21 (running for 00:09:16.73)<br>Memory usage on this node: 12.4/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         530.929</td><td style=\"text-align: right;\">1072</td><td style=\"text-align: right;\">0.286764</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1080\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1080\n",
      "    num_agent_steps_trained: 1080\n",
      "    num_env_steps_sampled: 1080\n",
      "    num_env_steps_trained: 1080\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-21\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: 0.07803779303418874\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 360\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.06103859001091583\n",
      "    episode_reward_mean: 0.06103859001091583\n",
      "    episode_reward_min: 0.06103859001091583\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.06103859001091583\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14606917776712558\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 899.1692589550483\n",
      "      mean_inference_ms: 1.6966360371287277\n",
      "      mean_raw_obs_processing_ms: 0.6796063446417088\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3849161863327026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1804289644933306e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.2757098376750946\n",
      "          total_loss: 6.314890384674072\n",
      "          vf_explained_var: 0.003028867533430457\n",
      "          vf_loss: 6.039180278778076\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1080\n",
      "    num_agent_steps_trained: 1080\n",
      "    num_env_steps_sampled: 1080\n",
      "    num_env_steps_trained: 1080\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1080\n",
      "  num_agent_steps_trained: 1080\n",
      "  num_env_steps_sampled: 1080\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1080\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.685714285714287\n",
      "    ram_util_percent: 40.25714285714286\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1445345539085945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 730.703700225637\n",
      "    mean_inference_ms: 1.2230973848342408\n",
      "    mean_raw_obs_processing_ms: 2.3887123891190867\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: 0.07803779303418874\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.4001217023542765\n",
      "      - 0.002954093988525308\n",
      "      - -2.4405825140847703\n",
      "      - 6.799199933192064\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1445345539085945\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 730.703700225637\n",
      "      mean_inference_ms: 1.2230973848342408\n",
      "      mean_raw_obs_processing_ms: 2.3887123891190867\n",
      "  time_since_restore: 536.1544301509857\n",
      "  time_this_iter_s: 5.2257702350616455\n",
      "  time_total_s: 536.1544301509857\n",
      "  timers:\n",
      "    learn_throughput: 164.393\n",
      "    learn_time_ms: 48.664\n",
      "    load_throughput: 43554.559\n",
      "    load_time_ms: 0.184\n",
      "    training_iteration_time_ms: 3216.16\n",
      "    update_time_ms: 2.371\n",
      "  timestamp: 1657049661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1080\n",
      "  training_iteration: 135\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1096\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1096\n",
      "    num_agent_steps_trained: 1096\n",
      "    num_env_steps_sampled: 1096\n",
      "    num_env_steps_trained: 1096\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: 0.0034774260742750805\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 364\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3845158815383911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4191429045240511e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.33460932970046997\n",
      "          total_loss: 2.987245798110962\n",
      "          vf_explained_var: -0.009636648930609226\n",
      "          vf_loss: 3.3218557834625244\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1096\n",
      "    num_agent_steps_trained: 1096\n",
      "    num_env_steps_sampled: 1096\n",
      "    num_env_steps_trained: 1096\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1096\n",
      "  num_agent_steps_trained: 1096\n",
      "  num_env_steps_sampled: 1096\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1096\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.4\n",
      "    ram_util_percent: 40.475\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1445627922591973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 729.6045026631718\n",
      "    mean_inference_ms: 1.2230686968731654\n",
      "    mean_raw_obs_processing_ms: 2.37419166366974\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: 0.0034774260742750805\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.002867132555271823\n",
      "      - 0.06193464120640613\n",
      "      - 0.08086972487038568\n",
      "      - -0.02066912388057318\n",
      "      - 37.09526182738594\n",
      "      - -4.371180787112884\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1445627922591973\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 729.6045026631718\n",
      "      mean_inference_ms: 1.2230686968731654\n",
      "      mean_raw_obs_processing_ms: 2.37419166366974\n",
      "  time_since_restore: 542.8235640525818\n",
      "  time_this_iter_s: 3.123120069503784\n",
      "  time_total_s: 542.8235640525818\n",
      "  timers:\n",
      "    learn_throughput: 167.538\n",
      "    learn_time_ms: 47.75\n",
      "    load_throughput: 46294.746\n",
      "    load_time_ms: 0.173\n",
      "    training_iteration_time_ms: 3206.839\n",
      "    update_time_ms: 2.287\n",
      "  timestamp: 1657049668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1096\n",
      "  training_iteration: 137\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:28 (running for 00:09:23.74)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         542.824</td><td style=\"text-align: right;\">1096</td><td style=\"text-align: right;\">0.00347743</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1112\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1112\n",
      "    num_agent_steps_trained: 1112\n",
      "    num_env_steps_sampled: 1112\n",
      "    num_env_steps_trained: 1112\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-35\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: -0.39661374729061727\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 370\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3853185176849365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.221603265046724e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5086604952812195\n",
      "          total_loss: 4.51775598526001\n",
      "          vf_explained_var: 0.008103663101792336\n",
      "          vf_loss: 4.009095668792725\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1112\n",
      "    num_agent_steps_trained: 1112\n",
      "    num_env_steps_sampled: 1112\n",
      "    num_env_steps_trained: 1112\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1112\n",
      "  num_agent_steps_trained: 1112\n",
      "  num_env_steps_sampled: 1112\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1112\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.24\n",
      "    ram_util_percent: 41.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1445310603554102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.8410060890207\n",
      "    mean_inference_ms: 1.2226091865949484\n",
      "    mean_raw_obs_processing_ms: 2.3256126769147603\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: -0.39661374729061727\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -2.4896171080586385\n",
      "      - 2.717723218105057\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1445310603554102\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.8410060890207\n",
      "      mean_inference_ms: 1.2226091865949484\n",
      "      mean_raw_obs_processing_ms: 2.3256126769147603\n",
      "  time_since_restore: 549.596551656723\n",
      "  time_this_iter_s: 3.041823148727417\n",
      "  time_total_s: 549.596551656723\n",
      "  timers:\n",
      "    learn_throughput: 169.948\n",
      "    learn_time_ms: 47.073\n",
      "    load_throughput: 46850.645\n",
      "    load_time_ms: 0.171\n",
      "    training_iteration_time_ms: 3222.598\n",
      "    update_time_ms: 2.294\n",
      "  timestamp: 1657049675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1112\n",
      "  training_iteration: 139\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:35 (running for 00:09:30.57)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         549.597</td><td style=\"text-align: right;\">1112</td><td style=\"text-align: right;\">-0.396614</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:40 (running for 00:09:35.60)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         549.597</td><td style=\"text-align: right;\">1112</td><td style=\"text-align: right;\">-0.396614</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1120\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1120\n",
      "    num_agent_steps_trained: 1120\n",
      "    num_env_steps_sampled: 1120\n",
      "    num_env_steps_trained: 1120\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: -0.050346775109528966\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 372\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.03357035873318992\n",
      "    episode_reward_mean: -0.03357035873318992\n",
      "    episode_reward_min: -0.03357035873318992\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03357035873318992\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14569338630227482\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 897.7449417114259\n",
      "      mean_inference_ms: 1.6772550695082722\n",
      "      mean_raw_obs_processing_ms: 0.6772237665512983\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3852015733718872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.928097157768207e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.2884170711040497\n",
      "          total_loss: 0.9344199895858765\n",
      "          vf_explained_var: 0.03385918214917183\n",
      "          vf_loss: 0.6460029482841492\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1120\n",
      "    num_agent_steps_trained: 1120\n",
      "    num_env_steps_sampled: 1120\n",
      "    num_env_steps_trained: 1120\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1120\n",
      "  num_agent_steps_trained: 1120\n",
      "  num_env_steps_sampled: 1120\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1120\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.575000000000003\n",
      "    ram_util_percent: 41.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14459547552772276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.5127377930196\n",
      "    mean_inference_ms: 1.2229292521151753\n",
      "    mean_raw_obs_processing_ms: 2.346292662831224\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: -0.050346775109528966\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -31.40331633810602\n",
      "      - -4.7537532127298725\n",
      "      - 0.04924819693288418\n",
      "      - 7.449928025468541\n",
      "      - 38.43752923534623\n",
      "      - -38.43775633427545\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14459547552772276\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.5127377930196\n",
      "      mean_inference_ms: 1.2229292521151753\n",
      "      mean_raw_obs_processing_ms: 2.346292662831224\n",
      "  time_since_restore: 555.1010715961456\n",
      "  time_this_iter_s: 5.504519939422607\n",
      "  time_total_s: 555.1010715961456\n",
      "  timers:\n",
      "    learn_throughput: 172.859\n",
      "    learn_time_ms: 46.28\n",
      "    load_throughput: 48363.263\n",
      "    load_time_ms: 0.165\n",
      "    training_iteration_time_ms: 3072.291\n",
      "    update_time_ms: 2.307\n",
      "  timestamp: 1657049680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1120\n",
      "  training_iteration: 140\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1136\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1136\n",
      "    num_agent_steps_trained: 1136\n",
      "    num_env_steps_sampled: 1136\n",
      "    num_env_steps_trained: 1136\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-46\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.57952533515323\n",
      "  episode_reward_mean: 0.2268119982111327\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 378\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854091167449951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2430323295120616e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.466254323720932\n",
      "          total_loss: 3.546266794204712\n",
      "          vf_explained_var: -0.01626228168606758\n",
      "          vf_loss: 4.012520790100098\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1136\n",
      "    num_agent_steps_trained: 1136\n",
      "    num_env_steps_sampled: 1136\n",
      "    num_env_steps_trained: 1136\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1136\n",
      "  num_agent_steps_trained: 1136\n",
      "  num_env_steps_sampled: 1136\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1136\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.5\n",
      "    ram_util_percent: 41.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.144545709600444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.8456049951129\n",
      "    mean_inference_ms: 1.2222878069636798\n",
      "    mean_raw_obs_processing_ms: 2.29948360974147\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.57952533515323\n",
      "    episode_reward_mean: 0.2268119982111327\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.21900878249786615\n",
      "      - -5.236772784182726\n",
      "      - 35.11503793792261\n",
      "      - -30.97972782931376\n",
      "      - -7.598070466232901\n",
      "      - 38.57952533515323\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.144545709600444\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.8456049951129\n",
      "      mean_inference_ms: 1.2222878069636798\n",
      "      mean_raw_obs_processing_ms: 2.29948360974147\n",
      "  time_since_restore: 560.3833432197571\n",
      "  time_this_iter_s: 2.8195672035217285\n",
      "  time_total_s: 560.3833432197571\n",
      "  timers:\n",
      "    learn_throughput: 169.783\n",
      "    learn_time_ms: 47.119\n",
      "    load_throughput: 46837.566\n",
      "    load_time_ms: 0.171\n",
      "    training_iteration_time_ms: 2923.242\n",
      "    update_time_ms: 2.31\n",
      "  timestamp: 1657049686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1136\n",
      "  training_iteration: 142\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:46 (running for 00:09:41.48)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         560.383</td><td style=\"text-align: right;\">1136</td><td style=\"text-align: right;\">0.226812</td><td style=\"text-align: right;\">             38.5795</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1152\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1152\n",
      "    num_agent_steps_trained: 1152\n",
      "    num_env_steps_sampled: 1152\n",
      "    num_env_steps_trained: 1152\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.40637179397578327\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 384\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3852202892303467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.302287500948296e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.4026944041252136\n",
      "          total_loss: 2.403886079788208\n",
      "          vf_explained_var: 0.0037181635852903128\n",
      "          vf_loss: 2.0011918544769287\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1152\n",
      "    num_agent_steps_trained: 1152\n",
      "    num_env_steps_sampled: 1152\n",
      "    num_env_steps_trained: 1152\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1152\n",
      "  num_agent_steps_trained: 1152\n",
      "  num_env_steps_sampled: 1152\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1152\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.433333333333334\n",
      "    ram_util_percent: 40.583333333333336\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14462409024541742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.6463705448774\n",
      "    mean_inference_ms: 1.2224725144951298\n",
      "    mean_raw_obs_processing_ms: 2.30714167163289\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.40637179397578327\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -2.9300970037309995\n",
      "      - 6.711616332712751\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14462409024541742\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.6463705448774\n",
      "      mean_inference_ms: 1.2224725144951298\n",
      "      mean_raw_obs_processing_ms: 2.30714167163289\n",
      "  time_since_restore: 567.484032869339\n",
      "  time_this_iter_s: 3.7711946964263916\n",
      "  time_total_s: 567.484032869339\n",
      "  timers:\n",
      "    learn_throughput: 170.83\n",
      "    learn_time_ms: 46.83\n",
      "    load_throughput: 49446.555\n",
      "    load_time_ms: 0.162\n",
      "    training_iteration_time_ms: 3125.551\n",
      "    update_time_ms: 2.301\n",
      "  timestamp: 1657049693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1152\n",
      "  training_iteration: 144\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:53 (running for 00:09:48.68)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         567.484</td><td style=\"text-align: right;\">1152</td><td style=\"text-align: right;\">-0.406372</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:34:58 (running for 00:09:53.68)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         567.484</td><td style=\"text-align: right;\">1152</td><td style=\"text-align: right;\">-0.406372</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1160\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1160\n",
      "    num_agent_steps_trained: 1160\n",
      "    num_env_steps_sampled: 1160\n",
      "    num_env_steps_trained: 1160\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.3843345646780393\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 386\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.0038735660907911473\n",
      "    episode_reward_mean: -0.0038735660907911473\n",
      "    episode_reward_min: -0.0038735660907911473\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0038735660907911473\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14527006582780316\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 905.4532701318914\n",
      "      mean_inference_ms: 1.6581470316106623\n",
      "      mean_raw_obs_processing_ms: 0.6756538694555109\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3845386505126953\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.588890417129733e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.4632736146450043\n",
      "          total_loss: 4.4673357009887695\n",
      "          vf_explained_var: -0.005924833007156849\n",
      "          vf_loss: 4.004061698913574\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1160\n",
      "    num_agent_steps_trained: 1160\n",
      "    num_env_steps_sampled: 1160\n",
      "    num_env_steps_trained: 1160\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1160\n",
      "  num_agent_steps_trained: 1160\n",
      "  num_env_steps_sampled: 1160\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1160\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.13\n",
      "    ram_util_percent: 40.580000000000005\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14454495348837637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.0984402983988\n",
      "    mean_inference_ms: 1.2219438150576625\n",
      "    mean_raw_obs_processing_ms: 2.274788357846746\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.3843345646780393\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -6.213050257930355\n",
      "      - -1.1294801335409588\n",
      "      - -3.6788336073256893\n",
      "      - -31.223287564501756\n",
      "      - 0.13340882619610994\n",
      "      - 37.00177761093411\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14454495348837637\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.0984402983988\n",
      "      mean_inference_ms: 1.2219438150576625\n",
      "      mean_raw_obs_processing_ms: 2.274788357846746\n",
      "  time_since_restore: 574.6879358291626\n",
      "  time_this_iter_s: 7.203902959823608\n",
      "  time_total_s: 574.6879358291626\n",
      "  timers:\n",
      "    learn_throughput: 171.149\n",
      "    learn_time_ms: 46.743\n",
      "    load_throughput: 48412.108\n",
      "    load_time_ms: 0.165\n",
      "    training_iteration_time_ms: 3252.212\n",
      "    update_time_ms: 2.306\n",
      "  timestamp: 1657049700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1160\n",
      "  training_iteration: 145\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:04 (running for 00:09:59.75)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         578.487</td><td style=\"text-align: right;\">1168</td><td style=\"text-align: right;\">-0.63974</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1176\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1176\n",
      "    num_agent_steps_trained: 1176\n",
      "    num_env_steps_sampled: 1176\n",
      "    num_env_steps_trained: 1176\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-07\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.31794568978988624\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 392\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3850247859954834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.780995035005617e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5735819935798645\n",
      "          total_loss: 0.577116072177887\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.003534040180966258\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1176\n",
      "    num_agent_steps_trained: 1176\n",
      "    num_env_steps_sampled: 1176\n",
      "    num_env_steps_trained: 1176\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1176\n",
      "  num_agent_steps_trained: 1176\n",
      "  num_env_steps_sampled: 1176\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1176\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.3\n",
      "    ram_util_percent: 40.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14459630873982898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.1207818316308\n",
      "    mean_inference_ms: 1.2219184482558314\n",
      "    mean_raw_obs_processing_ms: 2.282520839468053\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.31794568978988624\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.754061731470383\n",
      "      - 2.5021877211868144\n",
      "      - -0.5529850238856753\n",
      "      - -5.878906166073705\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14459630873982898\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.1207818316308\n",
      "      mean_inference_ms: 1.2219184482558314\n",
      "      mean_raw_obs_processing_ms: 2.282520839468053\n",
      "  time_since_restore: 581.6767616271973\n",
      "  time_this_iter_s: 3.1898117065429688\n",
      "  time_total_s: 581.6767616271973\n",
      "  timers:\n",
      "    learn_throughput: 170.019\n",
      "    learn_time_ms: 47.054\n",
      "    load_throughput: 46294.746\n",
      "    load_time_ms: 0.173\n",
      "    training_iteration_time_ms: 3284.068\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1657049707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1176\n",
      "  training_iteration: 147\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:11 (running for 00:10:07.01)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         585.695</td><td style=\"text-align: right;\">1184</td><td style=\"text-align: right;\">-0.679855</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1192\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1192\n",
      "    num_agent_steps_trained: 1192\n",
      "    num_env_steps_sampled: 1192\n",
      "    num_env_steps_trained: 1192\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.2676784975371451\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 396\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3839095830917358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.688213150278898e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.42966222763061523\n",
      "          total_loss: 3.8048243522644043\n",
      "          vf_explained_var: 0.0011903762351721525\n",
      "          vf_loss: 4.2344865798950195\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1192\n",
      "    num_agent_steps_trained: 1192\n",
      "    num_env_steps_sampled: 1192\n",
      "    num_env_steps_trained: 1192\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1192\n",
      "  num_agent_steps_trained: 1192\n",
      "  num_env_steps_sampled: 1192\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1192\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.175\n",
      "    ram_util_percent: 40.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14458727807054886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.5220798377526\n",
      "    mean_inference_ms: 1.2216087616228162\n",
      "    mean_raw_obs_processing_ms: 2.2706742788256857\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.2676784975371451\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -7.90390189850406\n",
      "      - -1.0842357843634858\n",
      "      - 1.99337226352689\n",
      "      - 1.3172543524573967\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14458727807054886\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.5220798377526\n",
      "      mean_inference_ms: 1.2216087616228162\n",
      "      mean_raw_obs_processing_ms: 2.2706742788256857\n",
      "  time_since_restore: 588.7920153141022\n",
      "  time_this_iter_s: 3.097475528717041\n",
      "  time_total_s: 588.7920153141022\n",
      "  timers:\n",
      "    learn_throughput: 166.883\n",
      "    learn_time_ms: 47.938\n",
      "    load_throughput: 44284.588\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3318.328\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1657049714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1192\n",
      "  training_iteration: 149\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:20 (running for 00:10:15.19)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         588.792</td><td style=\"text-align: right;\">1192</td><td style=\"text-align: right;\">-0.267678</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1200\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1200\n",
      "    num_agent_steps_trained: 1200\n",
      "    num_env_steps_sampled: 1200\n",
      "    num_env_steps_trained: 1200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-21\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.1539116879514922\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 400\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.1252721674282582\n",
      "    episode_reward_mean: 0.1252721674282582\n",
      "    episode_reward_min: 0.1252721674282582\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.1252721674282582\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1448327368432349\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 899.8255153278728\n",
      "      mean_inference_ms: 1.644055921952803\n",
      "      mean_raw_obs_processing_ms: 0.6749210776863518\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3839590549468994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.902214525121963e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.1741580218076706\n",
      "          total_loss: 3.2550976276397705\n",
      "          vf_explained_var: -0.03520423546433449\n",
      "          vf_loss: 3.080939769744873\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1200\n",
      "    num_agent_steps_trained: 1200\n",
      "    num_env_steps_sampled: 1200\n",
      "    num_env_steps_trained: 1200\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1200\n",
      "  num_agent_steps_trained: 1200\n",
      "  num_env_steps_sampled: 1200\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1200\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.311111111111112\n",
      "    ram_util_percent: 40.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14455801588170147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.0074602850566\n",
      "    mean_inference_ms: 1.2210750708516265\n",
      "    mean_raw_obs_processing_ms: 2.2590774327835033\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.1539116879514922\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 5.728268614178237\n",
      "      - -29.359690084766303\n",
      "      - -32.19259470548311\n",
      "      - 0.01010325931227618\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14455801588170147\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.0074602850566\n",
      "      mean_inference_ms: 1.2210750708516265\n",
      "      mean_raw_obs_processing_ms: 2.2590774327835033\n",
      "  time_since_restore: 594.8839447498322\n",
      "  time_this_iter_s: 6.0919294357299805\n",
      "  time_total_s: 594.8839447498322\n",
      "  timers:\n",
      "    learn_throughput: 165.691\n",
      "    learn_time_ms: 48.283\n",
      "    load_throughput: 43262.548\n",
      "    load_time_ms: 0.185\n",
      "    training_iteration_time_ms: 3414.194\n",
      "    update_time_ms: 2.396\n",
      "  timestamp: 1657049721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1200\n",
      "  training_iteration: 150\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1216\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1216\n",
      "    num_agent_steps_trained: 1216\n",
      "    num_env_steps_sampled: 1216\n",
      "    num_env_steps_trained: 1216\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: 0.39453988344413105\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 404\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385026454925537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.32449893298326e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.2431996464729309\n",
      "          total_loss: 9.756800651550293\n",
      "          vf_explained_var: -0.0020949363242834806\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1216\n",
      "    num_agent_steps_trained: 1216\n",
      "    num_env_steps_sampled: 1216\n",
      "    num_env_steps_trained: 1216\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1216\n",
      "  num_agent_steps_trained: 1216\n",
      "  num_env_steps_sampled: 1216\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1216\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.3\n",
      "    ram_util_percent: 40.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14451799040654975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.5148607634685\n",
      "    mean_inference_ms: 1.2205097748434475\n",
      "    mean_raw_obs_processing_ms: 2.2477570231892186\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: 0.39453988344413105\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 29.726577831078345\n",
      "      - 18.800333260938523\n",
      "      - -2.7433861023806756\n",
      "      - 19.152590526659914\n",
      "      - -3.450674473947064\n",
      "      - 5.860004388613746\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14451799040654975\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.5148607634685\n",
      "      mean_inference_ms: 1.2205097748434475\n",
      "      mean_raw_obs_processing_ms: 2.2477570231892186\n",
      "  time_since_restore: 600.8579308986664\n",
      "  time_this_iter_s: 2.9319682121276855\n",
      "  time_total_s: 600.8579308986664\n",
      "  timers:\n",
      "    learn_throughput: 167.097\n",
      "    learn_time_ms: 47.876\n",
      "    load_throughput: 43879.21\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3482.808\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1657049727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1216\n",
      "  training_iteration: 152\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:27 (running for 00:10:22.42)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         600.858</td><td style=\"text-align: right;\">1216</td><td style=\"text-align: right;\"> 0.39454</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1232\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1232\n",
      "    num_agent_steps_trained: 1232\n",
      "    num_env_steps_sampled: 1232\n",
      "    num_env_steps_trained: 1232\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-32\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.12039640018785633\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 410\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3846625089645386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.545082902041031e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.4288715422153473\n",
      "          total_loss: 6.944626808166504\n",
      "          vf_explained_var: -0.002152907894924283\n",
      "          vf_loss: 6.515754699707031\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1232\n",
      "    num_agent_steps_trained: 1232\n",
      "    num_env_steps_sampled: 1232\n",
      "    num_env_steps_trained: 1232\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1232\n",
      "  num_agent_steps_trained: 1232\n",
      "  num_env_steps_sampled: 1232\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1232\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.833333333333332\n",
      "    ram_util_percent: 39.733333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14440936320738185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 719.8427024255423\n",
      "    mean_inference_ms: 1.2194983661595635\n",
      "    mean_raw_obs_processing_ms: 2.207231368226968\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.12039640018785633\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -5.442313537793382\n",
      "      - -1.9744255753607973\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14440936320738185\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 719.8427024255423\n",
      "      mean_inference_ms: 1.2194983661595635\n",
      "      mean_raw_obs_processing_ms: 2.207231368226968\n",
      "  time_since_restore: 606.2123320102692\n",
      "  time_this_iter_s: 2.730020046234131\n",
      "  time_total_s: 606.2123320102692\n",
      "  timers:\n",
      "    learn_throughput: 165.706\n",
      "    learn_time_ms: 48.278\n",
      "    load_throughput: 46065.942\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3307.978\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1657049732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1232\n",
      "  training_iteration: 154\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:32 (running for 00:10:27.82)<br>Memory usage on this node: 12.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         606.212</td><td style=\"text-align: right;\">1232</td><td style=\"text-align: right;\">-0.120396</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:37 (running for 00:10:32.85)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         606.212</td><td style=\"text-align: right;\">1232</td><td style=\"text-align: right;\">-0.120396</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1240\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1240\n",
      "    num_agent_steps_trained: 1240\n",
      "    num_env_steps_sampled: 1240\n",
      "    num_env_steps_trained: 1240\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.31522601960687596\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 412\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.13801043082796127\n",
      "    episode_reward_mean: -0.13801043082796127\n",
      "    episode_reward_min: -0.13801043082796127\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.13801043082796127\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14389322159138132\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 897.8349604505173\n",
      "      mean_inference_ms: 1.6291724874618205\n",
      "      mean_raw_obs_processing_ms: 0.6722237201447182\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3853222131729126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2705181688943412e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.18692289292812347\n",
      "          total_loss: 6.191149711608887\n",
      "          vf_explained_var: 0.00244221487082541\n",
      "          vf_loss: 6.004227638244629\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1240\n",
      "    num_agent_steps_trained: 1240\n",
      "    num_env_steps_sampled: 1240\n",
      "    num_env_steps_trained: 1240\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1240\n",
      "  num_agent_steps_trained: 1240\n",
      "  num_env_steps_sampled: 1240\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1240\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.94444444444444\n",
      "    ram_util_percent: 40.58888888888889\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14447417495636145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 720.6200286257839\n",
      "    mean_inference_ms: 1.2197391667797048\n",
      "    mean_raw_obs_processing_ms: 2.2260024721814613\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.31522601960687596\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -29.38286632669427\n",
      "      - -30.582410392182616\n",
      "      - -0.016092991666067924\n",
      "      - 0.04741149958680124\n",
      "      - 29.121010871945508\n",
      "      - -29.07129185013125\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14447417495636145\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 720.6200286257839\n",
      "      mean_inference_ms: 1.2197391667797048\n",
      "      mean_raw_obs_processing_ms: 2.2260024721814613\n",
      "  time_since_restore: 611.93909907341\n",
      "  time_this_iter_s: 5.726767063140869\n",
      "  time_total_s: 611.93909907341\n",
      "  timers:\n",
      "    learn_throughput: 164.753\n",
      "    learn_time_ms: 48.558\n",
      "    load_throughput: 45343.827\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3246.231\n",
      "    update_time_ms: 2.56\n",
      "  timestamp: 1657049738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1240\n",
      "  training_iteration: 155\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1256\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1256\n",
      "    num_agent_steps_trained: 1256\n",
      "    num_env_steps_sampled: 1256\n",
      "    num_env_steps_trained: 1256\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-43\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: 0.3922324780140922\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 418\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.382980227470398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3511152246792335e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.15669487416744232\n",
      "          total_loss: 8.156700134277344\n",
      "          vf_explained_var: 3.373225627001375e-05\n",
      "          vf_loss: 8.000004768371582\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1256\n",
      "    num_agent_steps_trained: 1256\n",
      "    num_env_steps_sampled: 1256\n",
      "    num_env_steps_trained: 1256\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1256\n",
      "  num_agent_steps_trained: 1256\n",
      "  num_env_steps_sampled: 1256\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1256\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.339999999999996\n",
      "    ram_util_percent: 41.04\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14437663058829064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.9397098106551\n",
      "    mean_inference_ms: 1.2188640947030396\n",
      "    mean_raw_obs_processing_ms: 2.1869067557766875\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: 0.3922324780140922\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.01742316736429672\n",
      "      - -0.07698316762953172\n",
      "      - -0.01623115274238529\n",
      "      - -0.002832095234347909\n",
      "      - 0.004374984359521772\n",
      "      - 0.045370684093083535\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14437663058829064\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.9397098106551\n",
      "      mean_inference_ms: 1.2188640947030396\n",
      "      mean_raw_obs_processing_ms: 2.1869067557766875\n",
      "  time_since_restore: 617.4193515777588\n",
      "  time_this_iter_s: 2.87467098236084\n",
      "  time_total_s: 617.4193515777588\n",
      "  timers:\n",
      "    learn_throughput: 164.583\n",
      "    learn_time_ms: 48.608\n",
      "    load_throughput: 38831.654\n",
      "    load_time_ms: 0.206\n",
      "    training_iteration_time_ms: 3095.161\n",
      "    update_time_ms: 2.542\n",
      "  timestamp: 1657049743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1256\n",
      "  training_iteration: 157\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:44 (running for 00:10:39.14)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         617.419</td><td style=\"text-align: right;\">1256</td><td style=\"text-align: right;\">0.392232</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1272\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1272\n",
      "    num_agent_steps_trained: 1272\n",
      "    num_env_steps_sampled: 1272\n",
      "    num_env_steps_trained: 1272\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-50\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: 0.011537924077588272\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 424\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3853760957717896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.817832748609362e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.005770206451416016\n",
      "          total_loss: 5.99476432800293\n",
      "          vf_explained_var: -0.0007847627275623381\n",
      "          vf_loss: 6.000535011291504\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1272\n",
      "    num_agent_steps_trained: 1272\n",
      "    num_env_steps_sampled: 1272\n",
      "    num_env_steps_trained: 1272\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1272\n",
      "  num_agent_steps_trained: 1272\n",
      "  num_env_steps_sampled: 1272\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1272\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.974999999999998\n",
      "    ram_util_percent: 41.025000000000006\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14442926682777835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.9746953353475\n",
      "    mean_inference_ms: 1.2188603368324538\n",
      "    mean_raw_obs_processing_ms: 2.195358910534082\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: 0.011537924077588272\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 23.599996292152664\n",
      "      - 3.1558855069723784\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14442926682777835\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.9746953353475\n",
      "      mean_inference_ms: 1.2188603368324538\n",
      "      mean_raw_obs_processing_ms: 2.195358910534082\n",
      "  time_since_restore: 624.2217454910278\n",
      "  time_this_iter_s: 3.0045652389526367\n",
      "  time_total_s: 624.2217454910278\n",
      "  timers:\n",
      "    learn_throughput: 167.432\n",
      "    learn_time_ms: 47.781\n",
      "    load_throughput: 41496.948\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3063.752\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1657049750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1272\n",
      "  training_iteration: 159\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:50 (running for 00:10:46.01)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         624.222</td><td style=\"text-align: right;\">1272</td><td style=\"text-align: right;\">0.0115379</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:35:55 (running for 00:10:51.02)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         624.222</td><td style=\"text-align: right;\">1272</td><td style=\"text-align: right;\">0.0115379</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1280\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1280\n",
      "    num_agent_steps_trained: 1280\n",
      "    num_env_steps_sampled: 1280\n",
      "    num_env_steps_trained: 1280\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.25631684129244914\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 426\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.08067512295721979\n",
      "    episode_reward_mean: 0.08067512295721979\n",
      "    episode_reward_min: 0.08067512295721979\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.08067512295721979\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1443808840722153\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 896.3355403585533\n",
      "      mean_inference_ms: 1.612270001283626\n",
      "      mean_raw_obs_processing_ms: 0.6683020247626551\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385345458984375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.918259151054372e-07\n",
      "          model: {}\n",
      "          policy_loss: -0.3617621660232544\n",
      "          total_loss: -0.35985884070396423\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001903390628285706\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1280\n",
      "    num_agent_steps_trained: 1280\n",
      "    num_env_steps_sampled: 1280\n",
      "    num_env_steps_trained: 1280\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1280\n",
      "  num_agent_steps_trained: 1280\n",
      "  num_env_steps_sampled: 1280\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1280\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.32222222222222\n",
      "    ram_util_percent: 40.711111111111116\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1443581118287954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.8577905938103\n",
      "    mean_inference_ms: 1.2183559490057656\n",
      "    mean_raw_obs_processing_ms: 2.167468326608956\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.25631684129244914\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.007998514084869401\n",
      "      - 0.029651699343949733\n",
      "      - -0.3703937409005924\n",
      "      - 37.12537866711457\n",
      "      - 0.4539917123632584\n",
      "      - 0.21652385702157773\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1443581118287954\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.8577905938103\n",
      "      mean_inference_ms: 1.2183559490057656\n",
      "      mean_raw_obs_processing_ms: 2.167468326608956\n",
      "  time_since_restore: 630.2946670055389\n",
      "  time_this_iter_s: 6.072921514511108\n",
      "  time_total_s: 630.2946670055389\n",
      "  timers:\n",
      "    learn_throughput: 167.06\n",
      "    learn_time_ms: 47.887\n",
      "    load_throughput: 42350.665\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3027.719\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1657049756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1280\n",
      "  training_iteration: 160\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1296\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1296\n",
      "    num_agent_steps_trained: 1296\n",
      "    num_env_steps_sampled: 1296\n",
      "    num_env_steps_trained: 1296\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.6431205448964366\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 432\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3832980394363403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.09780387194769e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.1900642216205597\n",
      "          total_loss: 0.19038252532482147\n",
      "          vf_explained_var: 0.37833669781684875\n",
      "          vf_loss: 0.00031828167266212404\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1296\n",
      "    num_agent_steps_trained: 1296\n",
      "    num_env_steps_sampled: 1296\n",
      "    num_env_steps_trained: 1296\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1296\n",
      "  num_agent_steps_trained: 1296\n",
      "  num_env_steps_sampled: 1296\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1296\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.5\n",
      "    ram_util_percent: 40.81666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14441317300121256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.0301740363707\n",
      "    mean_inference_ms: 1.218384931702139\n",
      "    mean_raw_obs_processing_ms: 2.176065046035307\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.6431205448964366\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.22199225406923873\n",
      "      - -27.996457199392598\n",
      "      - -5.731685310295994\n",
      "      - 0.8601342640819425\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14441317300121256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.0301740363707\n",
      "      mean_inference_ms: 1.218384931702139\n",
      "      mean_raw_obs_processing_ms: 2.176065046035307\n",
      "  time_since_restore: 638.5383455753326\n",
      "  time_this_iter_s: 4.441232442855835\n",
      "  time_total_s: 638.5383455753326\n",
      "  timers:\n",
      "    learn_throughput: 166.617\n",
      "    learn_time_ms: 48.014\n",
      "    load_throughput: 41765.536\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 3255.227\n",
      "    update_time_ms: 2.299\n",
      "  timestamp: 1657049765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1296\n",
      "  training_iteration: 162\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:05 (running for 00:11:00.46)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         638.538</td><td style=\"text-align: right;\">1296</td><td style=\"text-align: right;\">-0.643121</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1312\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1312\n",
      "    num_agent_steps_trained: 1312\n",
      "    num_env_steps_sampled: 1312\n",
      "    num_env_steps_trained: 1312\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-12\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.3255353094754638\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 436\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3820347785949707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.352091870416189e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3718799650669098\n",
      "          total_loss: 1.4749813079833984\n",
      "          vf_explained_var: -0.0017499704845249653\n",
      "          vf_loss: 1.1031014919281006\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1312\n",
      "    num_agent_steps_trained: 1312\n",
      "    num_env_steps_sampled: 1312\n",
      "    num_env_steps_trained: 1312\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1312\n",
      "  num_agent_steps_trained: 1312\n",
      "  num_env_steps_sampled: 1312\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1312\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.999999999999996\n",
      "    ram_util_percent: 40.96666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14440885365797554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.7264035734329\n",
      "    mean_inference_ms: 1.2181112250588162\n",
      "    mean_raw_obs_processing_ms: 2.166624390525739\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.3255353094754638\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.01844907211788538\n",
      "      - -2.0534116460248555\n",
      "      - -0.029164099350485362\n",
      "      - 6.9488656304784655\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14440885365797554\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.7264035734329\n",
      "      mean_inference_ms: 1.2181112250588162\n",
      "      mean_raw_obs_processing_ms: 2.166624390525739\n",
      "  time_since_restore: 645.9713714122772\n",
      "  time_this_iter_s: 3.6986279487609863\n",
      "  time_total_s: 645.9713714122772\n",
      "  timers:\n",
      "    learn_throughput: 171.771\n",
      "    learn_time_ms: 46.574\n",
      "    load_throughput: 40407.553\n",
      "    load_time_ms: 0.198\n",
      "    training_iteration_time_ms: 3463.326\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1657049772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1312\n",
      "  training_iteration: 164\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:12 (running for 00:11:08.00)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         645.971</td><td style=\"text-align: right;\">1312</td><td style=\"text-align: right;\">-0.325535</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:17 (running for 00:11:13.00)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         645.971</td><td style=\"text-align: right;\">1312</td><td style=\"text-align: right;\">-0.325535</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1320\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1320\n",
      "    num_agent_steps_trained: 1320\n",
      "    num_env_steps_sampled: 1320\n",
      "    num_env_steps_trained: 1320\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.060334894676554986\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 440\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.09126683711399575\n",
      "    episode_reward_mean: -0.09126683711399575\n",
      "    episode_reward_min: -0.09126683711399575\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.09126683711399575\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1443195343017578\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 891.86772108078\n",
      "      mean_inference_ms: 1.598958969116211\n",
      "      mean_raw_obs_processing_ms: 0.6679534912109375\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.384843349456787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3472322279994842e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.1891862452030182\n",
      "          total_loss: 2.190727710723877\n",
      "          vf_explained_var: 0.003503558924421668\n",
      "          vf_loss: 2.0015416145324707\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1320\n",
      "    num_agent_steps_trained: 1320\n",
      "    num_env_steps_sampled: 1320\n",
      "    num_env_steps_trained: 1320\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1320\n",
      "  num_agent_steps_trained: 1320\n",
      "  num_env_steps_sampled: 1320\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1320\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.6125\n",
      "    ram_util_percent: 41.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14438795337886176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.4707804376941\n",
      "    mean_inference_ms: 1.2177816414308322\n",
      "    mean_raw_obs_processing_ms: 2.1573429727741047\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.060334894676554986\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3406312641689053\n",
      "      - -1.2964631669143565\n",
      "      - -5.579969234958831\n",
      "      - 0.24893818102930254\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14438795337886176\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.4707804376941\n",
      "      mean_inference_ms: 1.2177816414308322\n",
      "      mean_raw_obs_processing_ms: 2.1573429727741047\n",
      "  time_since_restore: 651.9540255069733\n",
      "  time_this_iter_s: 5.982654094696045\n",
      "  time_total_s: 651.9540255069733\n",
      "  timers:\n",
      "    learn_throughput: 171.312\n",
      "    learn_time_ms: 46.699\n",
      "    load_throughput: 41125.667\n",
      "    load_time_ms: 0.195\n",
      "    training_iteration_time_ms: 3515.901\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1657049778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1320\n",
      "  training_iteration: 165\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1336\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1336\n",
      "    num_agent_steps_trained: 1336\n",
      "    num_env_steps_sampled: 1336\n",
      "    num_env_steps_trained: 1336\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-26\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.307999881993861\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 444\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3857777118682861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.018413963218336e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.2537097632884979\n",
      "          total_loss: 2.2550172805786133\n",
      "          vf_explained_var: 2.192854844906833e-05\n",
      "          vf_loss: 2.001307487487793\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1336\n",
      "    num_agent_steps_trained: 1336\n",
      "    num_env_steps_sampled: 1336\n",
      "    num_env_steps_trained: 1336\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1336\n",
      "  num_agent_steps_trained: 1336\n",
      "  num_env_steps_sampled: 1336\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1336\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.120000000000005\n",
      "    ram_util_percent: 41.120000000000005\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14436888059447284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.2702830961169\n",
      "    mean_inference_ms: 1.217438497517118\n",
      "    mean_raw_obs_processing_ms: 2.1482161451850317\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.307999881993861\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.033343865562864505\n",
      "      - -4.498122742434692\n",
      "      - -0.005411484569420977\n",
      "      - 3.8827827591451047\n",
      "      - 0.023536795015435152\n",
      "      - 24.8877655569765\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14436888059447284\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.2702830961169\n",
      "      mean_inference_ms: 1.217438497517118\n",
      "      mean_raw_obs_processing_ms: 2.1482161451850317\n",
      "  time_since_restore: 659.5931718349457\n",
      "  time_this_iter_s: 3.907562017440796\n",
      "  time_total_s: 659.5931718349457\n",
      "  timers:\n",
      "    learn_throughput: 170.383\n",
      "    learn_time_ms: 46.953\n",
      "    load_throughput: 49265.059\n",
      "    load_time_ms: 0.162\n",
      "    training_iteration_time_ms: 3732.103\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1657049786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1336\n",
      "  training_iteration: 167\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:26 (running for 00:11:21.75)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         659.593</td><td style=\"text-align: right;\">1336</td><td style=\"text-align: right;\">  -0.308</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1352\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1352\n",
      "    num_agent_steps_trained: 1352\n",
      "    num_env_steps_sampled: 1352\n",
      "    num_env_steps_trained: 1352\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.5380618295063286\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 450\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3835406303405762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.055527253716718e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.39685338735580444\n",
      "          total_loss: -0.05189188942313194\n",
      "          vf_explained_var: 0.01272120513021946\n",
      "          vf_loss: 0.3449615240097046\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1352\n",
      "    num_agent_steps_trained: 1352\n",
      "    num_env_steps_sampled: 1352\n",
      "    num_env_steps_trained: 1352\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1352\n",
      "  num_agent_steps_trained: 1352\n",
      "  num_env_steps_sampled: 1352\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1352\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.583333333333332\n",
      "    ram_util_percent: 41.23333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14427356516537843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 716.0980082942422\n",
      "    mean_inference_ms: 1.2166123580349406\n",
      "    mean_raw_obs_processing_ms: 2.1135162013076307\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.5380618295063286\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 5.280195476040866\n",
      "      - 4.010518709061714\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14427356516537843\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 716.0980082942422\n",
      "      mean_inference_ms: 1.2166123580349406\n",
      "      mean_raw_obs_processing_ms: 2.1135162013076307\n",
      "  time_since_restore: 667.4499003887177\n",
      "  time_this_iter_s: 3.6543726921081543\n",
      "  time_total_s: 667.4499003887177\n",
      "  timers:\n",
      "    learn_throughput: 171.768\n",
      "    learn_time_ms: 46.574\n",
      "    load_throughput: 47669.317\n",
      "    load_time_ms: 0.168\n",
      "    training_iteration_time_ms: 3837.494\n",
      "    update_time_ms: 2.19\n",
      "  timestamp: 1657049794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1352\n",
      "  training_iteration: 169\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:34 (running for 00:11:29.64)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">          667.45</td><td style=\"text-align: right;\">1352</td><td style=\"text-align: right;\">-0.538062</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:39 (running for 00:11:34.68)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">          667.45</td><td style=\"text-align: right;\">1352</td><td style=\"text-align: right;\">-0.538062</td><td style=\"text-align: right;\">             38.1945</td><td style=\"text-align: right;\">            -39.3638</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1360\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1360\n",
      "    num_agent_steps_trained: 1360\n",
      "    num_env_steps_sampled: 1360\n",
      "    num_env_steps_trained: 1360\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.19445079897323\n",
      "  episode_reward_mean: -0.6571661017792867\n",
      "  episode_reward_min: -39.36378726006254\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 452\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.2554017398110475\n",
      "    episode_reward_mean: -1.2554017398110475\n",
      "    episode_reward_min: -1.2554017398110475\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.2554017398110475\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14400713652083016\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 886.2927237760673\n",
      "      mean_inference_ms: 1.5869117477565136\n",
      "      mean_raw_obs_processing_ms: 0.6646600741784549\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.385277271270752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6548350686207414e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.11372319608926773\n",
      "          total_loss: 0.9395074248313904\n",
      "          vf_explained_var: 0.04863453656435013\n",
      "          vf_loss: 0.8257842659950256\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1360\n",
      "    num_agent_steps_trained: 1360\n",
      "    num_env_steps_sampled: 1360\n",
      "    num_env_steps_trained: 1360\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1360\n",
      "  num_agent_steps_trained: 1360\n",
      "  num_env_steps_sampled: 1360\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1360\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.322222222222223\n",
      "    ram_util_percent: 41.3\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14432903672244007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 716.9899464237227\n",
      "    mean_inference_ms: 1.2167848423338556\n",
      "    mean_raw_obs_processing_ms: 2.13061540176035\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.19445079897323\n",
      "    episode_reward_mean: -0.6571661017792867\n",
      "    episode_reward_min: -39.36378726006254\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -2.4244952075538997\n",
      "      - -39.36378726006254\n",
      "      - 5.815723095636535\n",
      "      - 38.19445079897323\n",
      "      - -0.09779888594762909\n",
      "      - -28.169602046598687\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14432903672244007\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 716.9899464237227\n",
      "      mean_inference_ms: 1.2167848423338556\n",
      "      mean_raw_obs_processing_ms: 2.13061540176035\n",
      "  time_since_restore: 673.800975561142\n",
      "  time_this_iter_s: 6.351075172424316\n",
      "  time_total_s: 673.800975561142\n",
      "  timers:\n",
      "    learn_throughput: 171.196\n",
      "    learn_time_ms: 46.73\n",
      "    load_throughput: 47480.447\n",
      "    load_time_ms: 0.168\n",
      "    training_iteration_time_ms: 3909.694\n",
      "    update_time_ms: 2.253\n",
      "  timestamp: 1657049800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1360\n",
      "  training_iteration: 170\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:45 (running for 00:11:40.40)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         678.139</td><td style=\"text-align: right;\">1368</td><td style=\"text-align: right;\">-0.666289</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1376\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1376\n",
      "    num_agent_steps_trained: 1376\n",
      "    num_env_steps_sampled: 1376\n",
      "    num_env_steps_trained: 1376\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: -0.38391541014375163\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 458\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.38205885887146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8302952159720007e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.335783451795578\n",
      "          total_loss: -0.33186644315719604\n",
      "          vf_explained_var: -0.2599194347858429\n",
      "          vf_loss: 0.003916979767382145\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1376\n",
      "    num_agent_steps_trained: 1376\n",
      "    num_env_steps_sampled: 1376\n",
      "    num_env_steps_trained: 1376\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1376\n",
      "  num_agent_steps_trained: 1376\n",
      "  num_env_steps_sampled: 1376\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1376\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.02\n",
      "    ram_util_percent: 41.44\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14421925805560556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 716.2122599103227\n",
      "    mean_inference_ms: 1.2158992714513805\n",
      "    mean_raw_obs_processing_ms: 2.096791091567667\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: -0.38391541014375163\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.026706692119503828\n",
      "      - -0.3481757543563617\n",
      "      - 1.2584931377168496\n",
      "      - -3.904358585487813\n",
      "      - -0.061576747055660075\n",
      "      - -1.7871446904232045\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14421925805560556\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 716.2122599103227\n",
      "      mean_inference_ms: 1.2158992714513805\n",
      "      mean_raw_obs_processing_ms: 2.096791091567667\n",
      "  time_since_restore: 681.9295647144318\n",
      "  time_this_iter_s: 3.790391445159912\n",
      "  time_total_s: 681.9295647144318\n",
      "  timers:\n",
      "    learn_throughput: 171.815\n",
      "    learn_time_ms: 46.562\n",
      "    load_throughput: 46218.226\n",
      "    load_time_ms: 0.173\n",
      "    training_iteration_time_ms: 3898.223\n",
      "    update_time_ms: 2.182\n",
      "  timestamp: 1657049809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1376\n",
      "  training_iteration: 172\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:36:52 (running for 00:11:47.86)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         685.487</td><td style=\"text-align: right;\">1384</td><td style=\"text-align: right;\">-0.392288</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1392\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1392\n",
      "    num_agent_steps_trained: 1392\n",
      "    num_env_steps_sampled: 1392\n",
      "    num_env_steps_trained: 1392\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-36-56\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: -0.0720649282930523\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 464\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.382438063621521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.490955340472283e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.026786400005221367\n",
      "          total_loss: 0.06374058127403259\n",
      "          vf_explained_var: 0.06911886483430862\n",
      "          vf_loss: 0.03695417195558548\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1392\n",
      "    num_agent_steps_trained: 1392\n",
      "    num_env_steps_sampled: 1392\n",
      "    num_env_steps_trained: 1392\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1392\n",
      "  num_agent_steps_trained: 1392\n",
      "  num_env_steps_sampled: 1392\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1392\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.160000000000004\n",
      "    ram_util_percent: 41.54\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14423349284376183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.3629760093816\n",
      "    mean_inference_ms: 1.2156024267854781\n",
      "    mean_raw_obs_processing_ms: 2.105256456235768\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: -0.0720649282930523\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0472501857368306\n",
      "      - -1.2866341020318157\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14423349284376183\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.3629760093816\n",
      "      mean_inference_ms: 1.2156024267854781\n",
      "      mean_raw_obs_processing_ms: 2.105256456235768\n",
      "  time_since_restore: 689.211062669754\n",
      "  time_this_iter_s: 3.7245066165924072\n",
      "  time_total_s: 689.211062669754\n",
      "  timers:\n",
      "    learn_throughput: 168.179\n",
      "    learn_time_ms: 47.568\n",
      "    load_throughput: 45509.877\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3883.27\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1657049816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1392\n",
      "  training_iteration: 174\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:01 (running for 00:11:56.60)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         689.211</td><td style=\"text-align: right;\">1392</td><td style=\"text-align: right;\">-0.0720649</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1400\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1400\n",
      "    num_agent_steps_trained: 1400\n",
      "    num_env_steps_sampled: 1400\n",
      "    num_env_steps_trained: 1400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-02\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: 0.28060929581784794\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 466\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.2577233719074066\n",
      "    episode_reward_mean: 1.2577233719074066\n",
      "    episode_reward_min: 1.2577233719074066\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.2577233719074066\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14413527722628613\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 886.0405818471369\n",
      "      mean_inference_ms: 1.5793926310989093\n",
      "      mean_raw_obs_processing_ms: 0.6629206099600162\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.379852533340454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.517466085322667e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.03993494436144829\n",
      "          total_loss: 6.040794372558594\n",
      "          vf_explained_var: 0.0005529979825951159\n",
      "          vf_loss: 6.000858783721924\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1400\n",
      "    num_agent_steps_trained: 1400\n",
      "    num_env_steps_sampled: 1400\n",
      "    num_env_steps_trained: 1400\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1400\n",
      "  num_agent_steps_trained: 1400\n",
      "  num_env_steps_sampled: 1400\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1400\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.475\n",
      "    ram_util_percent: 41.6125\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14415238716103754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 716.5055450694559\n",
      "    mean_inference_ms: 1.2150808607352288\n",
      "    mean_raw_obs_processing_ms: 2.0805955375106486\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: 0.28060929581784794\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -33.5497342893281\n",
      "      - 34.430721268780985\n",
      "      - 0.03790891520182549\n",
      "      - -6.845280164935012\n",
      "      - 28.091082814013284\n",
      "      - 6.7637205141419585\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14415238716103754\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 716.5055450694559\n",
      "      mean_inference_ms: 1.2150808607352288\n",
      "      mean_raw_obs_processing_ms: 2.0805955375106486\n",
      "  time_since_restore: 694.861649274826\n",
      "  time_this_iter_s: 5.6505866050720215\n",
      "  time_total_s: 694.861649274826\n",
      "  timers:\n",
      "    learn_throughput: 170.35\n",
      "    learn_time_ms: 46.962\n",
      "    load_throughput: 46072.267\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3810.938\n",
      "    update_time_ms: 2.186\n",
      "  timestamp: 1657049822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1400\n",
      "  training_iteration: 175\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1416\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1416\n",
      "    num_agent_steps_trained: 1416\n",
      "    num_env_steps_sampled: 1416\n",
      "    num_env_steps_trained: 1416\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-08\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: -0.33243076902735474\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 472\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3843590021133423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0813369954121299e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.10247242450714111\n",
      "          total_loss: 7.90760612487793\n",
      "          vf_explained_var: -0.0009148677345365286\n",
      "          vf_loss: 8.010079383850098\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1416\n",
      "    num_agent_steps_trained: 1416\n",
      "    num_env_steps_sampled: 1416\n",
      "    num_env_steps_trained: 1416\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1416\n",
      "  num_agent_steps_trained: 1416\n",
      "  num_env_steps_sampled: 1416\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1416\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.475\n",
      "    ram_util_percent: 41.7\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14419120674637922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.6194695807714\n",
      "    mean_inference_ms: 1.214917589093355\n",
      "    mean_raw_obs_processing_ms: 2.089088192106794\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: -0.33243076902735474\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.16531939303608212\n",
      "      - 0.26235661935493937\n",
      "      - -17.244046118474923\n",
      "      - 18.449050801337858\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14419120674637922\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.6194695807714\n",
      "      mean_inference_ms: 1.214917589093355\n",
      "      mean_raw_obs_processing_ms: 2.089088192106794\n",
      "  time_since_restore: 700.7016932964325\n",
      "  time_this_iter_s: 2.520153522491455\n",
      "  time_total_s: 700.7016932964325\n",
      "  timers:\n",
      "    learn_throughput: 171.907\n",
      "    learn_time_ms: 46.537\n",
      "    load_throughput: 45497.535\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3630.859\n",
      "    update_time_ms: 2.09\n",
      "  timestamp: 1657049828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1416\n",
      "  training_iteration: 177\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:08 (running for 00:12:03.21)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         700.702</td><td style=\"text-align: right;\">1416</td><td style=\"text-align: right;\">-0.332431</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1432\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1432\n",
      "    num_agent_steps_trained: 1432\n",
      "    num_env_steps_sampled: 1432\n",
      "    num_env_steps_trained: 1432\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: -0.10684243324711908\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 476\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3859220743179321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.161091131507419e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.33923524618148804\n",
      "          total_loss: 4.5913004875183105\n",
      "          vf_explained_var: -0.004134901333600283\n",
      "          vf_loss: 4.252065181732178\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1432\n",
      "    num_agent_steps_trained: 1432\n",
      "    num_env_steps_sampled: 1432\n",
      "    num_env_steps_trained: 1432\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1432\n",
      "  num_agent_steps_trained: 1432\n",
      "  num_env_steps_sampled: 1432\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1432\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.63333333333333\n",
      "    ram_util_percent: 41.699999999999996\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.144176717648567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.7650289343824\n",
      "    mean_inference_ms: 1.214615493915482\n",
      "    mean_raw_obs_processing_ms: 2.0812865552278264\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: -0.10684243324711908\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.3576446090651366\n",
      "      - -1.8866403954140978\n",
      "      - -27.82915122626078\n",
      "      - -33.60922925860316\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.144176717648567\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.7650289343824\n",
      "      mean_inference_ms: 1.214615493915482\n",
      "      mean_raw_obs_processing_ms: 2.0812865552278264\n",
      "  time_since_restore: 705.9768736362457\n",
      "  time_this_iter_s: 2.39155912399292\n",
      "  time_total_s: 705.9768736362457\n",
      "  timers:\n",
      "    learn_throughput: 170.302\n",
      "    learn_time_ms: 46.975\n",
      "    load_throughput: 46091.253\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3372.915\n",
      "    update_time_ms: 2.009\n",
      "  timestamp: 1657049833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1432\n",
      "  training_iteration: 179\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:13 (running for 00:12:08.55)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         705.977</td><td style=\"text-align: right;\">1432</td><td style=\"text-align: right;\">-0.106842</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:18 (running for 00:12:13.59)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         705.977</td><td style=\"text-align: right;\">1432</td><td style=\"text-align: right;\">-0.106842</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1440\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1440\n",
      "    num_agent_steps_trained: 1440\n",
      "    num_env_steps_sampled: 1440\n",
      "    num_env_steps_trained: 1440\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: 0.6069769393056239\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 480\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.3336508891277516\n",
      "    episode_reward_mean: -1.3336508891277516\n",
      "    episode_reward_min: -1.3336508891277516\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3336508891277516\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1449956806427842\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 885.0148752195026\n",
      "      mean_inference_ms: 1.5784184867088948\n",
      "      mean_raw_obs_processing_ms: 0.6637332636281985\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3851120471954346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.9921920688357204e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.2252100259065628\n",
      "          total_loss: 4.244585990905762\n",
      "          vf_explained_var: 0.0006043791654519737\n",
      "          vf_loss: 4.019375801086426\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1440\n",
      "    num_agent_steps_trained: 1440\n",
      "    num_env_steps_sampled: 1440\n",
      "    num_env_steps_trained: 1440\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1440\n",
      "  num_agent_steps_trained: 1440\n",
      "  num_env_steps_sampled: 1440\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1440\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.150000000000002\n",
      "    ram_util_percent: 40.012499999999996\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14416399871226176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.9056843167017\n",
      "    mean_inference_ms: 1.2142848765292629\n",
      "    mean_raw_obs_processing_ms: 2.0736513452706764\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: 0.6069769393056239\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 27.951893376484062\n",
      "      - 0.2490750377206261\n",
      "      - -0.002492545053243478\n",
      "      - 0.020526372865152798\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14416399871226176\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.9056843167017\n",
      "      mean_inference_ms: 1.2142848765292629\n",
      "      mean_raw_obs_processing_ms: 2.0736513452706764\n",
      "  time_since_restore: 711.4622476100922\n",
      "  time_this_iter_s: 5.4853739738464355\n",
      "  time_total_s: 711.4622476100922\n",
      "  timers:\n",
      "    learn_throughput: 169.818\n",
      "    learn_time_ms: 47.109\n",
      "    load_throughput: 44501.899\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3241.474\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1657049838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1440\n",
      "  training_iteration: 180\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1456\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1456\n",
      "    num_agent_steps_trained: 1456\n",
      "    num_env_steps_sampled: 1456\n",
      "    num_env_steps_trained: 1456\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-24\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: 0.3519798651365465\n",
      "  episode_reward_min: -37.96764451149938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 484\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3810036182403564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.4170454910054104e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.21766138076782227\n",
      "          total_loss: 9.78233814239502\n",
      "          vf_explained_var: -0.0009868383640423417\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1456\n",
      "    num_agent_steps_trained: 1456\n",
      "    num_env_steps_sampled: 1456\n",
      "    num_env_steps_trained: 1456\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1456\n",
      "  num_agent_steps_trained: 1456\n",
      "  num_env_steps_sampled: 1456\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1456\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.625\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1441506619405212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.9782850053504\n",
      "    mean_inference_ms: 1.213950918002455\n",
      "    mean_raw_obs_processing_ms: 2.06616030297059\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: 0.3519798651365465\n",
      "    episode_reward_min: -37.96764451149938\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -28.246366920626933\n",
      "      - 34.23160917938309\n",
      "      - 1.36882476476856\n",
      "      - -34.251904604700165\n",
      "      - -0.07694697282279028\n",
      "      - 0.0735458260955304\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1441506619405212\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.9782850053504\n",
      "      mean_inference_ms: 1.213950918002455\n",
      "      mean_raw_obs_processing_ms: 2.06616030297059\n",
      "  time_since_restore: 716.7140836715698\n",
      "  time_this_iter_s: 2.636461019515991\n",
      "  time_total_s: 716.7140836715698\n",
      "  timers:\n",
      "    learn_throughput: 167.915\n",
      "    learn_time_ms: 47.643\n",
      "    load_throughput: 45374.485\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 2953.063\n",
      "    update_time_ms: 2.272\n",
      "  timestamp: 1657049844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456\n",
      "  training_iteration: 182\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:24 (running for 00:12:19.44)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         716.714</td><td style=\"text-align: right;\">1456</td><td style=\"text-align: right;\"> 0.35198</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -37.9676</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1472\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1472\n",
      "    num_agent_steps_trained: 1472\n",
      "    num_env_steps_sampled: 1472\n",
      "    num_env_steps_trained: 1472\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-31\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: -0.0002147669860108925\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 490\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3813174962997437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2257104344826075e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.37801113724708557\n",
      "          total_loss: 0.3794020116329193\n",
      "          vf_explained_var: -0.40156522393226624\n",
      "          vf_loss: 0.0013908863766118884\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1472\n",
      "    num_agent_steps_trained: 1472\n",
      "    num_env_steps_sampled: 1472\n",
      "    num_env_steps_trained: 1472\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1472\n",
      "  num_agent_steps_trained: 1472\n",
      "  num_env_steps_sampled: 1472\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1472\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.68\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14407267831009463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.1657326726657\n",
      "    mean_inference_ms: 1.2131797733904812\n",
      "    mean_raw_obs_processing_ms: 2.0359845728929242\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: -0.0002147669860108925\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.004299832907640244\n",
      "      - 34.4202031822133\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14407267831009463\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.1657326726657\n",
      "      mean_inference_ms: 1.2131797733904812\n",
      "      mean_raw_obs_processing_ms: 2.0359845728929242\n",
      "  time_since_restore: 723.4543244838715\n",
      "  time_this_iter_s: 3.542945623397827\n",
      "  time_total_s: 723.4543244838715\n",
      "  timers:\n",
      "    learn_throughput: 166.921\n",
      "    learn_time_ms: 47.927\n",
      "    load_throughput: 43844.809\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 2898.101\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1657049851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1472\n",
      "  training_iteration: 184\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:31 (running for 00:12:26.34)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         723.454</td><td style=\"text-align: right;\">1472</td><td style=\"text-align: right;\">-0.000214767</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:36 (running for 00:12:31.35)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         723.454</td><td style=\"text-align: right;\">1472</td><td style=\"text-align: right;\">-0.000214767</td><td style=\"text-align: right;\">             34.6828</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1480\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1480\n",
      "    num_agent_steps_trained: 1480\n",
      "    num_env_steps_sampled: 1480\n",
      "    num_env_steps_trained: 1480\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-37\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.68275836071352\n",
      "  episode_reward_mean: -0.10639271637728771\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 492\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.3593500809931505\n",
      "    episode_reward_mean: 1.3593500809931505\n",
      "    episode_reward_min: 1.3593500809931505\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3593500809931505\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14471794877733504\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 885.0608191319875\n",
      "      mean_inference_ms: 1.5686963285718645\n",
      "      mean_raw_obs_processing_ms: 0.6621863160814558\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3832448720932007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8663196189882e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.27251890301704407\n",
      "          total_loss: 4.2980637550354\n",
      "          vf_explained_var: 0.001610100269317627\n",
      "          vf_loss: 4.025544166564941\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1480\n",
      "    num_agent_steps_trained: 1480\n",
      "    num_env_steps_sampled: 1480\n",
      "    num_env_steps_trained: 1480\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1480\n",
      "  num_agent_steps_trained: 1480\n",
      "  num_env_steps_sampled: 1480\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1480\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.355555555555554\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14413736249222633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.0361266697718\n",
      "    mean_inference_ms: 1.2134215636240784\n",
      "    mean_raw_obs_processing_ms: 2.0517812644108187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.68275836071352\n",
      "    episode_reward_mean: -0.10639271637728771\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.05038390061450615\n",
      "      - -34.3924060581213\n",
      "      - 0.10298562305345049\n",
      "      - 34.68275836071352\n",
      "      - -1.4109890672355445\n",
      "      - 1.343357584717199\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14413736249222633\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.0361266697718\n",
      "      mean_inference_ms: 1.2134215636240784\n",
      "      mean_raw_obs_processing_ms: 2.0517812644108187\n",
      "  time_since_restore: 729.6613085269928\n",
      "  time_this_iter_s: 6.206984043121338\n",
      "  time_total_s: 729.6613085269928\n",
      "  timers:\n",
      "    learn_throughput: 165.054\n",
      "    learn_time_ms: 48.469\n",
      "    load_throughput: 41677.347\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 2951.032\n",
      "    update_time_ms: 2.512\n",
      "  timestamp: 1657049857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1480\n",
      "  training_iteration: 185\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1496\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1496\n",
      "    num_agent_steps_trained: 1496\n",
      "    num_env_steps_sampled: 1496\n",
      "    num_env_steps_trained: 1496\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-43\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.34671233019379843\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 498\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3839492797851562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.353378805419197e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.22025969624519348\n",
      "          total_loss: 2.2215375900268555\n",
      "          vf_explained_var: -0.002535057021304965\n",
      "          vf_loss: 2.0012776851654053\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1496\n",
      "    num_agent_steps_trained: 1496\n",
      "    num_env_steps_sampled: 1496\n",
      "    num_env_steps_trained: 1496\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1496\n",
      "  num_agent_steps_trained: 1496\n",
      "  num_env_steps_sampled: 1496\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1496\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.85\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14405395785356093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.1253783720672\n",
      "    mean_inference_ms: 1.2126008398398773\n",
      "    mean_raw_obs_processing_ms: 2.0222093384802413\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.34671233019379843\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.12510682599351242\n",
      "      - 5.891908200193896\n",
      "      - 29.346851617180302\n",
      "      - -0.6404114765472997\n",
      "      - -29.268300627934117\n",
      "      - -0.4068952898954379\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14405395785356093\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.1253783720672\n",
      "      mean_inference_ms: 1.2126008398398773\n",
      "      mean_raw_obs_processing_ms: 2.0222093384802413\n",
      "  time_since_restore: 735.6312918663025\n",
      "  time_this_iter_s: 3.0821375846862793\n",
      "  time_total_s: 735.6312918663025\n",
      "  timers:\n",
      "    learn_throughput: 161.645\n",
      "    learn_time_ms: 49.491\n",
      "    load_throughput: 41338.465\n",
      "    load_time_ms: 0.194\n",
      "    training_iteration_time_ms: 2963.809\n",
      "    update_time_ms: 2.695\n",
      "  timestamp: 1657049863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1496\n",
      "  training_iteration: 187\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:43 (running for 00:12:38.69)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         735.631</td><td style=\"text-align: right;\">1496</td><td style=\"text-align: right;\">-0.346712</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1512\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1512\n",
      "    num_agent_steps_trained: 1512\n",
      "    num_env_steps_sampled: 1512\n",
      "    num_env_steps_trained: 1512\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-51\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.39407781121944857\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 504\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.381507396697998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.118684501008829e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.24340540170669556\n",
      "          total_loss: -0.24226322770118713\n",
      "          vf_explained_var: 0.1548745185136795\n",
      "          vf_loss: 0.001142184599302709\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1512\n",
      "    num_agent_steps_trained: 1512\n",
      "    num_env_steps_sampled: 1512\n",
      "    num_env_steps_trained: 1512\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1512\n",
      "  num_agent_steps_trained: 1512\n",
      "  num_env_steps_sampled: 1512\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1512\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.18\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14411859301182225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.0716366969189\n",
      "    mean_inference_ms: 1.2125197768159128\n",
      "    mean_raw_obs_processing_ms: 2.0309452092618363\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.39407781121944857\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 29.483111082374563\n",
      "      - -1.9950475927025693\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14411859301182225\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.0716366969189\n",
      "      mean_inference_ms: 1.2125197768159128\n",
      "      mean_raw_obs_processing_ms: 2.0309452092618363\n",
      "  time_since_restore: 743.3801922798157\n",
      "  time_this_iter_s: 3.7153396606445312\n",
      "  time_total_s: 743.3801922798157\n",
      "  timers:\n",
      "    learn_throughput: 158.832\n",
      "    learn_time_ms: 50.368\n",
      "    load_throughput: 39208.264\n",
      "    load_time_ms: 0.204\n",
      "    training_iteration_time_ms: 3211.181\n",
      "    update_time_ms: 2.801\n",
      "  timestamp: 1657049871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1512\n",
      "  training_iteration: 189\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:51 (running for 00:12:46.49)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          743.38</td><td style=\"text-align: right;\">1512</td><td style=\"text-align: right;\">-0.394078</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:37:56 (running for 00:12:51.53)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          743.38</td><td style=\"text-align: right;\">1512</td><td style=\"text-align: right;\">-0.394078</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1520\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1520\n",
      "    num_agent_steps_trained: 1520\n",
      "    num_env_steps_sampled: 1520\n",
      "    num_env_steps_trained: 1520\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-37-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.6706902897443893\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 506\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.03726657519340426\n",
      "    episode_reward_mean: 0.03726657519340426\n",
      "    episode_reward_min: 0.03726657519340426\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.03726657519340426\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14439251111901324\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 890.6751902207084\n",
      "      mean_inference_ms: 1.5607647273851477\n",
      "      mean_raw_obs_processing_ms: 0.6609730098558508\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3831180334091187\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2840005133039085e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3939531445503235\n",
      "          total_loss: 0.39545029401779175\n",
      "          vf_explained_var: 0.7118089199066162\n",
      "          vf_loss: 0.001497144578024745\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1520\n",
      "    num_agent_steps_trained: 1520\n",
      "    num_env_steps_sampled: 1520\n",
      "    num_env_steps_trained: 1520\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1520\n",
      "  num_agent_steps_trained: 1520\n",
      "  num_env_steps_sampled: 1520\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1520\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.389999999999997\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14405301835290588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 717.2430123661424\n",
      "    mean_inference_ms: 1.211967672006065\n",
      "    mean_raw_obs_processing_ms: 2.008937455139135\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.6706902897443893\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.08063169078221222\n",
      "      - -11.641580709550645\n",
      "      - -6.523830589964319\n",
      "      - 6.609796568389228\n",
      "      - 7.426778120643149\n",
      "      - -34.32647917569932\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14405301835290588\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 717.2430123661424\n",
      "      mean_inference_ms: 1.211967672006065\n",
      "      mean_raw_obs_processing_ms: 2.008937455139135\n",
      "  time_since_restore: 749.9457559585571\n",
      "  time_this_iter_s: 6.565563678741455\n",
      "  time_total_s: 749.9457559585571\n",
      "  timers:\n",
      "    learn_throughput: 159.537\n",
      "    learn_time_ms: 50.145\n",
      "    load_throughput: 40573.678\n",
      "    load_time_ms: 0.197\n",
      "    training_iteration_time_ms: 3244.085\n",
      "    update_time_ms: 2.6\n",
      "  timestamp: 1657049877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1520\n",
      "  training_iteration: 190\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:02 (running for 00:12:57.35)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         754.151</td><td style=\"text-align: right;\">1528</td><td style=\"text-align: right;\">-0.553326</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1536\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1536\n",
      "    num_agent_steps_trained: 1536\n",
      "    num_env_steps_sampled: 1536\n",
      "    num_env_steps_trained: 1536\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.2985314800829438\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 512\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3849071264266968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6640949979773723e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.2470901757478714\n",
      "          total_loss: 0.6101271510124207\n",
      "          vf_explained_var: -0.00797365140169859\n",
      "          vf_loss: 0.36303701996803284\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1536\n",
      "    num_agent_steps_trained: 1536\n",
      "    num_env_steps_sampled: 1536\n",
      "    num_env_steps_trained: 1536\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1536\n",
      "  num_agent_steps_trained: 1536\n",
      "  num_env_steps_sampled: 1536\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1536\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.68\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14410922847142138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.4905128870286\n",
      "    mean_inference_ms: 1.2117172107248384\n",
      "    mean_raw_obs_processing_ms: 2.017659552242092\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.2985314800829438\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.9317767121135034\n",
      "      - -3.7269651010780294\n",
      "      - -0.0021074872794305533\n",
      "      - 34.044320810352154\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14410922847142138\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.4905128870286\n",
      "      mean_inference_ms: 1.2117172107248384\n",
      "      mean_raw_obs_processing_ms: 2.017659552242092\n",
      "  time_since_restore: 758.1100780963898\n",
      "  time_this_iter_s: 3.9593071937561035\n",
      "  time_total_s: 758.1100780963898\n",
      "  timers:\n",
      "    learn_throughput: 161.009\n",
      "    learn_time_ms: 49.687\n",
      "    load_throughput: 40088.927\n",
      "    load_time_ms: 0.2\n",
      "    training_iteration_time_ms: 3536.084\n",
      "    update_time_ms: 2.506\n",
      "  timestamp: 1657049886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1536\n",
      "  training_iteration: 192\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:10 (running for 00:13:05.92)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         762.626</td><td style=\"text-align: right;\">1544</td><td style=\"text-align: right;\">-0.241646</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1552\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1552\n",
      "    num_agent_steps_trained: 1552\n",
      "    num_env_steps_sampled: 1552\n",
      "    num_env_steps_trained: 1552\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.5819104289499364\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 516\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3856453895568848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.026987658609869e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3716532289981842\n",
      "          total_loss: 0.3722209334373474\n",
      "          vf_explained_var: -0.7954835891723633\n",
      "          vf_loss: 0.0005677291774190962\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1552\n",
      "    num_agent_steps_trained: 1552\n",
      "    num_env_steps_sampled: 1552\n",
      "    num_env_steps_trained: 1552\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1552\n",
      "  num_agent_steps_trained: 1552\n",
      "  num_env_steps_sampled: 1552\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1552\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.080000000000002\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14409350950040845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 718.8556663743484\n",
      "    mean_inference_ms: 1.2112889238624793\n",
      "    mean_raw_obs_processing_ms: 2.011173873163925\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.5819104289499364\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -22.78433605879537\n",
      "      - 5.262475121869102\n",
      "      - 0.0008587022943715716\n",
      "      - -37.96764451149938\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14409350950040845\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 718.8556663743484\n",
      "      mean_inference_ms: 1.2112889238624793\n",
      "      mean_raw_obs_processing_ms: 2.011173873163925\n",
      "  time_since_restore: 766.4607033729553\n",
      "  time_this_iter_s: 3.8344974517822266\n",
      "  time_total_s: 766.4607033729553\n",
      "  timers:\n",
      "    learn_throughput: 162.648\n",
      "    learn_time_ms: 49.186\n",
      "    load_throughput: 39208.264\n",
      "    load_time_ms: 0.204\n",
      "    training_iteration_time_ms: 3698.054\n",
      "    update_time_ms: 2.406\n",
      "  timestamp: 1657049894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1552\n",
      "  training_iteration: 194\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:19 (running for 00:13:14.77)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         766.461</td><td style=\"text-align: right;\">1552</td><td style=\"text-align: right;\">-0.58191</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1560\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1560\n",
      "    num_agent_steps_trained: 1560\n",
      "    num_env_steps_sampled: 1560\n",
      "    num_env_steps_trained: 1560\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.013282811135988597\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 520\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.008024736862237258\n",
      "    episode_reward_mean: -0.008024736862237258\n",
      "    episode_reward_min: -0.008024736862237258\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.008024736862237258\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14443316702115336\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 886.2059197183384\n",
      "      mean_inference_ms: 1.5496843952243613\n",
      "      mean_raw_obs_processing_ms: 0.6630198430206816\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3833297491073608\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0358379515528213e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5300710797309875\n",
      "          total_loss: 0.8479563593864441\n",
      "          vf_explained_var: 0.025236990302801132\n",
      "          vf_loss: 0.31788527965545654\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1560\n",
      "    num_agent_steps_trained: 1560\n",
      "    num_env_steps_sampled: 1560\n",
      "    num_env_steps_trained: 1560\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1560\n",
      "  num_agent_steps_trained: 1560\n",
      "  num_env_steps_sampled: 1560\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1560\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.822222222222223\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14408288527982702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 719.2864601796228\n",
      "    mean_inference_ms: 1.2109184317222328\n",
      "    mean_raw_obs_processing_ms: 2.004778939417197\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.013282811135988597\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 28.978554996389512\n",
      "      - -29.06526792283361\n",
      "      - 0.02211485836528526\n",
      "      - -0.0669490961559358\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14408288527982702\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 719.2864601796228\n",
      "      mean_inference_ms: 1.2109184317222328\n",
      "      mean_raw_obs_processing_ms: 2.004778939417197\n",
      "  time_since_restore: 772.6638708114624\n",
      "  time_this_iter_s: 6.20316743850708\n",
      "  time_total_s: 772.6638708114624\n",
      "  timers:\n",
      "    learn_throughput: 163.303\n",
      "    learn_time_ms: 48.989\n",
      "    load_throughput: 41507.214\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3749.088\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1657049900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1560\n",
      "  training_iteration: 195\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:24 (running for 00:13:19.94)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         776.554</td><td style=\"text-align: right;\">1568</td><td style=\"text-align: right;\">-0.0256683</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1576\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1576\n",
      "    num_agent_steps_trained: 1576\n",
      "    num_env_steps_sampled: 1576\n",
      "    num_env_steps_trained: 1576\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.045182605427826\n",
      "  episode_reward_mean: -0.02523136253444823\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 524\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.382055401802063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.087048713088734e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.26785457134246826\n",
      "          total_loss: -0.26683560013771057\n",
      "          vf_explained_var: -0.6305055022239685\n",
      "          vf_loss: 0.0010189503664150834\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1576\n",
      "    num_agent_steps_trained: 1576\n",
      "    num_env_steps_sampled: 1576\n",
      "    num_env_steps_trained: 1576\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1576\n",
      "  num_agent_steps_trained: 1576\n",
      "  num_env_steps_sampled: 1576\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1576\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.64\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14407066128994597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 719.7702469009715\n",
      "    mean_inference_ms: 1.2105293064130367\n",
      "    mean_raw_obs_processing_ms: 1.9984721915955215\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 34.045182605427826\n",
      "    episode_reward_mean: -0.02523136253444823\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.004066983366494137\n",
      "      - -0.02552775451222611\n",
      "      - 28.89377646788804\n",
      "      - -1.308947388957042\n",
      "      - -28.81368149022403\n",
      "      - 0.0118306338437697\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14407066128994597\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 719.7702469009715\n",
      "      mean_inference_ms: 1.2105293064130367\n",
      "      mean_raw_obs_processing_ms: 1.9984721915955215\n",
      "  time_since_restore: 780.0959420204163\n",
      "  time_this_iter_s: 3.5421142578125\n",
      "  time_total_s: 780.0959420204163\n",
      "  timers:\n",
      "    learn_throughput: 165.113\n",
      "    learn_time_ms: 48.452\n",
      "    load_throughput: 42892.026\n",
      "    load_time_ms: 0.187\n",
      "    training_iteration_time_ms: 3895.728\n",
      "    update_time_ms: 2.325\n",
      "  timestamp: 1657049908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1576\n",
      "  training_iteration: 197\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:32 (running for 00:13:27.76)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         784.277</td><td style=\"text-align: right;\">1584</td><td style=\"text-align: right;\">-0.0214841</td><td style=\"text-align: right;\">             34.0452</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1592\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1592\n",
      "    num_agent_steps_trained: 1592\n",
      "    num_env_steps_sampled: 1592\n",
      "    num_env_steps_trained: 1592\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-35\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.6475719111040273\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 530\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.381934404373169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.583081244571076e-07\n",
      "          model: {}\n",
      "          policy_loss: -0.026127682998776436\n",
      "          total_loss: 1.312961459159851\n",
      "          vf_explained_var: -0.02106560952961445\n",
      "          vf_loss: 1.3390891551971436\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1592\n",
      "    num_agent_steps_trained: 1592\n",
      "    num_env_steps_sampled: 1592\n",
      "    num_env_steps_trained: 1592\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1592\n",
      "  num_agent_steps_trained: 1592\n",
      "  num_env_steps_sampled: 1592\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1592\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.525\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14399496182986182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 719.6858494624613\n",
      "    mean_inference_ms: 1.2095858349885469\n",
      "    mean_raw_obs_processing_ms: 1.9716133634948736\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.6475719111040273\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.011636754549362083\n",
      "      - 0.011438880627527515\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14399496182986182\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 719.6858494624613\n",
      "      mean_inference_ms: 1.2095858349885469\n",
      "      mean_raw_obs_processing_ms: 1.9716133634948736\n",
      "  time_since_restore: 787.0770180225372\n",
      "  time_this_iter_s: 2.800048589706421\n",
      "  time_total_s: 787.0770180225372\n",
      "  timers:\n",
      "    learn_throughput: 168.181\n",
      "    learn_time_ms: 47.568\n",
      "    load_throughput: 43492.459\n",
      "    load_time_ms: 0.184\n",
      "    training_iteration_time_ms: 3818.855\n",
      "    update_time_ms: 2.345\n",
      "  timestamp: 1657049915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1592\n",
      "  training_iteration: 199\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:40 (running for 00:13:35.58)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         787.077</td><td style=\"text-align: right;\">1592</td><td style=\"text-align: right;\">0.647572</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1600\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1600\n",
      "    num_agent_steps_trained: 1600\n",
      "    num_env_steps_sampled: 1600\n",
      "    num_env_steps_trained: 1600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-41\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.61930676827058\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 532\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.0927941305578206\n",
      "    episode_reward_mean: 0.0927941305578206\n",
      "    episode_reward_min: 0.0927941305578206\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0927941305578206\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14405802261730855\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 890.4770287600429\n",
      "      mean_inference_ms: 1.5404854924225608\n",
      "      mean_raw_obs_processing_ms: 0.662596757746925\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3847416639328003\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.247876859153621e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.22455747425556183\n",
      "          total_loss: 1.8142198324203491\n",
      "          vf_explained_var: 0.0014894545311108232\n",
      "          vf_loss: 2.0387771129608154\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1600\n",
      "    num_agent_steps_trained: 1600\n",
      "    num_env_steps_sampled: 1600\n",
      "    num_env_steps_trained: 1600\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1600\n",
      "  num_agent_steps_trained: 1600\n",
      "  num_env_steps_sampled: 1600\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1600\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.377777777777778\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14404281308797062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 720.6723158694392\n",
      "    mean_inference_ms: 1.2097063275490192\n",
      "    mean_raw_obs_processing_ms: 1.9862084613016224\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.61930676827058\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.10383466105485284\n",
      "      - 1.3156175813837132\n",
      "      - -1.277610138577571\n",
      "      - -1.2656497393298864\n",
      "      - 1.3641730973661663\n",
      "      - 28.739998375105003\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14404281308797062\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 720.6723158694392\n",
      "      mean_inference_ms: 1.2097063275490192\n",
      "      mean_raw_obs_processing_ms: 1.9862084613016224\n",
      "  time_since_restore: 793.0134363174438\n",
      "  time_this_iter_s: 5.936418294906616\n",
      "  time_total_s: 793.0134363174438\n",
      "  timers:\n",
      "    learn_throughput: 166.633\n",
      "    learn_time_ms: 48.01\n",
      "    load_throughput: 43408.062\n",
      "    load_time_ms: 0.184\n",
      "    training_iteration_time_ms: 3768.547\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1657049921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1600\n",
      "  training_iteration: 200\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1616\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1616\n",
      "    num_agent_steps_trained: 1616\n",
      "    num_env_steps_sampled: 1616\n",
      "    num_env_steps_trained: 1616\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.2874788206678266\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 538\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3815522193908691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0597498152928893e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.37126579880714417\n",
      "          total_loss: 1.8867039680480957\n",
      "          vf_explained_var: 0.00559015478938818\n",
      "          vf_loss: 2.257969856262207\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1616\n",
      "    num_agent_steps_trained: 1616\n",
      "    num_env_steps_sampled: 1616\n",
      "    num_env_steps_trained: 1616\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1616\n",
      "  num_agent_steps_trained: 1616\n",
      "  num_env_steps_sampled: 1616\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1616\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.549999999999997\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14396216972804612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 720.3422809401424\n",
      "    mean_inference_ms: 1.2088187735893556\n",
      "    mean_raw_obs_processing_ms: 1.9601946502516105\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.2874788206678266\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0045498771156922135\n",
      "      - 1.2682606975206243\n",
      "      - -0.1729220789426904\n",
      "      - -1.32238224010089\n",
      "      - -28.559293940414484\n",
      "      - 0.0012365710524645523\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14396216972804612\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 720.3422809401424\n",
      "      mean_inference_ms: 1.2088187735893556\n",
      "      mean_raw_obs_processing_ms: 1.9601946502516105\n",
      "  time_since_restore: 799.0485727787018\n",
      "  time_this_iter_s: 3.2038228511810303\n",
      "  time_total_s: 799.0485727787018\n",
      "  timers:\n",
      "    learn_throughput: 167.344\n",
      "    learn_time_ms: 47.806\n",
      "    load_throughput: 41791.546\n",
      "    load_time_ms: 0.191\n",
      "    training_iteration_time_ms: 3555.63\n",
      "    update_time_ms: 2.325\n",
      "  timestamp: 1657049927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1616\n",
      "  training_iteration: 202\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:47 (running for 00:13:42.68)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         799.049</td><td style=\"text-align: right;\">1616</td><td style=\"text-align: right;\">-0.287479</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1632\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1632\n",
      "    num_agent_steps_trained: 1632\n",
      "    num_env_steps_sampled: 1632\n",
      "    num_env_steps_trained: 1632\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.013113904036337967\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 544\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3799270391464233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4024326446815394e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.3939644992351532\n",
      "          total_loss: -0.04511724412441254\n",
      "          vf_explained_var: 0.05712488666176796\n",
      "          vf_loss: 0.34884724020957947\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1632\n",
      "    num_agent_steps_trained: 1632\n",
      "    num_env_steps_sampled: 1632\n",
      "    num_env_steps_trained: 1632\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1632\n",
      "  num_agent_steps_trained: 1632\n",
      "  num_env_steps_sampled: 1632\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1632\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.5\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14399316499555564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.5120151575471\n",
      "    mean_inference_ms: 1.2085989353891242\n",
      "    mean_raw_obs_processing_ms: 1.9688746173531895\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.013113904036337967\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3771818788536359\n",
      "      - 0.004219493046228928\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14399316499555564\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.5120151575471\n",
      "      mean_inference_ms: 1.2085989353891242\n",
      "      mean_raw_obs_processing_ms: 1.9688746173531895\n",
      "  time_since_restore: 805.7695438861847\n",
      "  time_this_iter_s: 3.1346383094787598\n",
      "  time_total_s: 805.7695438861847\n",
      "  timers:\n",
      "    learn_throughput: 167.353\n",
      "    learn_time_ms: 47.803\n",
      "    load_throughput: 45639.869\n",
      "    load_time_ms: 0.175\n",
      "    training_iteration_time_ms: 3392.344\n",
      "    update_time_ms: 2.299\n",
      "  timestamp: 1657049934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1632\n",
      "  training_iteration: 204\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:54 (running for 00:13:49.48)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">          805.77</td><td style=\"text-align: right;\">1632</td><td style=\"text-align: right;\">-0.0131139</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:38:59 (running for 00:13:54.51)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">          805.77</td><td style=\"text-align: right;\">1632</td><td style=\"text-align: right;\">-0.0131139</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1640\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1640\n",
      "    num_agent_steps_trained: 1640\n",
      "    num_env_steps_sampled: 1640\n",
      "    num_env_steps_trained: 1640\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-00\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.0006943099003707564\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 546\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.061612636509530194\n",
      "    episode_reward_mean: -0.061612636509530194\n",
      "    episode_reward_min: -0.061612636509530194\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.061612636509530194\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1436856485182239\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 889.4380677130914\n",
      "      mean_inference_ms: 1.5326872948677308\n",
      "      mean_raw_obs_processing_ms: 0.6634477646120133\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3810714483261108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0990101145580411e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.38596484065055847\n",
      "          total_loss: 0.3891749382019043\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0032101604156196117\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1640\n",
      "    num_agent_steps_trained: 1640\n",
      "    num_env_steps_sampled: 1640\n",
      "    num_env_steps_trained: 1640\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1640\n",
      "  num_agent_steps_trained: 1640\n",
      "  num_env_steps_sampled: 1640\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1640\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.211111111111112\n",
      "    ram_util_percent: 39.81111111111111\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14392837909308714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 720.8887633096477\n",
      "    mean_inference_ms: 1.2080860457212945\n",
      "    mean_raw_obs_processing_ms: 1.949079667057527\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.0006943099003707564\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3239552091071503\n",
      "      - 0.06252016211621925\n",
      "      - 1.308160042155703\n",
      "      - -0.07066076024837553\n",
      "      - -1.3100858580979973\n",
      "      - -1.309627184095194\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14392837909308714\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 720.8887633096477\n",
      "      mean_inference_ms: 1.2080860457212945\n",
      "      mean_raw_obs_processing_ms: 1.949079667057527\n",
      "  time_since_restore: 812.2872822284698\n",
      "  time_this_iter_s: 6.517738342285156\n",
      "  time_total_s: 812.2872822284698\n",
      "  timers:\n",
      "    learn_throughput: 169.322\n",
      "    learn_time_ms: 47.247\n",
      "    load_throughput: 44243.713\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3384.125\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1657049940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1640\n",
      "  training_iteration: 205\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:04 (running for 00:14:00.06)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         816.276</td><td style=\"text-align: right;\">1648</td><td style=\"text-align: right;\">-0.0126191</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1656\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1656\n",
      "    num_agent_steps_trained: 1656\n",
      "    num_env_steps_sampled: 1656\n",
      "    num_env_steps_trained: 1656\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.27366804199514505\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 552\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.377416729927063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.06258693349082e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.3759719431400299\n",
      "          total_loss: 1.6271474361419678\n",
      "          vf_explained_var: 0.0013741691363975406\n",
      "          vf_loss: 2.003119468688965\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1656\n",
      "    num_agent_steps_trained: 1656\n",
      "    num_env_steps_sampled: 1656\n",
      "    num_env_steps_trained: 1656\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1656\n",
      "  num_agent_steps_trained: 1656\n",
      "  num_env_steps_sampled: 1656\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1656\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.16\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14395765063200325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.0039141123308\n",
      "    mean_inference_ms: 1.2078442518002643\n",
      "    mean_raw_obs_processing_ms: 1.9576868104669316\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.27366804199514505\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.2812553222416578\n",
      "      - 0.012570761271470499\n",
      "      - -0.006210816727537982\n",
      "      - 0.022017412297348216\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14395765063200325\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.0039141123308\n",
      "      mean_inference_ms: 1.2078442518002643\n",
      "      mean_raw_obs_processing_ms: 1.9576868104669316\n",
      "  time_since_restore: 819.8611555099487\n",
      "  time_this_iter_s: 3.585378646850586\n",
      "  time_total_s: 819.8611555099487\n",
      "  timers:\n",
      "    learn_throughput: 169.409\n",
      "    learn_time_ms: 47.223\n",
      "    load_throughput: 44046.248\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3398.227\n",
      "    update_time_ms: 2.201\n",
      "  timestamp: 1657049948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1656\n",
      "  training_iteration: 207\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:13 (running for 00:14:08.28)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         824.375</td><td style=\"text-align: right;\">1664</td><td style=\"text-align: right;\">-0.0122948</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1672\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1672\n",
      "    num_agent_steps_trained: 1672\n",
      "    num_env_steps_sampled: 1672\n",
      "    num_env_steps_trained: 1672\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.0006406392346945389\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 556\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3812634944915771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0073997802683152e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.017071697860956192\n",
      "          total_loss: 0.6145417094230652\n",
      "          vf_explained_var: -0.025531355291604996\n",
      "          vf_loss: 0.6316134333610535\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1672\n",
      "    num_agent_steps_trained: 1672\n",
      "    num_env_steps_sampled: 1672\n",
      "    num_env_steps_trained: 1672\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1672\n",
      "  num_agent_steps_trained: 1672\n",
      "  num_env_steps_sampled: 1672\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1672\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.639999999999997\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14394101577710208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.217967452276\n",
      "    mean_inference_ms: 1.207469716082481\n",
      "    mean_raw_obs_processing_ms: 1.9522462128941112\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.0006406392346945389\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0045129899511331395\n",
      "      - -0.025560031131282468\n",
      "      - -1.2175756769607666\n",
      "      - 0.005413541225461782\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14394101577710208\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.217967452276\n",
      "      mean_inference_ms: 1.207469716082481\n",
      "      mean_raw_obs_processing_ms: 1.9522462128941112\n",
      "  time_since_restore: 828.17351770401\n",
      "  time_this_iter_s: 3.798809289932251\n",
      "  time_total_s: 828.17351770401\n",
      "  timers:\n",
      "    learn_throughput: 167.236\n",
      "    learn_time_ms: 47.837\n",
      "    load_throughput: 45319.33\n",
      "    load_time_ms: 0.177\n",
      "    training_iteration_time_ms: 3531.335\n",
      "    update_time_ms: 2.158\n",
      "  timestamp: 1657049956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1672\n",
      "  training_iteration: 209\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:21 (running for 00:14:17.10)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         828.174</td><td style=\"text-align: right;\">1672</td><td style=\"text-align: right;\">0.000640639</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1680\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1680\n",
      "    num_agent_steps_trained: 1680\n",
      "    num_env_steps_sampled: 1680\n",
      "    num_env_steps_trained: 1680\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-23\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.27805800955172094\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 560\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.441339278033468\n",
      "    episode_reward_mean: -1.441339278033468\n",
      "    episode_reward_min: -1.441339278033468\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.441339278033468\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14359932246170645\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 889.6873316426916\n",
      "      mean_inference_ms: 1.526642972090113\n",
      "      mean_raw_obs_processing_ms: 0.6624507153128076\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3825958967208862\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2203856385895051e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.17686966061592102\n",
      "          total_loss: 2.8087964057922363\n",
      "          vf_explained_var: 0.0022010186221450567\n",
      "          vf_loss: 2.6319267749786377\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1680\n",
      "    num_agent_steps_trained: 1680\n",
      "    num_env_steps_sampled: 1680\n",
      "    num_env_steps_trained: 1680\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1680\n",
      "  num_agent_steps_trained: 1680\n",
      "  num_env_steps_sampled: 1680\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1680\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.679999999999998\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14393290325155597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.4319698241903\n",
      "    mean_inference_ms: 1.2071322435351244\n",
      "    mean_raw_obs_processing_ms: 1.9469580907013389\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.27805800955172094\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 27.958556413637087\n",
      "      - -0.49953802445662276\n",
      "      - 0.05558731609970369\n",
      "      - 0.013135283799381114\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14393290325155597\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.4319698241903\n",
      "      mean_inference_ms: 1.2071322435351244\n",
      "      mean_raw_obs_processing_ms: 1.9469580907013389\n",
      "  time_since_restore: 834.7281036376953\n",
      "  time_this_iter_s: 6.554585933685303\n",
      "  time_total_s: 834.7281036376953\n",
      "  timers:\n",
      "    learn_throughput: 168.112\n",
      "    learn_time_ms: 47.587\n",
      "    load_throughput: 42377.408\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3640.7\n",
      "    update_time_ms: 2.176\n",
      "  timestamp: 1657049963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1680\n",
      "  training_iteration: 210\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:27 (running for 00:14:22.86)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">          838.83</td><td style=\"text-align: right;\">1688</td><td style=\"text-align: right;\">0.0305719</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1696\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1696\n",
      "    num_agent_steps_trained: 1696\n",
      "    num_env_steps_sampled: 1696\n",
      "    num_env_steps_trained: 1696\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-30\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.014087023141006598\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 564\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3825500011444092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.803010940828244e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.287329763174057\n",
      "          total_loss: 5.48217248916626\n",
      "          vf_explained_var: -0.0013874828582629561\n",
      "          vf_loss: 5.769501686096191\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1696\n",
      "    num_agent_steps_trained: 1696\n",
      "    num_env_steps_sampled: 1696\n",
      "    num_env_steps_trained: 1696\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1696\n",
      "  num_agent_steps_trained: 1696\n",
      "  num_env_steps_sampled: 1696\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1696\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.075\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14392665228056528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.6224821697592\n",
      "    mean_inference_ms: 1.2068177680399703\n",
      "    mean_raw_obs_processing_ms: 1.941783604524858\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.014087023141006598\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.06661619928000562\n",
      "      - 33.96142229551505\n",
      "      - -27.56386426989468\n",
      "      - -33.96483841111468\n",
      "      - -9.592655049672771e-05\n",
      "      - 27.86026249553345\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14392665228056528\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.6224821697592\n",
      "      mean_inference_ms: 1.2068177680399703\n",
      "      mean_raw_obs_processing_ms: 1.941783604524858\n",
      "  time_since_restore: 841.5825068950653\n",
      "  time_this_iter_s: 2.752187967300415\n",
      "  time_total_s: 841.5825068950653\n",
      "  timers:\n",
      "    learn_throughput: 164.611\n",
      "    learn_time_ms: 48.599\n",
      "    load_throughput: 42463.214\n",
      "    load_time_ms: 0.188\n",
      "    training_iteration_time_ms: 3722.582\n",
      "    update_time_ms: 2.226\n",
      "  timestamp: 1657049970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1696\n",
      "  training_iteration: 212\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:33 (running for 00:14:28.96)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         844.798</td><td style=\"text-align: right;\">1704</td><td style=\"text-align: right;\">0.573841</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1712\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1712\n",
      "    num_agent_steps_trained: 1712\n",
      "    num_env_steps_sampled: 1712\n",
      "    num_env_steps_trained: 1712\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: 0.0024545825414762136\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 570\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3764458894729614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.806267386607942e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5109497308731079\n",
      "          total_loss: 1.5952471494674683\n",
      "          vf_explained_var: -0.04556975141167641\n",
      "          vf_loss: 2.1061971187591553\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1712\n",
      "    num_agent_steps_trained: 1712\n",
      "    num_env_steps_sampled: 1712\n",
      "    num_env_steps_trained: 1712\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1712\n",
      "  num_agent_steps_trained: 1712\n",
      "  num_env_steps_sampled: 1712\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1712\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.18\n",
      "    ram_util_percent: 39.81999999999999\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1438598250206469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.242129026316\n",
      "    mean_inference_ms: 1.2059891273679126\n",
      "    mean_raw_obs_processing_ms: 1.918087627637004\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: 0.0024545825414762136\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 34.045182605427826\n",
      "      - -32.75223392004677\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1438598250206469\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.242129026316\n",
      "      mean_inference_ms: 1.2059891273679126\n",
      "      mean_raw_obs_processing_ms: 1.918087627637004\n",
      "  time_since_restore: 847.6921274662018\n",
      "  time_this_iter_s: 2.8944122791290283\n",
      "  time_total_s: 847.6921274662018\n",
      "  timers:\n",
      "    learn_throughput: 156.411\n",
      "    learn_time_ms: 51.147\n",
      "    load_throughput: 42414.906\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3661.24\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1657049976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1712\n",
      "  training_iteration: 214\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:41 (running for 00:14:36.89)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         847.692</td><td style=\"text-align: right;\">1712</td><td style=\"text-align: right;\">0.00245458</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1720\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1720\n",
      "    num_agent_steps_trained: 1720\n",
      "    num_env_steps_sampled: 1720\n",
      "    num_env_steps_trained: 1720\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-43\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.06178242029314687\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 572\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.4443072371752428\n",
      "    episode_reward_mean: 1.4443072371752428\n",
      "    episode_reward_min: 1.4443072371752428\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.4443072371752428\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14365086188683143\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 890.0947130643405\n",
      "      mean_inference_ms: 1.5224420107327976\n",
      "      mean_raw_obs_processing_ms: 0.6620095326350286\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3803355693817139\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.800769491135725e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.2997649013996124\n",
      "          total_loss: 2.4878649711608887\n",
      "          vf_explained_var: -0.01657908782362938\n",
      "          vf_loss: 2.7876293659210205\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1720\n",
      "    num_agent_steps_trained: 1720\n",
      "    num_env_steps_sampled: 1720\n",
      "    num_env_steps_trained: 1720\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1720\n",
      "  num_agent_steps_trained: 1720\n",
      "  num_env_steps_sampled: 1720\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1720\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.08888888888889\n",
      "    ram_util_percent: 39.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1439131408764059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.0424940895887\n",
      "    mean_inference_ms: 1.2061965217591402\n",
      "    mean_raw_obs_processing_ms: 1.931730081015169\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.06178242029314687\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.2161384107242057\n",
      "      - 30.954930391529906\n",
      "      - -0.27869941070666115\n",
      "      - -6.599217082893684\n",
      "      - 0.4673975408495714\n",
      "      - 0.4134146072635687\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1439131408764059\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.0424940895887\n",
      "      mean_inference_ms: 1.2061965217591402\n",
      "      mean_raw_obs_processing_ms: 1.931730081015169\n",
      "  time_since_restore: 854.2752029895782\n",
      "  time_this_iter_s: 6.583075523376465\n",
      "  time_total_s: 854.2752029895782\n",
      "  timers:\n",
      "    learn_throughput: 155.169\n",
      "    learn_time_ms: 51.557\n",
      "    load_throughput: 41548.331\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3649.8\n",
      "    update_time_ms: 2.474\n",
      "  timestamp: 1657049983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1720\n",
      "  training_iteration: 215\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1736\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1736\n",
      "    num_agent_steps_trained: 1736\n",
      "    num_env_steps_sampled: 1736\n",
      "    num_env_steps_trained: 1736\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-50\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.25657600608025694\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 578\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3762508630752563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.053185678756563e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.2984023094177246\n",
      "          total_loss: 3.268842935562134\n",
      "          vf_explained_var: 0.005056772846728563\n",
      "          vf_loss: 3.5672452449798584\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1736\n",
      "    num_agent_steps_trained: 1736\n",
      "    num_env_steps_sampled: 1736\n",
      "    num_env_steps_trained: 1736\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1736\n",
      "  num_agent_steps_trained: 1736\n",
      "  num_env_steps_sampled: 1736\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1736\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1\n",
      "    ram_util_percent: 39.86\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14385350977858938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.7346734637244\n",
      "    mean_inference_ms: 1.2055237178452163\n",
      "    mean_raw_obs_processing_ms: 1.908564411252791\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.25657600608025694\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 8.558621245440943\n",
      "      - -1.740161627622932\n",
      "      - -28.237229339530305\n",
      "      - -0.5423511012027191\n",
      "      - 22.698713399027334\n",
      "      - 8.800161866814545\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14385350977858938\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.7346734637244\n",
      "      mean_inference_ms: 1.2055237178452163\n",
      "      mean_raw_obs_processing_ms: 1.908564411252791\n",
      "  time_since_restore: 860.8979530334473\n",
      "  time_this_iter_s: 3.506591558456421\n",
      "  time_total_s: 860.8979530334473\n",
      "  timers:\n",
      "    learn_throughput: 155.197\n",
      "    learn_time_ms: 51.547\n",
      "    load_throughput: 38484.267\n",
      "    load_time_ms: 0.208\n",
      "    training_iteration_time_ms: 3554.733\n",
      "    update_time_ms: 2.432\n",
      "  timestamp: 1657049990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1736\n",
      "  training_iteration: 217\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:50 (running for 00:14:45.24)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         860.898</td><td style=\"text-align: right;\">1736</td><td style=\"text-align: right;\">-0.256576</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1752\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1752\n",
      "    num_agent_steps_trained: 1752\n",
      "    num_env_steps_sampled: 1752\n",
      "    num_env_steps_trained: 1752\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.3303494552935811\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 584\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.380912184715271\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0930610187642742e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.003243439830839634\n",
      "          total_loss: 3.9968698024749756\n",
      "          vf_explained_var: 0.000850677490234375\n",
      "          vf_loss: 4.0001139640808105\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1752\n",
      "    num_agent_steps_trained: 1752\n",
      "    num_env_steps_sampled: 1752\n",
      "    num_env_steps_trained: 1752\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1752\n",
      "  num_agent_steps_trained: 1752\n",
      "  num_env_steps_sampled: 1752\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1752\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.4\n",
      "    ram_util_percent: 39.879999999999995\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14391300319740996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.8813116755255\n",
      "    mean_inference_ms: 1.205558550108846\n",
      "    mean_raw_obs_processing_ms: 1.917183107164954\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.3303494552935811\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -21.332315470307137\n",
      "      - 0.028332262281487886\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14391300319740996\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.8813116755255\n",
      "      mean_inference_ms: 1.205558550108846\n",
      "      mean_raw_obs_processing_ms: 1.917183107164954\n",
      "  time_since_restore: 868.4781913757324\n",
      "  time_this_iter_s: 3.269001007080078\n",
      "  time_total_s: 868.4781913757324\n",
      "  timers:\n",
      "    learn_throughput: 158.218\n",
      "    learn_time_ms: 50.563\n",
      "    load_throughput: 39466.516\n",
      "    load_time_ms: 0.203\n",
      "    training_iteration_time_ms: 3481.525\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1657049997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1752\n",
      "  training_iteration: 219\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:39:57 (running for 00:14:52.94)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         868.478</td><td style=\"text-align: right;\">1752</td><td style=\"text-align: right;\">-0.330349</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:02 (running for 00:14:57.94)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         868.478</td><td style=\"text-align: right;\">1752</td><td style=\"text-align: right;\">-0.330349</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">            -39.8294</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1760\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1760\n",
      "    num_agent_steps_trained: 1760\n",
      "    num_env_steps_sampled: 1760\n",
      "    num_env_steps_trained: 1760\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-03\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.14217387800817488\n",
      "  episode_reward_min: -39.82938783525944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 586\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.3350275696487388\n",
      "    episode_reward_mean: -1.3350275696487388\n",
      "    episode_reward_min: -1.3350275696487388\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3350275696487388\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1442898484997283\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 889.3945450173285\n",
      "      mean_inference_ms: 1.5160983666441494\n",
      "      mean_raw_obs_processing_ms: 0.661772892887431\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.377658724784851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.9723209081566893e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.18449418246746063\n",
      "          total_loss: 8.77420711517334\n",
      "          vf_explained_var: 0.00038697718991898\n",
      "          vf_loss: 8.589712142944336\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1760\n",
      "    num_agent_steps_trained: 1760\n",
      "    num_env_steps_sampled: 1760\n",
      "    num_env_steps_trained: 1760\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1760\n",
      "  num_agent_steps_trained: 1760\n",
      "  num_env_steps_sampled: 1760\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1760\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.4625\n",
      "    ram_util_percent: 40.60000000000001\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14385869291058812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.344022803795\n",
      "    mean_inference_ms: 1.205135036779038\n",
      "    mean_raw_obs_processing_ms: 1.899251620739979\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.14217387800817488\n",
      "    episode_reward_min: -39.82938783525944\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.8963657684230029\n",
      "      - -39.82938783525944\n",
      "      - -0.03775552007020799\n",
      "      - -0.05320960838018918\n",
      "      - 23.729311372827855\n",
      "      - 0.06879703735012221\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14385869291058812\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.344022803795\n",
      "      mean_inference_ms: 1.205135036779038\n",
      "      mean_raw_obs_processing_ms: 1.899251620739979\n",
      "  time_since_restore: 874.3215346336365\n",
      "  time_this_iter_s: 5.843343257904053\n",
      "  time_total_s: 874.3215346336365\n",
      "  timers:\n",
      "    learn_throughput: 159.267\n",
      "    learn_time_ms: 50.23\n",
      "    load_throughput: 40935.015\n",
      "    load_time_ms: 0.195\n",
      "    training_iteration_time_ms: 3422.809\n",
      "    update_time_ms: 2.409\n",
      "  timestamp: 1657050003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1760\n",
      "  training_iteration: 220\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1776\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1776\n",
      "    num_agent_steps_trained: 1776\n",
      "    num_env_steps_sampled: 1776\n",
      "    num_env_steps_trained: 1776\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.47754320835136\n",
      "  episode_reward_mean: -0.00927507738837825\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 592\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3727507591247559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8073075100110145e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5396856665611267\n",
      "          total_loss: 2.647799491882324\n",
      "          vf_explained_var: -0.00028684138669632375\n",
      "          vf_loss: 3.187485694885254\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1776\n",
      "    num_agent_steps_trained: 1776\n",
      "    num_env_steps_sampled: 1776\n",
      "    num_env_steps_trained: 1776\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1776\n",
      "  num_agent_steps_trained: 1776\n",
      "  num_env_steps_sampled: 1776\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1776\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.825\n",
      "    ram_util_percent: 40.7\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14390325691752337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.4602120085497\n",
      "    mean_inference_ms: 1.2050569999807441\n",
      "    mean_raw_obs_processing_ms: 1.9077329750090797\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.47754320835136\n",
      "    episode_reward_mean: -0.00927507738837825\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -23.726180049445254\n",
      "      - -0.030545371873027083\n",
      "      - -0.019887898838931384\n",
      "      - 27.279763427472034\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14390325691752337\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.4602120085497\n",
      "      mean_inference_ms: 1.2050569999807441\n",
      "      mean_raw_obs_processing_ms: 1.9077329750090797\n",
      "  time_since_restore: 880.2787482738495\n",
      "  time_this_iter_s: 2.7612340450286865\n",
      "  time_total_s: 880.2787482738495\n",
      "  timers:\n",
      "    learn_throughput: 161.936\n",
      "    learn_time_ms: 49.402\n",
      "    load_throughput: 42372.057\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3333.038\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1657050009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1776\n",
      "  training_iteration: 222\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:09 (running for 00:15:04.86)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         880.279</td><td style=\"text-align: right;\">1776</td><td style=\"text-align: right;\">-0.00927508</td><td style=\"text-align: right;\">             35.4775</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1792\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1792\n",
      "    num_agent_steps_trained: 1792\n",
      "    num_env_steps_sampled: 1792\n",
      "    num_env_steps_trained: 1792\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.4137870467136473\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 596\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3726792335510254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.52436756354291e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.48741334676742554\n",
      "          total_loss: 3.5551540851593018\n",
      "          vf_explained_var: 0.0023825527168810368\n",
      "          vf_loss: 4.042567253112793\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1792\n",
      "    num_agent_steps_trained: 1792\n",
      "    num_env_steps_sampled: 1792\n",
      "    num_env_steps_trained: 1792\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1792\n",
      "  num_agent_steps_trained: 1792\n",
      "  num_env_steps_sampled: 1792\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1792\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.65\n",
      "    ram_util_percent: 40.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14389214776365178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.7014236600974\n",
      "    mean_inference_ms: 1.2048044339924617\n",
      "    mean_raw_obs_processing_ms: 1.9031472757003756\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.4137870467136473\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.021684615169693888\n",
      "      - -27.281473561622725\n",
      "      - -0.08404510763523976\n",
      "      - -0.0325724886830987\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14389214776365178\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.7014236600974\n",
      "      mean_inference_ms: 1.2048044339924617\n",
      "      mean_raw_obs_processing_ms: 1.9031472757003756\n",
      "  time_since_restore: 885.8128545284271\n",
      "  time_this_iter_s: 2.720766544342041\n",
      "  time_total_s: 885.8128545284271\n",
      "  timers:\n",
      "    learn_throughput: 167.737\n",
      "    learn_time_ms: 47.694\n",
      "    load_throughput: 42217.453\n",
      "    load_time_ms: 0.189\n",
      "    training_iteration_time_ms: 3275.89\n",
      "    update_time_ms: 2.159\n",
      "  timestamp: 1657050015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1792\n",
      "  training_iteration: 224\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:15 (running for 00:15:10.43)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         885.813</td><td style=\"text-align: right;\">1792</td><td style=\"text-align: right;\">0.413787</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:20 (running for 00:15:15.46)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         885.813</td><td style=\"text-align: right;\">1792</td><td style=\"text-align: right;\">0.413787</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1800\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1800\n",
      "    num_agent_steps_trained: 1800\n",
      "    num_env_steps_sampled: 1800\n",
      "    num_env_steps_trained: 1800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-20\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.6285659443428805\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 600\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.12051406057169722\n",
      "    episode_reward_mean: -0.12051406057169722\n",
      "    episode_reward_min: -0.12051406057169722\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.12051406057169722\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14391541481018066\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 889.085944961099\n",
      "      mean_inference_ms: 1.508269239874447\n",
      "      mean_raw_obs_processing_ms: 0.6619060740751378\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3766041994094849\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.123387043364346e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.4351630210876465\n",
      "          total_loss: 10.435162544250488\n",
      "          vf_explained_var: 0.0012359400279819965\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1800\n",
      "    num_agent_steps_trained: 1800\n",
      "    num_env_steps_sampled: 1800\n",
      "    num_env_steps_trained: 1800\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1800\n",
      "  num_agent_steps_trained: 1800\n",
      "  num_env_steps_sampled: 1800\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1800\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.21428571428571\n",
      "    ram_util_percent: 40.800000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14388133030026973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.8773169569436\n",
      "    mean_inference_ms: 1.2045554404164847\n",
      "    mean_raw_obs_processing_ms: 1.8986982559295467\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.6285659443428805\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.09912426053168644\n",
      "      - 0.04183131768556114\n",
      "      - -0.01889518420745684\n",
      "      - 0.05605469674731345\n",
      "      - -0.11988951153589777\n",
      "      - -0.05329485128616762\n",
      "      - 0.024019255904593084\n",
      "      - -0.00983681488278143\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14388133030026973\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.8773169569436\n",
      "      mean_inference_ms: 1.2045554404164847\n",
      "      mean_raw_obs_processing_ms: 1.8986982559295467\n",
      "  time_since_restore: 890.8966343402863\n",
      "  time_this_iter_s: 5.083779811859131\n",
      "  time_total_s: 890.8966343402863\n",
      "  timers:\n",
      "    learn_throughput: 168.494\n",
      "    learn_time_ms: 47.48\n",
      "    load_throughput: 44413.543\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3135.655\n",
      "    update_time_ms: 2.173\n",
      "  timestamp: 1657050020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1800\n",
      "  training_iteration: 225\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:25 (running for 00:15:20.52)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         895.795</td><td style=\"text-align: right;\">1816</td><td style=\"text-align: right;\">0.299064</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1824\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1824\n",
      "    num_agent_steps_trained: 1824\n",
      "    num_env_steps_sampled: 1824\n",
      "    num_env_steps_trained: 1824\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-29\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.014320922032052716\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 608\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.379868745803833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.753112524573226e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.22206343710422516\n",
      "          total_loss: 2.222308397293091\n",
      "          vf_explained_var: -0.0017144441371783614\n",
      "          vf_loss: 2.0002450942993164\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1824\n",
      "    num_agent_steps_trained: 1824\n",
      "    num_env_steps_sampled: 1824\n",
      "    num_env_steps_trained: 1824\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1824\n",
      "  num_agent_steps_trained: 1824\n",
      "  num_env_steps_sampled: 1824\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1824\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.4\n",
      "    ram_util_percent: 40.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1438305953560643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.00779011861\n",
      "    mean_inference_ms: 1.20388451364619\n",
      "    mean_raw_obs_processing_ms: 1.8899873352437828\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.014320922032052716\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3513396045452835\n",
      "      - 0.007565929282716266\n",
      "      - 0.010165370163700782\n",
      "      - -0.0006406467425044138\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1438305953560643\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.00779011861\n",
      "      mean_inference_ms: 1.20388451364619\n",
      "      mean_raw_obs_processing_ms: 1.8899873352437828\n",
      "  time_since_restore: 899.5585906505585\n",
      "  time_this_iter_s: 3.763991594314575\n",
      "  time_total_s: 899.5585906505585\n",
      "  timers:\n",
      "    learn_throughput: 165.343\n",
      "    learn_time_ms: 48.384\n",
      "    load_throughput: 35084.099\n",
      "    load_time_ms: 0.228\n",
      "    training_iteration_time_ms: 2908.374\n",
      "    update_time_ms: 2.035\n",
      "  timestamp: 1657050029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1824\n",
      "  training_iteration: 228\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:32 (running for 00:15:27.37)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         902.543</td><td style=\"text-align: right;\">1832</td><td style=\"text-align: right;\">0.646147</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:37 (running for 00:15:32.37)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         902.543</td><td style=\"text-align: right;\">1832</td><td style=\"text-align: right;\">0.646147</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1840\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1840\n",
      "    num_agent_steps_trained: 1840\n",
      "    num_env_steps_sampled: 1840\n",
      "    num_env_steps_trained: 1840\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-38\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.01425847354261954\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 612\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.006168259268979126\n",
      "    episode_reward_mean: 0.006168259268979126\n",
      "    episode_reward_min: 0.006168259268979126\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.006168259268979126\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14410773627191995\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 888.5318495386796\n",
      "      mean_inference_ms: 1.504654506985232\n",
      "      mean_raw_obs_processing_ms: 0.6672787151748327\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3793175220489502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.9055154199304525e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3431984782218933\n",
      "          total_loss: 4.3436360359191895\n",
      "          vf_explained_var: 0.00012391209020279348\n",
      "          vf_loss: 4.000437259674072\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1840\n",
      "    num_agent_steps_trained: 1840\n",
      "    num_env_steps_sampled: 1840\n",
      "    num_env_steps_trained: 1840\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1840\n",
      "  num_agent_steps_trained: 1840\n",
      "  num_env_steps_sampled: 1840\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1840\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.0625\n",
      "    ram_util_percent: 41.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14380026052422468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.9576737694277\n",
      "    mean_inference_ms: 1.203506347501494\n",
      "    mean_raw_obs_processing_ms: 1.8857376163708468\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.01425847354261954\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.032975321102985955\n",
      "      - -0.003201687144305132\n",
      "      - 0.014174091894092133\n",
      "      - 0.0016288973291544995\n",
      "      - -0.058088321506351126\n",
      "      - 0.04802169242953713\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14380026052422468\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.9576737694277\n",
      "      mean_inference_ms: 1.203506347501494\n",
      "      mean_raw_obs_processing_ms: 1.8857376163708468\n",
      "  time_since_restore: 908.529244184494\n",
      "  time_this_iter_s: 5.985955476760864\n",
      "  time_total_s: 908.529244184494\n",
      "  timers:\n",
      "    learn_throughput: 166.04\n",
      "    learn_time_ms: 48.181\n",
      "    load_throughput: 35967.877\n",
      "    load_time_ms: 0.222\n",
      "    training_iteration_time_ms: 2892.659\n",
      "    update_time_ms: 2.041\n",
      "  timestamp: 1657050038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1840\n",
      "  training_iteration: 230\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:42 (running for 00:15:37.59)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         912.676</td><td style=\"text-align: right;\">1848</td><td style=\"text-align: right;\"> 0.30357</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1856\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1856\n",
      "    num_agent_steps_trained: 1856\n",
      "    num_env_steps_sampled: 1856\n",
      "    num_env_steps_trained: 1856\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.29132915085342803\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 618\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3807545900344849\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4007255231263116e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.16882209479808807\n",
      "          total_loss: 0.8712654113769531\n",
      "          vf_explained_var: 0.020723531022667885\n",
      "          vf_loss: 1.0400874614715576\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1856\n",
      "    num_agent_steps_trained: 1856\n",
      "    num_env_steps_sampled: 1856\n",
      "    num_env_steps_trained: 1856\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1856\n",
      "  num_agent_steps_trained: 1856\n",
      "  num_env_steps_sampled: 1856\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1856\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.966666666666665\n",
      "    ram_util_percent: 41.11666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1436928543774779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.1451036320422\n",
      "    mean_inference_ms: 1.202547227645487\n",
      "    mean_raw_obs_processing_ms: 1.864906793684103\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.29132915085342803\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3191954188030932\n",
      "      - 0.06498624553725207\n",
      "      - 0.0014006953770079322\n",
      "      - -1.3266637448712202\n",
      "      - -0.008137932662487413\n",
      "      - 0.006998678075989306\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1436928543774779\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.1451036320422\n",
      "      mean_inference_ms: 1.202547227645487\n",
      "      mean_raw_obs_processing_ms: 1.864906793684103\n",
      "  time_since_restore: 916.3843455314636\n",
      "  time_this_iter_s: 3.708479166030884\n",
      "  time_total_s: 916.3843455314636\n",
      "  timers:\n",
      "    learn_throughput: 167.356\n",
      "    learn_time_ms: 47.802\n",
      "    load_throughput: 36018.068\n",
      "    load_time_ms: 0.222\n",
      "    training_iteration_time_ms: 3082.488\n",
      "    update_time_ms: 1.981\n",
      "  timestamp: 1657050046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1856\n",
      "  training_iteration: 232\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:49 (running for 00:15:44.52)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         919.564</td><td style=\"text-align: right;\">1864</td><td style=\"text-align: right;\">0.239915</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1872\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1872\n",
      "    num_agent_steps_trained: 1872\n",
      "    num_env_steps_sampled: 1872\n",
      "    num_env_steps_trained: 1872\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-53\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.026129058597161456\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 624\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3854104280471802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.9339263417350594e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.22531892359256744\n",
      "          total_loss: 2.2268826961517334\n",
      "          vf_explained_var: -0.00031089383992366493\n",
      "          vf_loss: 2.001563549041748\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1872\n",
      "    num_agent_steps_trained: 1872\n",
      "    num_env_steps_sampled: 1872\n",
      "    num_env_steps_trained: 1872\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1872\n",
      "  num_agent_steps_trained: 1872\n",
      "  num_env_steps_sampled: 1872\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1872\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.433333333333337\n",
      "    ram_util_percent: 41.199999999999996\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14369350293406516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.5771048125564\n",
      "    mean_inference_ms: 1.2021796085070513\n",
      "    mean_raw_obs_processing_ms: 1.8734412017894502\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.026129058597161456\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0010264562064958849\n",
      "      - 27.91951813902773\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14369350293406516\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.5771048125564\n",
      "      mean_inference_ms: 1.2021796085070513\n",
      "      mean_raw_obs_processing_ms: 1.8734412017894502\n",
      "  time_since_restore: 923.4267468452454\n",
      "  time_this_iter_s: 3.8622705936431885\n",
      "  time_total_s: 923.4267468452454\n",
      "  timers:\n",
      "    learn_throughput: 165.38\n",
      "    learn_time_ms: 48.374\n",
      "    load_throughput: 33104.215\n",
      "    load_time_ms: 0.242\n",
      "    training_iteration_time_ms: 3233.238\n",
      "    update_time_ms: 2.034\n",
      "  timestamp: 1657050053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1872\n",
      "  training_iteration: 234\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:40:58 (running for 00:15:53.45)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         923.427</td><td style=\"text-align: right;\">1872</td><td style=\"text-align: right;\">0.0261291</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1880\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1880\n",
      "    num_agent_steps_trained: 1880\n",
      "    num_env_steps_sampled: 1880\n",
      "    num_env_steps_trained: 1880\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-40-58\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.0330806525141997\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 626\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.007635691293326863\n",
      "    episode_reward_mean: -0.007635691293326863\n",
      "    episode_reward_min: -0.007635691293326863\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.007635691293326863\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14399978476510922\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 884.8766911197716\n",
      "      mean_inference_ms: 1.4975272433858524\n",
      "      mean_raw_obs_processing_ms: 0.6664773108254016\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3849798440933228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.183920489391312e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.23621448874473572\n",
      "          total_loss: 1.791894793510437\n",
      "          vf_explained_var: 0.0010351677192375064\n",
      "          vf_loss: 2.028109312057495\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1880\n",
      "    num_agent_steps_trained: 1880\n",
      "    num_env_steps_sampled: 1880\n",
      "    num_env_steps_trained: 1880\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1880\n",
      "  num_agent_steps_trained: 1880\n",
      "  num_env_steps_sampled: 1880\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1880\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.12857142857143\n",
      "    ram_util_percent: 41.24285714285714\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14361657859954083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.7996925684461\n",
      "    mean_inference_ms: 1.2016387429630668\n",
      "    mean_raw_obs_processing_ms: 1.8571174087497082\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.0330806525141997\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.010616193151597675\n",
      "      - 0.02003323721184591\n",
      "      - 2.6262060008736974\n",
      "      - 35.47754320835136\n",
      "      - -2.181969490251376\n",
      "      - -0.644742667015187\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14361657859954083\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.7996925684461\n",
      "      mean_inference_ms: 1.2016387429630668\n",
      "      mean_raw_obs_processing_ms: 1.8571174087497082\n",
      "  time_since_restore: 928.7315933704376\n",
      "  time_this_iter_s: 5.304846525192261\n",
      "  time_total_s: 928.7315933704376\n",
      "  timers:\n",
      "    learn_throughput: 166.621\n",
      "    learn_time_ms: 48.013\n",
      "    load_throughput: 33334.425\n",
      "    load_time_ms: 0.24\n",
      "    training_iteration_time_ms: 3303.362\n",
      "    update_time_ms: 2.024\n",
      "  timestamp: 1657050058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1880\n",
      "  training_iteration: 235\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1896\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1896\n",
      "    num_agent_steps_trained: 1896\n",
      "    num_env_steps_sampled: 1896\n",
      "    num_env_steps_trained: 1896\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.2985044212197179\n",
      "  episode_reward_min: -34.373019185827154\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 632\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3736793994903564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.511942501674639e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.05414450913667679\n",
      "          total_loss: 2.062842607498169\n",
      "          vf_explained_var: -0.002396619413048029\n",
      "          vf_loss: 2.008697986602783\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1896\n",
      "    num_agent_steps_trained: 1896\n",
      "    num_env_steps_sampled: 1896\n",
      "    num_env_steps_trained: 1896\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1896\n",
      "  num_agent_steps_trained: 1896\n",
      "  num_env_steps_sampled: 1896\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1896\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.560000000000002\n",
      "    ram_util_percent: 41.3\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14361684407724687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.1383905443482\n",
      "    mean_inference_ms: 1.2012946595980278\n",
      "    mean_raw_obs_processing_ms: 1.8656330374495749\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.2985044212197179\n",
      "    episode_reward_min: -34.373019185827154\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.25152619573474055\n",
      "      - -28.13614381799992\n",
      "      - -7.756596367203638\n",
      "      - 0.7830355702931051\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14361684407724687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.1383905443482\n",
      "      mean_inference_ms: 1.2012946595980278\n",
      "      mean_raw_obs_processing_ms: 1.8656330374495749\n",
      "  time_since_restore: 935.0928785800934\n",
      "  time_this_iter_s: 3.105018138885498\n",
      "  time_total_s: 935.0928785800934\n",
      "  timers:\n",
      "    learn_throughput: 169.582\n",
      "    learn_time_ms: 47.175\n",
      "    load_throughput: 38773.321\n",
      "    load_time_ms: 0.206\n",
      "    training_iteration_time_ms: 3449.719\n",
      "    update_time_ms: 2.081\n",
      "  timestamp: 1657050065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1896\n",
      "  training_iteration: 237\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:05 (running for 00:16:00.24)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         935.093</td><td style=\"text-align: right;\">1896</td><td style=\"text-align: right;\">-0.298504</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">             -34.373</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1912\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1912\n",
      "    num_agent_steps_trained: 1912\n",
      "    num_env_steps_sampled: 1912\n",
      "    num_env_steps_trained: 1912\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.25162236560275686\n",
      "  episode_reward_min: -37.57764111347744\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 636\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3789061307907104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.9258393119089305e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.2166845202445984\n",
      "          total_loss: 2.218311071395874\n",
      "          vf_explained_var: -0.00039915242814458907\n",
      "          vf_loss: 2.001626491546631\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1912\n",
      "    num_agent_steps_trained: 1912\n",
      "    num_env_steps_sampled: 1912\n",
      "    num_env_steps_trained: 1912\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1912\n",
      "  num_agent_steps_trained: 1912\n",
      "  num_env_steps_sampled: 1912\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1912\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.74\n",
      "    ram_util_percent: 41.3\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1435789096409391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.9764871081455\n",
      "    mean_inference_ms: 1.2008350026686179\n",
      "    mean_raw_obs_processing_ms: 1.8617349070529015\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.25162236560275686\n",
      "    episode_reward_min: -37.57764111347744\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.019591826795787193\n",
      "      - -26.52504174150708\n",
      "      - 0.018356061015162517\n",
      "      - -0.0893400645347433\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1435789096409391\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.9764871081455\n",
      "      mean_inference_ms: 1.2008350026686179\n",
      "      mean_raw_obs_processing_ms: 1.8617349070529015\n",
      "  time_since_restore: 942.1601037979126\n",
      "  time_this_iter_s: 3.4371631145477295\n",
      "  time_total_s: 942.1601037979126\n",
      "  timers:\n",
      "    learn_throughput: 168.925\n",
      "    learn_time_ms: 47.358\n",
      "    load_throughput: 41466.179\n",
      "    load_time_ms: 0.193\n",
      "    training_iteration_time_ms: 3481.492\n",
      "    update_time_ms: 2.156\n",
      "  timestamp: 1657050072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1912\n",
      "  training_iteration: 239\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:12 (running for 00:16:07.34)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">          942.16</td><td style=\"text-align: right;\">1912</td><td style=\"text-align: right;\">-0.251622</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.5776</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:17 (running for 00:16:12.37)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">          942.16</td><td style=\"text-align: right;\">1912</td><td style=\"text-align: right;\">-0.251622</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.5776</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1920\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1920\n",
      "    num_agent_steps_trained: 1920\n",
      "    num_env_steps_sampled: 1920\n",
      "    num_env_steps_trained: 1920\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.30097187981880535\n",
      "  episode_reward_min: -37.57764111347744\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 640\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.07867072788649698\n",
      "    episode_reward_mean: 0.07867072788649698\n",
      "    episode_reward_min: 0.07867072788649698\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.07867072788649698\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14352305182095232\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 888.1407277337436\n",
      "      mean_inference_ms: 1.4872912702889276\n",
      "      mean_raw_obs_processing_ms: 0.6665065370757004\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3779648542404175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.912855077534914e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.13698339462280273\n",
      "          total_loss: 5.863499164581299\n",
      "          vf_explained_var: -0.0017511367332190275\n",
      "          vf_loss: 6.00048303604126\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1920\n",
      "    num_agent_steps_trained: 1920\n",
      "    num_env_steps_sampled: 1920\n",
      "    num_env_steps_trained: 1920\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1920\n",
      "  num_agent_steps_trained: 1920\n",
      "  num_env_steps_sampled: 1920\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1920\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.46666666666667\n",
      "    ram_util_percent: 41.388888888888886\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14354671852968603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.8116839136221\n",
      "    mean_inference_ms: 1.2003873385542763\n",
      "    mean_raw_obs_processing_ms: 1.8579672880582025\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.30097187981880535\n",
      "    episode_reward_min: -37.57764111347744\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.03786317039394782\n",
      "      - -0.052298413865209614\n",
      "      - 0.06059330456286727\n",
      "      - -1.3283332624238304\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14354671852968603\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.8116839136221\n",
      "      mean_inference_ms: 1.2003873385542763\n",
      "      mean_raw_obs_processing_ms: 1.8579672880582025\n",
      "  time_since_restore: 948.6091687679291\n",
      "  time_this_iter_s: 6.4490649700164795\n",
      "  time_total_s: 948.6091687679291\n",
      "  timers:\n",
      "    learn_throughput: 167.303\n",
      "    learn_time_ms: 47.818\n",
      "    load_throughput: 40539.365\n",
      "    load_time_ms: 0.197\n",
      "    training_iteration_time_ms: 3474.381\n",
      "    update_time_ms: 2.157\n",
      "  timestamp: 1657050078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1920\n",
      "  training_iteration: 240\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1936\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1936\n",
      "    num_agent_steps_trained: 1936\n",
      "    num_env_steps_sampled: 1936\n",
      "    num_env_steps_trained: 1936\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-25\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.40017352687569013\n",
      "  episode_reward_min: -37.57764111347744\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 644\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3831008672714233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.491816525842296e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5828539729118347\n",
      "          total_loss: -0.3099533021450043\n",
      "          vf_explained_var: 0.00469317426905036\n",
      "          vf_loss: 0.27290067076683044\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1936\n",
      "    num_agent_steps_trained: 1936\n",
      "    num_env_steps_sampled: 1936\n",
      "    num_env_steps_trained: 1936\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1936\n",
      "  num_agent_steps_trained: 1936\n",
      "  num_env_steps_sampled: 1936\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1936\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.360000000000003\n",
      "    ram_util_percent: 41.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14351571806497387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.6274934438488\n",
      "    mean_inference_ms: 1.1999425868150988\n",
      "    mean_raw_obs_processing_ms: 1.8542818274563841\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.40017352687569013\n",
      "    episode_reward_min: -37.57764111347744\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0011777666439983792\n",
      "      - 0.006681241219468781\n",
      "      - 0.05474599473736952\n",
      "      - 0.00039313347433200807\n",
      "      - -0.05167486172569913\n",
      "      - 0.011305006965128594\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14351571806497387\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.6274934438488\n",
      "      mean_inference_ms: 1.1999425868150988\n",
      "      mean_raw_obs_processing_ms: 1.8542818274563841\n",
      "  time_since_restore: 955.3319919109344\n",
      "  time_this_iter_s: 3.4204771518707275\n",
      "  time_total_s: 955.3319919109344\n",
      "  timers:\n",
      "    learn_throughput: 165.698\n",
      "    learn_time_ms: 48.281\n",
      "    load_throughput: 38921.74\n",
      "    load_time_ms: 0.206\n",
      "    training_iteration_time_ms: 3361.038\n",
      "    update_time_ms: 2.153\n",
      "  timestamp: 1657050085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1936\n",
      "  training_iteration: 242\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:25 (running for 00:16:20.64)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         955.332</td><td style=\"text-align: right;\">1936</td><td style=\"text-align: right;\">0.400174</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.5776</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1952\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1952\n",
      "    num_agent_steps_trained: 1952\n",
      "    num_env_steps_sampled: 1952\n",
      "    num_env_steps_trained: 1952\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.02585272673736875\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 650\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3837971687316895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.4715167203103192e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.006093600764870644\n",
      "          total_loss: 0.6909956336021423\n",
      "          vf_explained_var: 0.0029791593551635742\n",
      "          vf_loss: 0.6849020719528198\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1952\n",
      "    num_agent_steps_trained: 1952\n",
      "    num_env_steps_sampled: 1952\n",
      "    num_env_steps_trained: 1952\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1952\n",
      "  num_agent_steps_trained: 1952\n",
      "  num_env_steps_sampled: 1952\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1952\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.880000000000003\n",
      "    ram_util_percent: 41.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14340696698205244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.6450805252366\n",
      "    mean_inference_ms: 1.1989552041972744\n",
      "    mean_raw_obs_processing_ms: 1.8351301209854813\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.02585272673736875\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.003139168149247973\n",
      "      - 27.28372637881457\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14340696698205244\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.6450805252366\n",
      "      mean_inference_ms: 1.1989552041972744\n",
      "      mean_raw_obs_processing_ms: 1.8351301209854813\n",
      "  time_since_restore: 962.4242770671844\n",
      "  time_this_iter_s: 3.403742551803589\n",
      "  time_total_s: 962.4242770671844\n",
      "  timers:\n",
      "    learn_throughput: 173.201\n",
      "    learn_time_ms: 46.189\n",
      "    load_throughput: 42441.73\n",
      "    load_time_ms: 0.188\n",
      "    training_iteration_time_ms: 3365.995\n",
      "    update_time_ms: 2.117\n",
      "  timestamp: 1657050092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1952\n",
      "  training_iteration: 244\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:32 (running for 00:16:27.80)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         962.424</td><td style=\"text-align: right;\">1952</td><td style=\"text-align: right;\">0.0258527</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:37 (running for 00:16:32.81)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         962.424</td><td style=\"text-align: right;\">1952</td><td style=\"text-align: right;\">0.0258527</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1960\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1960\n",
      "    num_agent_steps_trained: 1960\n",
      "    num_env_steps_sampled: 1960\n",
      "    num_env_steps_trained: 1960\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-38\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.24633499087692118\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 652\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.05122246524477703\n",
      "    episode_reward_mean: 0.05122246524477703\n",
      "    episode_reward_min: 0.05122246524477703\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.05122246524477703\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14361014237274997\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 882.5351805300327\n",
      "      mean_inference_ms: 1.480606762138573\n",
      "      mean_raw_obs_processing_ms: 0.6673770981865961\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3837319612503052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.44183811446419e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.39192676544189453\n",
      "          total_loss: -0.3856024742126465\n",
      "          vf_explained_var: 0.12526623904705048\n",
      "          vf_loss: 0.006324336864054203\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1960\n",
      "    num_agent_steps_trained: 1960\n",
      "    num_env_steps_sampled: 1960\n",
      "    num_env_steps_trained: 1960\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1960\n",
      "  num_agent_steps_trained: 1960\n",
      "  num_env_steps_sampled: 1960\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1960\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.6875\n",
      "    ram_util_percent: 41.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1434486410412714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.2091267514987\n",
      "    mean_inference_ms: 1.199063216031986\n",
      "    mean_raw_obs_processing_ms: 1.8470719143491277\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.24633499087692118\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.016600175368748804\n",
      "      - -27.285853426699088\n",
      "      - 1.3650822410933472\n",
      "      - -0.05573623598763511\n",
      "      - -1.3632512008428075\n",
      "      - 27.849323521772703\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1434486410412714\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.2091267514987\n",
      "      mean_inference_ms: 1.199063216031986\n",
      "      mean_raw_obs_processing_ms: 1.8470719143491277\n",
      "  time_since_restore: 968.3359413146973\n",
      "  time_this_iter_s: 5.911664247512817\n",
      "  time_total_s: 968.3359413146973\n",
      "  timers:\n",
      "    learn_throughput: 173.047\n",
      "    learn_time_ms: 46.23\n",
      "    load_throughput: 41292.68\n",
      "    load_time_ms: 0.194\n",
      "    training_iteration_time_ms: 3457.599\n",
      "    update_time_ms: 2.126\n",
      "  timestamp: 1657050098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1960\n",
      "  training_iteration: 245\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1976\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1976\n",
      "    num_agent_steps_trained: 1976\n",
      "    num_env_steps_sampled: 1976\n",
      "    num_env_steps_trained: 1976\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.03380724300335469\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 658\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.377164363861084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.4912613854394294e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.24988217651844025\n",
      "          total_loss: 1.7527546882629395\n",
      "          vf_explained_var: 0.00263789901509881\n",
      "          vf_loss: 2.002636671066284\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1976\n",
      "    num_agent_steps_trained: 1976\n",
      "    num_env_steps_sampled: 1976\n",
      "    num_env_steps_trained: 1976\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1976\n",
      "  num_agent_steps_trained: 1976\n",
      "  num_env_steps_sampled: 1976\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1976\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.1\n",
      "    ram_util_percent: 41.7\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14333961495061445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.1976255031649\n",
      "    mean_inference_ms: 1.1981207912942229\n",
      "    mean_raw_obs_processing_ms: 1.8282515497441154\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.03380724300335469\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.00021715742998229537\n",
      "      - 0.013212396525026149\n",
      "      - 2.71911600244076\n",
      "      - -0.008708450212787766\n",
      "      - -4.452194907116745\n",
      "      - 0.05502507469557805\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14333961495061445\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.1976255031649\n",
      "      mean_inference_ms: 1.1981207912942229\n",
      "      mean_raw_obs_processing_ms: 1.8282515497441154\n",
      "  time_since_restore: 975.567628622055\n",
      "  time_this_iter_s: 3.6266534328460693\n",
      "  time_total_s: 975.567628622055\n",
      "  timers:\n",
      "    learn_throughput: 175.369\n",
      "    learn_time_ms: 45.618\n",
      "    load_throughput: 43662.241\n",
      "    load_time_ms: 0.183\n",
      "    training_iteration_time_ms: 3544.757\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1657050105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1976\n",
      "  training_iteration: 247\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:45 (running for 00:16:41.06)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         975.568</td><td style=\"text-align: right;\">1976</td><td style=\"text-align: right;\">0.0338072</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 1992\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 1992\n",
      "    num_agent_steps_trained: 1992\n",
      "    num_env_steps_sampled: 1992\n",
      "    num_env_steps_trained: 1992\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.24743294098204544\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 664\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3820239305496216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2890740183356684e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5310252904891968\n",
      "          total_loss: 1.4703408479690552\n",
      "          vf_explained_var: -8.618831634521484e-05\n",
      "          vf_loss: 2.001366138458252\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 1992\n",
      "    num_agent_steps_trained: 1992\n",
      "    num_env_steps_sampled: 1992\n",
      "    num_env_steps_trained: 1992\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 1992\n",
      "  num_agent_steps_trained: 1992\n",
      "  num_env_steps_sampled: 1992\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 1992\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.63333333333333\n",
      "    ram_util_percent: 41.76666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14335239280651407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.5248794051063\n",
      "    mean_inference_ms: 1.1978315889705842\n",
      "    mean_raw_obs_processing_ms: 1.836629583794243\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.24743294098204544\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 4.5012044933135655\n",
      "      - -7.574292027694579\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14335239280651407\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.5248794051063\n",
      "      mean_inference_ms: 1.1978315889705842\n",
      "      mean_raw_obs_processing_ms: 1.836629583794243\n",
      "  time_since_restore: 983.3803038597107\n",
      "  time_this_iter_s: 4.240804433822632\n",
      "  time_total_s: 983.3803038597107\n",
      "  timers:\n",
      "    learn_throughput: 174.707\n",
      "    learn_time_ms: 45.791\n",
      "    load_throughput: 42842.737\n",
      "    load_time_ms: 0.187\n",
      "    training_iteration_time_ms: 3619.209\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1657050113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1992\n",
      "  training_iteration: 249\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:53 (running for 00:16:48.98)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">          983.38</td><td style=\"text-align: right;\">1992</td><td style=\"text-align: right;\">-0.247433</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:41:58 (running for 00:16:53.99)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">          983.38</td><td style=\"text-align: right;\">1992</td><td style=\"text-align: right;\">-0.247433</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_env_steps_sampled: 2000\n",
      "    num_env_steps_trained: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-41-59\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.21777695670783956\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 666\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.011326670361100355\n",
      "    episode_reward_mean: -0.011326670361100355\n",
      "    episode_reward_min: -0.011326670361100355\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.011326670361100355\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1433148289358379\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 880.2706816338546\n",
      "      mean_inference_ms: 1.4729262977246418\n",
      "      mean_raw_obs_processing_ms: 0.6671757098065307\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3807214498519897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.019809683901258e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.4436664879322052\n",
      "          total_loss: 0.44629642367362976\n",
      "          vf_explained_var: -0.36822307109832764\n",
      "          vf_loss: 0.0026298800949007273\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_env_steps_sampled: 2000\n",
      "    num_env_steps_trained: 2000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2000\n",
      "  num_agent_steps_trained: 2000\n",
      "  num_env_steps_sampled: 2000\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2000\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.166666666666668\n",
      "    ram_util_percent: 41.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14327286661916236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.8274798878781\n",
      "    mean_inference_ms: 1.197305510435845\n",
      "    mean_raw_obs_processing_ms: 1.8215991500019177\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.21777695670783956\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 34.42674575437887\n",
      "      - -0.06156793061457222\n",
      "      - 5.0939937118099685\n",
      "      - -34.372421050176285\n",
      "      - -5.140784484244882\n",
      "      - 0.010032886163614974\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14327286661916236\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.8274798878781\n",
      "      mean_inference_ms: 1.197305510435845\n",
      "      mean_raw_obs_processing_ms: 1.8215991500019177\n",
      "  time_since_restore: 989.3292429447174\n",
      "  time_this_iter_s: 5.948939085006714\n",
      "  time_total_s: 989.3292429447174\n",
      "  timers:\n",
      "    learn_throughput: 175.348\n",
      "    learn_time_ms: 45.624\n",
      "    load_throughput: 43890.689\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3651.302\n",
      "    update_time_ms: 2.094\n",
      "  timestamp: 1657050119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 250\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2016\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2016\n",
      "    num_agent_steps_trained: 2016\n",
      "    num_env_steps_sampled: 2016\n",
      "    num_env_steps_trained: 2016\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-07\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.2179050829050634\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 672\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3777539730072021\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.5628571418346837e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.28405922651290894\n",
      "          total_loss: -0.2826792597770691\n",
      "          vf_explained_var: 0.07749313861131668\n",
      "          vf_loss: 0.0013799025909975171\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2016\n",
      "    num_agent_steps_trained: 2016\n",
      "    num_env_steps_sampled: 2016\n",
      "    num_env_steps_trained: 2016\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2016\n",
      "  num_agent_steps_trained: 2016\n",
      "  num_env_steps_sampled: 2016\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2016\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.46\n",
      "    ram_util_percent: 41.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14328076201457962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.2492206405421\n",
      "    mean_inference_ms: 1.1969295466673353\n",
      "    mean_raw_obs_processing_ms: 1.8299574773010454\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.2179050829050634\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 7.588392686741738\n",
      "      - -0.11364824856600464\n",
      "      - -0.06604125414358342\n",
      "      - 0.0705691255146933\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14328076201457962\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.2492206405421\n",
      "      mean_inference_ms: 1.1969295466673353\n",
      "      mean_raw_obs_processing_ms: 1.8299574773010454\n",
      "  time_since_restore: 996.6234157085419\n",
      "  time_this_iter_s: 3.6384756565093994\n",
      "  time_total_s: 996.6234157085419\n",
      "  timers:\n",
      "    learn_throughput: 173.645\n",
      "    learn_time_ms: 46.071\n",
      "    load_throughput: 44454.732\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3708.578\n",
      "    update_time_ms: 2.129\n",
      "  timestamp: 1657050127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2016\n",
      "  training_iteration: 252\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:07 (running for 00:17:02.57)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         996.623</td><td style=\"text-align: right;\">2016</td><td style=\"text-align: right;\">-0.217905</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2032\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2032\n",
      "    num_agent_steps_trained: 2032\n",
      "    num_env_steps_sampled: 2032\n",
      "    num_env_steps_trained: 2032\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.004438960801534759\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 676\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3833101987838745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4591651961382013e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.10728109627962112\n",
      "          total_loss: 4.4417405128479\n",
      "          vf_explained_var: 0.000427289807703346\n",
      "          vf_loss: 4.5490217208862305\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2032\n",
      "    num_agent_steps_trained: 2032\n",
      "    num_env_steps_sampled: 2032\n",
      "    num_env_steps_trained: 2032\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2032\n",
      "  num_agent_steps_trained: 2032\n",
      "  num_env_steps_sampled: 2032\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2032\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.419999999999998\n",
      "    ram_util_percent: 42.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14323650190749457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.1688711382125\n",
      "    mean_inference_ms: 1.1963966799012706\n",
      "    mean_raw_obs_processing_ms: 1.8266906134523426\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.004438960801534759\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -2.208763582066897\n",
      "      - -0.008179670872472955\n",
      "      - 0.0978832401232026\n",
      "      - 0.00873129780476034\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14323650190749457\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.1688711382125\n",
      "      mean_inference_ms: 1.1963966799012706\n",
      "      mean_raw_obs_processing_ms: 1.8266906134523426\n",
      "  time_since_restore: 1004.0061798095703\n",
      "  time_this_iter_s: 3.732800006866455\n",
      "  time_total_s: 1004.0061798095703\n",
      "  timers:\n",
      "    learn_throughput: 170.554\n",
      "    learn_time_ms: 46.906\n",
      "    load_throughput: 40751.071\n",
      "    load_time_ms: 0.196\n",
      "    training_iteration_time_ms: 3737.786\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1657050134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2032\n",
      "  training_iteration: 254\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:14 (running for 00:17:10.00)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         1004.01</td><td style=\"text-align: right;\">2032</td><td style=\"text-align: right;\">-0.00443896</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:19 (running for 00:17:15.03)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         1004.01</td><td style=\"text-align: right;\">2032</td><td style=\"text-align: right;\">-0.00443896</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2040\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2040\n",
      "    num_agent_steps_trained: 2040\n",
      "    num_env_steps_sampled: 2040\n",
      "    num_env_steps_trained: 2040\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.04711438838188576\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 680\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.07572814757499957\n",
      "    episode_reward_mean: -0.07572814757499957\n",
      "    episode_reward_min: -0.07572814757499957\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.07572814757499957\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14325550624302455\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 881.269055527526\n",
      "      mean_inference_ms: 1.4705518623451133\n",
      "      mean_raw_obs_processing_ms: 0.6679389383885768\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.382524013519287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.534262972010765e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.16347765922546387\n",
      "          total_loss: 3.5922203063964844\n",
      "          vf_explained_var: -0.003157444763928652\n",
      "          vf_loss: 3.4287426471710205\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2040\n",
      "    num_agent_steps_trained: 2040\n",
      "    num_env_steps_sampled: 2040\n",
      "    num_env_steps_trained: 2040\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2040\n",
      "  num_agent_steps_trained: 2040\n",
      "  num_env_steps_sampled: 2040\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2040\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.81111111111111\n",
      "    ram_util_percent: 42.01111111111111\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14319092909947778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.0771712993097\n",
      "    mean_inference_ms: 1.1958536902571533\n",
      "    mean_raw_obs_processing_ms: 1.8234740098806261\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.04711438838188576\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 2.437769895596425\n",
      "      - -0.3754424571653949\n",
      "      - -0.011991693237953904\n",
      "      - 0.0034592384734172255\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14319092909947778\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.0771712993097\n",
      "      mean_inference_ms: 1.1958536902571533\n",
      "      mean_raw_obs_processing_ms: 1.8234740098806261\n",
      "  time_since_restore: 1010.102963924408\n",
      "  time_this_iter_s: 6.0967841148376465\n",
      "  time_total_s: 1010.102963924408\n",
      "  timers:\n",
      "    learn_throughput: 166.329\n",
      "    learn_time_ms: 48.097\n",
      "    load_throughput: 40632.637\n",
      "    load_time_ms: 0.197\n",
      "    training_iteration_time_ms: 3660.253\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1657050140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2040\n",
      "  training_iteration: 255\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2056\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2056\n",
      "    num_agent_steps_trained: 2056\n",
      "    num_env_steps_sampled: 2056\n",
      "    num_env_steps_trained: 2056\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: -0.292086612092052\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 684\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3795204162597656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.130068974627648e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.2364056408405304\n",
      "          total_loss: 2.239126682281494\n",
      "          vf_explained_var: 0.0018203755607828498\n",
      "          vf_loss: 2.002720832824707\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2056\n",
      "    num_agent_steps_trained: 2056\n",
      "    num_env_steps_sampled: 2056\n",
      "    num_env_steps_trained: 2056\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2056\n",
      "  num_agent_steps_trained: 2056\n",
      "  num_env_steps_sampled: 2056\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2056\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.96666666666667\n",
      "    ram_util_percent: 42.1\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1431410394598932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.9993220800083\n",
      "    mean_inference_ms: 1.19529875429365\n",
      "    mean_raw_obs_processing_ms: 1.8203094227122694\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: -0.292086612092052\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -2.4781281168041858\n",
      "      - -0.008297362680843356\n",
      "      - -5.976158042380728\n",
      "      - -0.05377295050943742\n",
      "      - 2.3816470297274464\n",
      "      - -0.13504447923504515\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1431410394598932\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.9993220800083\n",
      "      mean_inference_ms: 1.19529875429365\n",
      "      mean_raw_obs_processing_ms: 1.8203094227122694\n",
      "  time_since_restore: 1016.8650193214417\n",
      "  time_this_iter_s: 3.683494806289673\n",
      "  time_total_s: 1016.8650193214417\n",
      "  timers:\n",
      "    learn_throughput: 163.63\n",
      "    learn_time_ms: 48.891\n",
      "    load_throughput: 41272.364\n",
      "    load_time_ms: 0.194\n",
      "    training_iteration_time_ms: 3613.208\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1657050147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2056\n",
      "  training_iteration: 257\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:27 (running for 00:17:22.98)<br>Memory usage on this node: 13.0/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         1016.87</td><td style=\"text-align: right;\">2056</td><td style=\"text-align: right;\">-0.292087</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2072\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2072\n",
      "    num_agent_steps_trained: 2072\n",
      "    num_env_steps_sampled: 2072\n",
      "    num_env_steps_trained: 2072\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-35\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.4497330066186467\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 690\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3778153657913208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.7814227147900965e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.17002536356449127\n",
      "          total_loss: 2.171694040298462\n",
      "          vf_explained_var: -0.0010776002891361713\n",
      "          vf_loss: 2.0016684532165527\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2072\n",
      "    num_agent_steps_trained: 2072\n",
      "    num_env_steps_sampled: 2072\n",
      "    num_env_steps_trained: 2072\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2072\n",
      "  num_agent_steps_trained: 2072\n",
      "  num_env_steps_sampled: 2072\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2072\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.150000000000002\n",
      "    ram_util_percent: 42.2\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14300596225370343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.5247463308277\n",
      "    mean_inference_ms: 1.1941615759786235\n",
      "    mean_raw_obs_processing_ms: 1.8028676555146919\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.4497330066186467\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 34.42761736824973\n",
      "      - -34.373019185827154\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14300596225370343\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.5247463308277\n",
      "      mean_inference_ms: 1.1941615759786235\n",
      "      mean_raw_obs_processing_ms: 1.8028676555146919\n",
      "  time_since_restore: 1024.0965547561646\n",
      "  time_this_iter_s: 3.1547274589538574\n",
      "  time_total_s: 1024.0965547561646\n",
      "  timers:\n",
      "    learn_throughput: 165.823\n",
      "    learn_time_ms: 48.244\n",
      "    load_throughput: 41734.368\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 3555.312\n",
      "    update_time_ms: 2.014\n",
      "  timestamp: 1657050155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2072\n",
      "  training_iteration: 259\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:35 (running for 00:17:30.28)<br>Memory usage on this node: 13.0/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          1024.1</td><td style=\"text-align: right;\">2072</td><td style=\"text-align: right;\">0.449733</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:40 (running for 00:17:35.28)<br>Memory usage on this node: 13.0/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          1024.1</td><td style=\"text-align: right;\">2072</td><td style=\"text-align: right;\">0.449733</td><td style=\"text-align: right;\">             39.5136</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2080\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2080\n",
      "    num_agent_steps_trained: 2080\n",
      "    num_env_steps_sampled: 2080\n",
      "    num_env_steps_trained: 2080\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.51356837488812\n",
      "  episode_reward_mean: 0.4995845049465108\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 692\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.049920499213259806\n",
      "    episode_reward_mean: 0.049920499213259806\n",
      "    episode_reward_min: 0.049920499213259806\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.049920499213259806\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14277476413994078\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 881.1542775220933\n",
      "      mean_inference_ms: 1.4658930954659821\n",
      "      mean_raw_obs_processing_ms: 0.6673047497014332\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.382441759109497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.418293883645674e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.3786522448062897\n",
      "          total_loss: 3.871419668197632\n",
      "          vf_explained_var: 0.00048196513671427965\n",
      "          vf_loss: 4.250072002410889\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2080\n",
      "    num_agent_steps_trained: 2080\n",
      "    num_env_steps_sampled: 2080\n",
      "    num_env_steps_trained: 2080\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2080\n",
      "  num_agent_steps_trained: 2080\n",
      "  num_env_steps_sampled: 2080\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2080\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.75714285714286\n",
      "    ram_util_percent: 42.214285714285715\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14304250532156898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.9737115770308\n",
      "    mean_inference_ms: 1.194226591454462\n",
      "    mean_raw_obs_processing_ms: 1.8141414437201633\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.51356837488812\n",
      "    episode_reward_mean: 0.4995845049465108\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 6.079174691192176\n",
      "      - -0.008914742586872637\n",
      "      - 0.22553419402392905\n",
      "      - 39.51356837488812\n",
      "      - -0.14519700282973602\n",
      "      - -0.17574002689181967\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14304250532156898\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.9737115770308\n",
      "      mean_inference_ms: 1.194226591454462\n",
      "      mean_raw_obs_processing_ms: 1.8141414437201633\n",
      "  time_since_restore: 1029.1305193901062\n",
      "  time_this_iter_s: 5.03396463394165\n",
      "  time_total_s: 1029.1305193901062\n",
      "  timers:\n",
      "    learn_throughput: 165.7\n",
      "    learn_time_ms: 48.28\n",
      "    load_throughput: 40980.01\n",
      "    load_time_ms: 0.195\n",
      "    training_iteration_time_ms: 3431.745\n",
      "    update_time_ms: 1.973\n",
      "  timestamp: 1657050160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2080\n",
      "  training_iteration: 260\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2096\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2096\n",
      "    num_agent_steps_trained: 2096\n",
      "    num_env_steps_sampled: 2096\n",
      "    num_env_steps_trained: 2096\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: -0.6701406814278528\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 698\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3818799257278442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0330239092581905e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.22281529009342194\n",
      "          total_loss: 2.2231712341308594\n",
      "          vf_explained_var: 0.0008687317604199052\n",
      "          vf_loss: 2.0003559589385986\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2096\n",
      "    num_agent_steps_trained: 2096\n",
      "    num_env_steps_sampled: 2096\n",
      "    num_env_steps_trained: 2096\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2096\n",
      "  num_agent_steps_trained: 2096\n",
      "  num_env_steps_sampled: 2096\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2096\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.3\n",
      "    ram_util_percent: 42.3\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.142911420298338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.5595162137146\n",
      "    mean_inference_ms: 1.1930810256233373\n",
      "    mean_raw_obs_processing_ms: 1.79699705169666\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: -0.6701406814278528\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -5.069772450445164\n",
      "      - -0.5078072996813603\n",
      "      - -8.29849332255445\n",
      "      - -0.17936540185714733\n",
      "      - 8.124730288912218\n",
      "      - -32.41896980877002\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.142911420298338\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.5595162137146\n",
      "      mean_inference_ms: 1.1930810256233373\n",
      "      mean_raw_obs_processing_ms: 1.79699705169666\n",
      "  time_since_restore: 1036.1149263381958\n",
      "  time_this_iter_s: 3.7596397399902344\n",
      "  time_total_s: 1036.1149263381958\n",
      "  timers:\n",
      "    learn_throughput: 168.197\n",
      "    learn_time_ms: 47.563\n",
      "    load_throughput: 41677.347\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 3400.82\n",
      "    update_time_ms: 1.954\n",
      "  timestamp: 1657050167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2096\n",
      "  training_iteration: 262\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:47 (running for 00:17:42.42)<br>Memory usage on this node: 13.0/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         1036.11</td><td style=\"text-align: right;\">2096</td><td style=\"text-align: right;\">-0.670141</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2112\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2112\n",
      "    num_agent_steps_trained: 2112\n",
      "    num_env_steps_sampled: 2112\n",
      "    num_env_steps_trained: 2112\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-42-54\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: 0.08745504224046922\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 704\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3802071809768677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.6929147881892277e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5713880062103271\n",
      "          total_loss: 0.574289083480835\n",
      "          vf_explained_var: 0.11565154045820236\n",
      "          vf_loss: 0.002901121275499463\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2112\n",
      "    num_agent_steps_trained: 2112\n",
      "    num_env_steps_sampled: 2112\n",
      "    num_env_steps_trained: 2112\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2112\n",
      "  num_agent_steps_trained: 2112\n",
      "  num_env_steps_sampled: 2112\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2112\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.380000000000003\n",
      "    ram_util_percent: 42.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14290592480771835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.1934174445506\n",
      "    mean_inference_ms: 1.1925959453132682\n",
      "    mean_raw_obs_processing_ms: 1.8051681771273218\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: 0.08745504224046922\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.12605473249591626\n",
      "      - -28.787404059017433\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14290592480771835\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.1934174445506\n",
      "      mean_inference_ms: 1.1925959453132682\n",
      "      mean_raw_obs_processing_ms: 1.8051681771273218\n",
      "  time_since_restore: 1043.273999452591\n",
      "  time_this_iter_s: 3.4311182498931885\n",
      "  time_total_s: 1043.273999452591\n",
      "  timers:\n",
      "    learn_throughput: 168.555\n",
      "    learn_time_ms: 47.462\n",
      "    load_throughput: 44484.2\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3378.475\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1657050174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2112\n",
      "  training_iteration: 264\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:54 (running for 00:17:49.70)<br>Memory usage on this node: 13.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         1043.27</td><td style=\"text-align: right;\">2112</td><td style=\"text-align: right;\">0.087455</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:42:59 (running for 00:17:54.71)<br>Memory usage on this node: 13.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         1043.27</td><td style=\"text-align: right;\">2112</td><td style=\"text-align: right;\">0.087455</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8607</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2120\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2120\n",
      "    num_agent_steps_trained: 2120\n",
      "    num_env_steps_sampled: 2120\n",
      "    num_env_steps_trained: 2120\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-00\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: -0.00019217390157542978\n",
      "  episode_reward_min: -37.86066125426519\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 706\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.012003828426398133\n",
      "    episode_reward_mean: -0.012003828426398133\n",
      "    episode_reward_min: -0.012003828426398133\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.012003828426398133\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14234930276870728\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 881.1014667153358\n",
      "      mean_inference_ms: 1.4578580856323242\n",
      "      mean_raw_obs_processing_ms: 0.6653934717178345\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.380798101425171\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.328606213581224e-07\n",
      "          model: {}\n",
      "          policy_loss: -0.37483158707618713\n",
      "          total_loss: -0.2915101647377014\n",
      "          vf_explained_var: -0.05175723508000374\n",
      "          vf_loss: 0.08332139998674393\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2120\n",
      "    num_agent_steps_trained: 2120\n",
      "    num_env_steps_sampled: 2120\n",
      "    num_env_steps_trained: 2120\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2120\n",
      "  num_agent_steps_trained: 2120\n",
      "  num_env_steps_sampled: 2120\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2120\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.4\n",
      "    ram_util_percent: 42.42222222222222\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14282151837227577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 721.8399283883972\n",
      "    mean_inference_ms: 1.192000671185343\n",
      "    mean_raw_obs_processing_ms: 1.7912335074048606\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: -0.00019217390157542978\n",
      "    episode_reward_min: -37.86066125426519\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.00014375617835460375\n",
      "      - 0.028202264843351976\n",
      "      - 29.107577016874373\n",
      "      - 32.73125371094137\n",
      "      - -29.078267556166644\n",
      "      - -34.10105697243378\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14282151837227577\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 721.8399283883972\n",
      "      mean_inference_ms: 1.192000671185343\n",
      "      mean_raw_obs_processing_ms: 1.7912335074048606\n",
      "  time_since_restore: 1049.511827468872\n",
      "  time_this_iter_s: 6.237828016281128\n",
      "  time_total_s: 1049.511827468872\n",
      "  timers:\n",
      "    learn_throughput: 171.01\n",
      "    learn_time_ms: 46.781\n",
      "    load_throughput: 45516.05\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3408.876\n",
      "    update_time_ms: 2.004\n",
      "  timestamp: 1657050180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2120\n",
      "  training_iteration: 265\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2136\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2136\n",
      "    num_agent_steps_trained: 2136\n",
      "    num_env_steps_sampled: 2136\n",
      "    num_env_steps_trained: 2136\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-06\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: 0.012725272382887383\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 712\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.381203532218933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.924400860138121e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.3762883245944977\n",
      "          total_loss: -0.2756412625312805\n",
      "          vf_explained_var: -0.00022474925208371133\n",
      "          vf_loss: 0.10064705461263657\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2136\n",
      "    num_agent_steps_trained: 2136\n",
      "    num_env_steps_sampled: 2136\n",
      "    num_env_steps_trained: 2136\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2136\n",
      "  num_agent_steps_trained: 2136\n",
      "  num_env_steps_sampled: 2136\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2136\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.8\n",
      "    ram_util_percent: 42.5\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14281422068776656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.4804010829262\n",
      "    mean_inference_ms: 1.1915356448403742\n",
      "    mean_raw_obs_processing_ms: 1.7993640362736243\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: 0.012725272382887383\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.021385274995946446\n",
      "      - 28.98682745605833\n",
      "      - 0.00373806527346332\n",
      "      - 0.007545547198843372\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14281422068776656\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.4804010829262\n",
      "      mean_inference_ms: 1.1915356448403742\n",
      "      mean_raw_obs_processing_ms: 1.7993640362736243\n",
      "  time_since_restore: 1055.6503438949585\n",
      "  time_this_iter_s: 3.0249500274658203\n",
      "  time_total_s: 1055.6503438949585\n",
      "  timers:\n",
      "    learn_throughput: 168.815\n",
      "    learn_time_ms: 47.389\n",
      "    load_throughput: 46059.618\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3346.358\n",
      "    update_time_ms: 2.046\n",
      "  timestamp: 1657050186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2136\n",
      "  training_iteration: 267\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:07 (running for 00:18:02.21)<br>Memory usage on this node: 13.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         1055.65</td><td style=\"text-align: right;\">2136</td><td style=\"text-align: right;\">0.0127253</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2152\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2152\n",
      "    num_agent_steps_trained: 2152\n",
      "    num_env_steps_sampled: 2152\n",
      "    num_env_steps_trained: 2152\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: -0.2906886987926691\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 716\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.376981258392334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8002643855652423e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.42506811022758484\n",
      "          total_loss: 2.425208330154419\n",
      "          vf_explained_var: 0.00017761786875780672\n",
      "          vf_loss: 2.0001397132873535\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2152\n",
      "    num_agent_steps_trained: 2152\n",
      "    num_env_steps_sampled: 2152\n",
      "    num_env_steps_trained: 2152\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2152\n",
      "  num_agent_steps_trained: 2152\n",
      "  num_env_steps_sampled: 2152\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2152\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.4\n",
      "    ram_util_percent: 42.6\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.142768412768529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.6165801117604\n",
      "    mean_inference_ms: 1.1910455794704649\n",
      "    mean_raw_obs_processing_ms: 1.796504457156544\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: -0.2906886987926691\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -2.495552819391783\n",
      "      - 1.2614047510430821\n",
      "      - -3.8017791712209537\n",
      "      - 0.04453453428876042\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.142768412768529\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.6165801117604\n",
      "      mean_inference_ms: 1.1910455794704649\n",
      "      mean_raw_obs_processing_ms: 1.796504457156544\n",
      "  time_since_restore: 1062.672526359558\n",
      "  time_this_iter_s: 3.386366605758667\n",
      "  time_total_s: 1062.672526359558\n",
      "  timers:\n",
      "    learn_throughput: 171.054\n",
      "    learn_time_ms: 46.769\n",
      "    load_throughput: 47601.691\n",
      "    load_time_ms: 0.168\n",
      "    training_iteration_time_ms: 3325.228\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1657050194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2152\n",
      "  training_iteration: 269\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:14 (running for 00:18:09.28)<br>Memory usage on this node: 13.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         1062.67</td><td style=\"text-align: right;\">2152</td><td style=\"text-align: right;\">-0.290689</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:19 (running for 00:18:14.33)<br>Memory usage on this node: 13.1/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         1062.67</td><td style=\"text-align: right;\">2152</td><td style=\"text-align: right;\">-0.290689</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2160\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2160\n",
      "    num_agent_steps_trained: 2160\n",
      "    num_env_steps_sampled: 2160\n",
      "    num_env_steps_trained: 2160\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: 0.13627543617448992\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 720\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.07292342114468908\n",
      "    episode_reward_mean: -0.07292342114468908\n",
      "    episode_reward_min: -0.07292342114468908\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.07292342114468908\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1421501300086273\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 879.9705563878721\n",
      "      mean_inference_ms: 1.4549881402700227\n",
      "      mean_raw_obs_processing_ms: 0.6651878356933594\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3830273151397705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.524867133819498e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.20187953114509583\n",
      "          total_loss: 6.206518650054932\n",
      "          vf_explained_var: -0.0004166563448961824\n",
      "          vf_loss: 6.004640579223633\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2160\n",
      "    num_agent_steps_trained: 2160\n",
      "    num_env_steps_sampled: 2160\n",
      "    num_env_steps_trained: 2160\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2160\n",
      "  num_agent_steps_trained: 2160\n",
      "  num_env_steps_sampled: 2160\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2160\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.8125\n",
      "    ram_util_percent: 42.675000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14272217978019427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.768967155656\n",
      "    mean_inference_ms: 1.190579630076037\n",
      "    mean_raw_obs_processing_ms: 1.793656183757015\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: 0.13627543617448992\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -22.708426078086653\n",
      "      - 0.05425608460807485\n",
      "      - -0.06312847233989238\n",
      "      - 0.012313237383629438\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14272217978019427\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.768967155656\n",
      "      mean_inference_ms: 1.190579630076037\n",
      "      mean_raw_obs_processing_ms: 1.793656183757015\n",
      "  time_since_restore: 1068.3699679374695\n",
      "  time_this_iter_s: 5.697441577911377\n",
      "  time_total_s: 1068.3699679374695\n",
      "  timers:\n",
      "    learn_throughput: 173.221\n",
      "    learn_time_ms: 46.184\n",
      "    load_throughput: 49373.796\n",
      "    load_time_ms: 0.162\n",
      "    training_iteration_time_ms: 3408.283\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1657050199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2160\n",
      "  training_iteration: 270\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2176\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2176\n",
      "    num_agent_steps_trained: 2176\n",
      "    num_env_steps_sampled: 2176\n",
      "    num_env_steps_trained: 2176\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-27\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: 0.32242195229193543\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 724\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3757705688476562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.908882253104821e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5406596064567566\n",
      "          total_loss: 3.4600000381469727\n",
      "          vf_explained_var: 0.002335796831175685\n",
      "          vf_loss: 4.000658988952637\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2176\n",
      "    num_agent_steps_trained: 2176\n",
      "    num_env_steps_sampled: 2176\n",
      "    num_env_steps_trained: 2176\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2176\n",
      "  num_agent_steps_trained: 2176\n",
      "  num_env_steps_sampled: 2176\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2176\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.757142857142863\n",
      "    ram_util_percent: 42.699999999999996\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1426736510241767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.9003954221147\n",
      "    mean_inference_ms: 1.1901313946669896\n",
      "    mean_raw_obs_processing_ms: 1.7908071465253246\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: 0.32242195229193543\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 28.618099668842717\n",
      "      - -0.0023956819046713207\n",
      "      - -0.06286585207329054\n",
      "      - 0.04012844136742677\n",
      "      - -28.613179344569218\n",
      "      - 0.020561736528736496\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1426736510241767\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.9003954221147\n",
      "      mean_inference_ms: 1.1901313946669896\n",
      "      mean_raw_obs_processing_ms: 1.7908071465253246\n",
      "  time_since_restore: 1075.8384637832642\n",
      "  time_this_iter_s: 4.372059106826782\n",
      "  time_total_s: 1075.8384637832642\n",
      "  timers:\n",
      "    learn_throughput: 172.816\n",
      "    learn_time_ms: 46.292\n",
      "    load_throughput: 49308.497\n",
      "    load_time_ms: 0.162\n",
      "    training_iteration_time_ms: 3456.505\n",
      "    update_time_ms: 2.119\n",
      "  timestamp: 1657050207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2176\n",
      "  training_iteration: 272\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:27 (running for 00:18:22.60)<br>Memory usage on this node: 13.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         1075.84</td><td style=\"text-align: right;\">2176</td><td style=\"text-align: right;\">0.322422</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2192\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2192\n",
      "    num_agent_steps_trained: 2192\n",
      "    num_env_steps_sampled: 2192\n",
      "    num_env_steps_trained: 2192\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-35\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.99874232229823\n",
      "  episode_reward_mean: -0.014296681626003931\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 730\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3819829225540161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1749354345956817e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.2157408893108368\n",
      "          total_loss: 0.13618721067905426\n",
      "          vf_explained_var: -0.003928490448743105\n",
      "          vf_loss: 0.35192808508872986\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2192\n",
      "    num_agent_steps_trained: 2192\n",
      "    num_env_steps_sampled: 2192\n",
      "    num_env_steps_trained: 2192\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2192\n",
      "  num_agent_steps_trained: 2192\n",
      "  num_env_steps_sampled: 2192\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2192\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.183333333333334\n",
      "    ram_util_percent: 42.78333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14254300348381124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 722.7878444892181\n",
      "    mean_inference_ms: 1.1891471752629261\n",
      "    mean_raw_obs_processing_ms: 1.7746070205861266\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.99874232229823\n",
      "    episode_reward_mean: -0.014296681626003931\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 37.99874232229823\n",
      "      - -7.255440580924876\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14254300348381124\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 722.7878444892181\n",
      "      mean_inference_ms: 1.1891471752629261\n",
      "      mean_raw_obs_processing_ms: 1.7746070205861266\n",
      "  time_since_restore: 1083.4946103096008\n",
      "  time_this_iter_s: 4.240837335586548\n",
      "  time_total_s: 1083.4946103096008\n",
      "  timers:\n",
      "    learn_throughput: 172.379\n",
      "    learn_time_ms: 46.409\n",
      "    load_throughput: 49092.073\n",
      "    load_time_ms: 0.163\n",
      "    training_iteration_time_ms: 3506.138\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1657050215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2192\n",
      "  training_iteration: 274\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:35 (running for 00:18:30.32)<br>Memory usage on this node: 13.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         1083.49</td><td style=\"text-align: right;\">2192</td><td style=\"text-align: right;\">-0.0142967</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:40 (running for 00:18:35.33)<br>Memory usage on this node: 13.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         1083.49</td><td style=\"text-align: right;\">2192</td><td style=\"text-align: right;\">-0.0142967</td><td style=\"text-align: right;\">             37.9987</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2200\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2200\n",
      "    num_agent_steps_trained: 2200\n",
      "    num_env_steps_sampled: 2200\n",
      "    num_env_steps_trained: 2200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.73681409490054\n",
      "  episode_reward_mean: -0.3347514908791674\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 732\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.013629178893729277\n",
      "    episode_reward_mean: 0.013629178893729277\n",
      "    episode_reward_min: 0.013629178893729277\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.013629178893729277\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14204433165400862\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 879.8857798059303\n",
      "      mean_inference_ms: 1.4511806419096798\n",
      "      mean_raw_obs_processing_ms: 0.6636496049812042\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3762165307998657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7833313904702663e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.36204278469085693\n",
      "          total_loss: 0.8992084264755249\n",
      "          vf_explained_var: 0.01881062053143978\n",
      "          vf_loss: 0.5371657609939575\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2200\n",
      "    num_agent_steps_trained: 2200\n",
      "    num_env_steps_sampled: 2200\n",
      "    num_env_steps_trained: 2200\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2200\n",
      "  num_agent_steps_trained: 2200\n",
      "  num_env_steps_sampled: 2200\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2200\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.450000000000003\n",
      "    ram_util_percent: 42.875\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14258144627514174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.2707311620388\n",
      "    mean_inference_ms: 1.1892563030479517\n",
      "    mean_raw_obs_processing_ms: 1.7852435673666343\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.73681409490054\n",
      "    episode_reward_mean: -0.3347514908791674\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.08273826788415528\n",
      "      - 6.885514042524257\n",
      "      - 0.10184008988825477\n",
      "      - -37.57764111347744\n",
      "      - 28.810144671751903\n",
      "      - -0.15823147338245747\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14258144627514174\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.2707311620388\n",
      "      mean_inference_ms: 1.1892563030479517\n",
      "      mean_raw_obs_processing_ms: 1.7852435673666343\n",
      "  time_since_restore: 1089.435122013092\n",
      "  time_this_iter_s: 5.940511703491211\n",
      "  time_total_s: 1089.435122013092\n",
      "  timers:\n",
      "    learn_throughput: 173.426\n",
      "    learn_time_ms: 46.129\n",
      "    load_throughput: 48863.306\n",
      "    load_time_ms: 0.164\n",
      "    training_iteration_time_ms: 3477.151\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1657050221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2200\n",
      "  training_iteration: 275\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2216\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2216\n",
      "    num_agent_steps_trained: 2216\n",
      "    num_env_steps_sampled: 2216\n",
      "    num_env_steps_trained: 2216\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.73681409490054\n",
      "  episode_reward_mean: 0.3956237413765104\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 738\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3770451545715332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1125164292025147e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.23899626731872559\n",
      "          total_loss: 3.7642741203308105\n",
      "          vf_explained_var: 0.000299151724902913\n",
      "          vf_loss: 4.003270149230957\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2216\n",
      "    num_agent_steps_trained: 2216\n",
      "    num_env_steps_sampled: 2216\n",
      "    num_env_steps_trained: 2216\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2216\n",
      "  num_agent_steps_trained: 2216\n",
      "  num_env_steps_sampled: 2216\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2216\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.72\n",
      "    ram_util_percent: 43.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14244552738302577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.2205046706782\n",
      "    mean_inference_ms: 1.188315486319913\n",
      "    mean_raw_obs_processing_ms: 1.7692409607501203\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.73681409490054\n",
      "    episode_reward_mean: 0.3956237413765104\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0026838340580543907\n",
      "      - -0.005422393977639217\n",
      "      - -28.763955551008905\n",
      "      - -0.03306287812237274\n",
      "      - 0.05113404309604408\n",
      "      - 37.383873890391506\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14244552738302577\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.2205046706782\n",
      "      mean_inference_ms: 1.188315486319913\n",
      "      mean_raw_obs_processing_ms: 1.7692409607501203\n",
      "  time_since_restore: 1095.9495038986206\n",
      "  time_this_iter_s: 2.907963752746582\n",
      "  time_total_s: 1095.9495038986206\n",
      "  timers:\n",
      "    learn_throughput: 176.589\n",
      "    learn_time_ms: 45.303\n",
      "    load_throughput: 47716.769\n",
      "    load_time_ms: 0.168\n",
      "    training_iteration_time_ms: 3514.953\n",
      "    update_time_ms: 2.058\n",
      "  timestamp: 1657050227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2216\n",
      "  training_iteration: 277\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:47 (running for 00:18:42.90)<br>Memory usage on this node: 13.2/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         1095.95</td><td style=\"text-align: right;\">2216</td><td style=\"text-align: right;\">0.395624</td><td style=\"text-align: right;\">             37.7368</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2232\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2232\n",
      "    num_agent_steps_trained: 2232\n",
      "    num_env_steps_sampled: 2232\n",
      "    num_env_steps_trained: 2232\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.73681409490054\n",
      "  episode_reward_mean: -0.05386871723129239\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 744\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3766199350357056\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.840334546926897e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.16348201036453247\n",
      "          total_loss: 2.163583517074585\n",
      "          vf_explained_var: 0.001037164474837482\n",
      "          vf_loss: 2.0001022815704346\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2232\n",
      "    num_agent_steps_trained: 2232\n",
      "    num_env_steps_sampled: 2232\n",
      "    num_env_steps_trained: 2232\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2232\n",
      "  num_agent_steps_trained: 2232\n",
      "  num_env_steps_sampled: 2232\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2232\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.380000000000003\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14242713553767572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.9082311091151\n",
      "    mean_inference_ms: 1.187988638358737\n",
      "    mean_raw_obs_processing_ms: 1.7770260547635575\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.73681409490054\n",
      "    episode_reward_mean: -0.05386871723129239\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3521890869395212\n",
      "      - 1.3439216590946697\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14242713553767572\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.9082311091151\n",
      "      mean_inference_ms: 1.187988638358737\n",
      "      mean_raw_obs_processing_ms: 1.7770260547635575\n",
      "  time_since_restore: 1102.2050333023071\n",
      "  time_this_iter_s: 3.024130344390869\n",
      "  time_total_s: 1102.2050333023071\n",
      "  timers:\n",
      "    learn_throughput: 170.385\n",
      "    learn_time_ms: 46.952\n",
      "    load_throughput: 44602.462\n",
      "    load_time_ms: 0.179\n",
      "    training_iteration_time_ms: 3437.936\n",
      "    update_time_ms: 2.167\n",
      "  timestamp: 1657050234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2232\n",
      "  training_iteration: 279\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:54 (running for 00:18:49.31)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         1102.21</td><td style=\"text-align: right;\">2232</td><td style=\"text-align: right;\">-0.0538687</td><td style=\"text-align: right;\">             37.7368</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:43:59 (running for 00:18:54.31)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         1102.21</td><td style=\"text-align: right;\">2232</td><td style=\"text-align: right;\">-0.0538687</td><td style=\"text-align: right;\">             37.7368</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2240\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2240\n",
      "    num_agent_steps_trained: 2240\n",
      "    num_env_steps_sampled: 2240\n",
      "    num_env_steps_trained: 2240\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.73681409490054\n",
      "  episode_reward_mean: -0.10198389331884221\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 746\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.062244990486512775\n",
      "    episode_reward_mean: 0.062244990486512775\n",
      "    episode_reward_min: 0.062244990486512775\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.062244990486512775\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14182378554485253\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 881.3234602911233\n",
      "      mean_inference_ms: 1.4458238725831523\n",
      "      mean_raw_obs_processing_ms: 0.6632212351059773\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3704276084899902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0190052509860834e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.3500155806541443\n",
      "          total_loss: 3.649458646774292\n",
      "          vf_explained_var: 0.00832201074808836\n",
      "          vf_loss: 3.999473810195923\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2240\n",
      "    num_agent_steps_trained: 2240\n",
      "    num_env_steps_sampled: 2240\n",
      "    num_env_steps_trained: 2240\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2240\n",
      "  num_agent_steps_trained: 2240\n",
      "  num_env_steps_sampled: 2240\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2240\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.75\n",
      "    ram_util_percent: 39.925\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14234363024856614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 723.6130093807973\n",
      "    mean_inference_ms: 1.187481632186514\n",
      "    mean_raw_obs_processing_ms: 1.7639121200337313\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.73681409490054\n",
      "    episode_reward_mean: -0.10198389331884221\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.49852168660089546\n",
      "      - -37.86066125426519\n",
      "      - -0.04943740342331804\n",
      "      - 0.010392666414908547\n",
      "      - 0.008901328716478307\n",
      "      - 0.05919245681834573\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14234363024856614\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 723.6130093807973\n",
      "      mean_inference_ms: 1.187481632186514\n",
      "      mean_raw_obs_processing_ms: 1.7639121200337313\n",
      "  time_since_restore: 1108.206990480423\n",
      "  time_this_iter_s: 6.001957178115845\n",
      "  time_total_s: 1108.206990480423\n",
      "  timers:\n",
      "    learn_throughput: 168.353\n",
      "    learn_time_ms: 47.519\n",
      "    load_throughput: 44092.552\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3425.969\n",
      "    update_time_ms: 2.215\n",
      "  timestamp: 1657050240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2240\n",
      "  training_iteration: 280\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2256\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2256\n",
      "    num_agent_steps_trained: 2256\n",
      "    num_env_steps_sampled: 2256\n",
      "    num_env_steps_trained: 2256\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-06\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.3710326578241963\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 752\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3852437734603882\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4961492524889763e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.01960434764623642\n",
      "          total_loss: 6.468948841094971\n",
      "          vf_explained_var: -0.00014682610344607383\n",
      "          vf_loss: 6.488553047180176\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2256\n",
      "    num_agent_steps_trained: 2256\n",
      "    num_env_steps_sampled: 2256\n",
      "    num_env_steps_trained: 2256\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2256\n",
      "  num_agent_steps_trained: 2256\n",
      "  num_env_steps_sampled: 2256\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2256\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.65\n",
      "    ram_util_percent: 40.7\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14233936840705913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.2050626839793\n",
      "    mean_inference_ms: 1.1872175095091442\n",
      "    mean_raw_obs_processing_ms: 1.7717081682315656\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.3710326578241963\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.054534514266967316\n",
      "      - -0.05356267899678002\n",
      "      - -0.14582892509754242\n",
      "      - -0.0031710868669441172\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14233936840705913\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.2050626839793\n",
      "      mean_inference_ms: 1.1872175095091442\n",
      "      mean_raw_obs_processing_ms: 1.7717081682315656\n",
      "  time_since_restore: 1114.1339037418365\n",
      "  time_this_iter_s: 2.9590842723846436\n",
      "  time_total_s: 1114.1339037418365\n",
      "  timers:\n",
      "    learn_throughput: 165.146\n",
      "    learn_time_ms: 48.442\n",
      "    load_throughput: 44495.998\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3271.98\n",
      "    update_time_ms: 2.228\n",
      "  timestamp: 1657050246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2256\n",
      "  training_iteration: 282\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:06 (running for 00:19:01.39)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         1114.13</td><td style=\"text-align: right;\">2256</td><td style=\"text-align: right;\">0.371033</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2272\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2272\n",
      "    num_agent_steps_trained: 2272\n",
      "    num_env_steps_sampled: 2272\n",
      "    num_env_steps_trained: 2272\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-12\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.3283007955685096\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 756\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.383729338645935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2426387658924796e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.3128996789455414\n",
      "          total_loss: 8.31301498413086\n",
      "          vf_explained_var: -0.001788834691978991\n",
      "          vf_loss: 8.000114440917969\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2272\n",
      "    num_agent_steps_trained: 2272\n",
      "    num_env_steps_sampled: 2272\n",
      "    num_env_steps_trained: 2272\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2272\n",
      "  num_agent_steps_trained: 2272\n",
      "  num_env_steps_sampled: 2272\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2272\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.72\n",
      "    ram_util_percent: 40.7\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14229655671071564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.2875676576\n",
      "    mean_inference_ms: 1.1868673087382635\n",
      "    mean_raw_obs_processing_ms: 1.7691647818019363\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.3283007955685096\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 28.65091922938692\n",
      "      - 0.00429705930273605\n",
      "      - -28.643104322199694\n",
      "      - -1.3126224231995707\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14229655671071564\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.2875676576\n",
      "      mean_inference_ms: 1.1868673087382635\n",
      "      mean_raw_obs_processing_ms: 1.7691647818019363\n",
      "  time_since_restore: 1120.21488571167\n",
      "  time_this_iter_s: 3.131115674972534\n",
      "  time_total_s: 1120.21488571167\n",
      "  timers:\n",
      "    learn_throughput: 164.493\n",
      "    learn_time_ms: 48.634\n",
      "    load_throughput: 44828.9\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3114.286\n",
      "    update_time_ms: 2.157\n",
      "  timestamp: 1657050252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2272\n",
      "  training_iteration: 284\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:12 (running for 00:19:07.52)<br>Memory usage on this node: 12.5/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         1120.21</td><td style=\"text-align: right;\">2272</td><td style=\"text-align: right;\">0.328301</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:17 (running for 00:19:12.55)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         1120.21</td><td style=\"text-align: right;\">2272</td><td style=\"text-align: right;\">0.328301</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2280\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2280\n",
      "    num_agent_steps_trained: 2280\n",
      "    num_env_steps_sampled: 2280\n",
      "    num_env_steps_trained: 2280\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.3424970260005331\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 760\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 1.3358627072606484\n",
      "    episode_reward_mean: 1.3358627072606484\n",
      "    episode_reward_min: 1.3358627072606484\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3358627072606484\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14158459596855696\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 879.9412638642067\n",
      "      mean_inference_ms: 1.4420498249142668\n",
      "      mean_raw_obs_processing_ms: 0.6625139436056448\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3805564641952515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.3447561034117825e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.06897997111082077\n",
      "          total_loss: 3.192760705947876\n",
      "          vf_explained_var: 0.0020242074970155954\n",
      "          vf_loss: 3.2617406845092773\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2280\n",
      "    num_agent_steps_trained: 2280\n",
      "    num_env_steps_sampled: 2280\n",
      "    num_env_steps_trained: 2280\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2280\n",
      "  num_agent_steps_trained: 2280\n",
      "  num_env_steps_sampled: 2280\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2280\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.512500000000003\n",
      "    ram_util_percent: 40.7625\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14225189099871374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.3378017002913\n",
      "    mean_inference_ms: 1.1865050067140146\n",
      "    mean_raw_obs_processing_ms: 1.7666569863221313\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.3424970260005331\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.054061663780948344\n",
      "      - 0.05294654490911821\n",
      "      - 0.04846139648244929\n",
      "      - 0.0029060154485520684\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14225189099871374\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.3378017002913\n",
      "      mean_inference_ms: 1.1865050067140146\n",
      "      mean_raw_obs_processing_ms: 1.7666569863221313\n",
      "  time_since_restore: 1125.8254187107086\n",
      "  time_this_iter_s: 5.610532999038696\n",
      "  time_total_s: 1125.8254187107086\n",
      "  timers:\n",
      "    learn_throughput: 166.045\n",
      "    learn_time_ms: 48.18\n",
      "    load_throughput: 44876.865\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3103.283\n",
      "    update_time_ms: 2.253\n",
      "  timestamp: 1657050258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2280\n",
      "  training_iteration: 285\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2296\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2296\n",
      "    num_agent_steps_trained: 2296\n",
      "    num_env_steps_sampled: 2296\n",
      "    num_env_steps_trained: 2296\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-26\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.013424382324052091\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 764\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3737314939498901\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1418465874157846e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.17880387604236603\n",
      "          total_loss: 0.3822780251502991\n",
      "          vf_explained_var: -0.026986658573150635\n",
      "          vf_loss: 0.20347411930561066\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2296\n",
      "    num_agent_steps_trained: 2296\n",
      "    num_env_steps_sampled: 2296\n",
      "    num_env_steps_trained: 2296\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2296\n",
      "  num_agent_steps_trained: 2296\n",
      "  num_env_steps_sampled: 2296\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2296\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.966666666666665\n",
      "    ram_util_percent: 40.81666666666666\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1422051816084924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.3974036409097\n",
      "    mean_inference_ms: 1.1861550777366114\n",
      "    mean_raw_obs_processing_ms: 1.7641383752642579\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.013424382324052091\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.10694362011521541\n",
      "      - -0.0005454868452040573\n",
      "      - 0.061404859255712374\n",
      "      - -0.04667954408266228\n",
      "      - 0.014840196451646914\n",
      "      - -0.08061539223381509\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1422051816084924\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.3974036409097\n",
      "      mean_inference_ms: 1.1861550777366114\n",
      "      mean_raw_obs_processing_ms: 1.7641383752642579\n",
      "  time_since_restore: 1134.1538064479828\n",
      "  time_this_iter_s: 4.151815176010132\n",
      "  time_total_s: 1134.1538064479828\n",
      "  timers:\n",
      "    learn_throughput: 166.618\n",
      "    learn_time_ms: 48.014\n",
      "    load_throughput: 44098.347\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3284.663\n",
      "    update_time_ms: 2.248\n",
      "  timestamp: 1657050266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2296\n",
      "  training_iteration: 287\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:26 (running for 00:19:21.59)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         1134.15</td><td style=\"text-align: right;\">2296</td><td style=\"text-align: right;\">-0.0134244</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2312\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2312\n",
      "    num_agent_steps_trained: 2312\n",
      "    num_env_steps_sampled: 2312\n",
      "    num_env_steps_trained: 2312\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.3265053878209475\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 770\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3735864162445068\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8259551072551403e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.363252192735672\n",
      "          total_loss: 0.16787239909172058\n",
      "          vf_explained_var: 0.039276860654354095\n",
      "          vf_loss: 0.5311245322227478\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2312\n",
      "    num_agent_steps_trained: 2312\n",
      "    num_env_steps_sampled: 2312\n",
      "    num_env_steps_trained: 2312\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2312\n",
      "  num_agent_steps_trained: 2312\n",
      "  num_env_steps_sampled: 2312\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2312\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.633333333333336\n",
      "    ram_util_percent: 40.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14207522367863304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.0992358010336\n",
      "    mean_inference_ms: 1.1853265727979574\n",
      "    mean_raw_obs_processing_ms: 1.7490888895351093\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.3265053878209475\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.006502538945038716\n",
      "      - 0.0007386871484706958\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14207522367863304\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.0992358010336\n",
      "      mean_inference_ms: 1.1853265727979574\n",
      "      mean_raw_obs_processing_ms: 1.7490888895351093\n",
      "  time_since_restore: 1141.5715272426605\n",
      "  time_this_iter_s: 3.693352222442627\n",
      "  time_total_s: 1141.5715272426605\n",
      "  timers:\n",
      "    learn_throughput: 170.864\n",
      "    learn_time_ms: 46.821\n",
      "    load_throughput: 47001.586\n",
      "    load_time_ms: 0.17\n",
      "    training_iteration_time_ms: 3401.456\n",
      "    update_time_ms: 2.138\n",
      "  timestamp: 1657050273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2312\n",
      "  training_iteration: 289\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:33 (running for 00:19:29.07)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         1141.57</td><td style=\"text-align: right;\">2312</td><td style=\"text-align: right;\">0.326505</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:38 (running for 00:19:34.07)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         1141.57</td><td style=\"text-align: right;\">2312</td><td style=\"text-align: right;\">0.326505</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2320\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2320\n",
      "    num_agent_steps_trained: 2320\n",
      "    num_env_steps_sampled: 2320\n",
      "    num_env_steps_trained: 2320\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.012368118202705532\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 772\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.01993644116260107\n",
      "    episode_reward_mean: -0.01993644116260107\n",
      "    episode_reward_min: -0.01993644116260107\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.01993644116260107\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.141406740461077\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 883.3529540470669\n",
      "      mean_inference_ms: 1.44012451171875\n",
      "      mean_raw_obs_processing_ms: 0.6632872990199498\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3809864521026611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.1079300697456347e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.07000316679477692\n",
      "          total_loss: 0.07096835225820541\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009651790023781359\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2320\n",
      "    num_agent_steps_trained: 2320\n",
      "    num_env_steps_sampled: 2320\n",
      "    num_env_steps_trained: 2320\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2320\n",
      "  num_agent_steps_trained: 2320\n",
      "  num_env_steps_sampled: 2320\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2320\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.42222222222222\n",
      "    ram_util_percent: 40.97777777777778\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1421101031976354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.494849137935\n",
      "    mean_inference_ms: 1.185447566423487\n",
      "    mean_raw_obs_processing_ms: 1.7592385732521378\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.012368118202705532\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.06258420345400806\n",
      "      - 1.3194228264943093\n",
      "      - 28.75774794563998\n",
      "      - -1.3138704556885732\n",
      "      - 0.0791241835415164\n",
      "      - -6.463971219603806\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1421101031976354\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.494849137935\n",
      "      mean_inference_ms: 1.185447566423487\n",
      "      mean_raw_obs_processing_ms: 1.7592385732521378\n",
      "  time_since_restore: 1148.48046875\n",
      "  time_this_iter_s: 6.9089415073394775\n",
      "  time_total_s: 1148.48046875\n",
      "  timers:\n",
      "    learn_throughput: 170.345\n",
      "    learn_time_ms: 46.963\n",
      "    load_throughput: 46903.036\n",
      "    load_time_ms: 0.171\n",
      "    training_iteration_time_ms: 3456.758\n",
      "    update_time_ms: 2.096\n",
      "  timestamp: 1657050280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2320\n",
      "  training_iteration: 290\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:44 (running for 00:19:39.61)<br>Memory usage on this node: 12.6/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         1152.04</td><td style=\"text-align: right;\">2328</td><td style=\"text-align: right;\">-0.30124</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2336\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2336\n",
      "    num_agent_steps_trained: 2336\n",
      "    num_env_steps_sampled: 2336\n",
      "    num_env_steps_trained: 2336\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.2239637223613501\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 778\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3758131265640259\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.749909824866336e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.034036748111248016\n",
      "          total_loss: 0.5423896312713623\n",
      "          vf_explained_var: 0.027281025424599648\n",
      "          vf_loss: 0.5764263868331909\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2336\n",
      "    num_agent_steps_trained: 2336\n",
      "    num_env_steps_sampled: 2336\n",
      "    num_env_steps_trained: 2336\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2336\n",
      "  num_agent_steps_trained: 2336\n",
      "  num_env_steps_sampled: 2336\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2336\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.05\n",
      "    ram_util_percent: 41.1\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14198503293913212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.2468979290658\n",
      "    mean_inference_ms: 1.1846626108199045\n",
      "    mean_raw_obs_processing_ms: 1.7443869834300565\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.2239637223613501\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.056818057244790277\n",
      "      - -0.049842494228974576\n",
      "      - 6.337431615265189\n",
      "      - -0.014133916502131472\n",
      "      - -28.77752759958171\n",
      "      - 0.01080251346851524\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14198503293913212\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.2468979290658\n",
      "      mean_inference_ms: 1.1846626108199045\n",
      "      mean_raw_obs_processing_ms: 1.7443869834300565\n",
      "  time_since_restore: 1155.2737505435944\n",
      "  time_this_iter_s: 3.2354209423065186\n",
      "  time_total_s: 1155.2737505435944\n",
      "  timers:\n",
      "    learn_throughput: 173.269\n",
      "    learn_time_ms: 46.171\n",
      "    load_throughput: 42441.73\n",
      "    load_time_ms: 0.188\n",
      "    training_iteration_time_ms: 3543.316\n",
      "    update_time_ms: 1.964\n",
      "  timestamp: 1657050287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2336\n",
      "  training_iteration: 292\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:44:51 (running for 00:19:47.02)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         1159.34</td><td style=\"text-align: right;\">2344</td><td style=\"text-align: right;\">-0.211103</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2352\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2352\n",
      "    num_agent_steps_trained: 2352\n",
      "    num_env_steps_sampled: 2352\n",
      "    num_env_steps_trained: 2352\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-44-55\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.3244715863993328\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 784\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3791905641555786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.89565183367813e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.23393514752388\n",
      "          total_loss: 3.046372890472412\n",
      "          vf_explained_var: -0.0001299142895732075\n",
      "          vf_loss: 3.280308961868286\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2352\n",
      "    num_agent_steps_trained: 2352\n",
      "    num_env_steps_sampled: 2352\n",
      "    num_env_steps_trained: 2352\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2352\n",
      "  num_agent_steps_trained: 2352\n",
      "  num_env_steps_sampled: 2352\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2352\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.95\n",
      "    ram_util_percent: 41.18333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1419725069265676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.7413622830122\n",
      "    mean_inference_ms: 1.1844864366521262\n",
      "    mean_raw_obs_processing_ms: 1.7521342516812712\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.3244715863993328\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.06332308739195858\n",
      "      - 0.043275399082342236\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1419725069265676\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.7413622830122\n",
      "      mean_inference_ms: 1.1844864366521262\n",
      "      mean_raw_obs_processing_ms: 1.7521342516812712\n",
      "  time_since_restore: 1163.37193441391\n",
      "  time_this_iter_s: 4.029189348220825\n",
      "  time_total_s: 1163.37193441391\n",
      "  timers:\n",
      "    learn_throughput: 176.136\n",
      "    learn_time_ms: 45.419\n",
      "    load_throughput: 43279.288\n",
      "    load_time_ms: 0.185\n",
      "    training_iteration_time_ms: 3745.135\n",
      "    update_time_ms: 2.035\n",
      "  timestamp: 1657050295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2352\n",
      "  training_iteration: 294\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:00 (running for 00:19:56.06)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         1163.37</td><td style=\"text-align: right;\">2352</td><td style=\"text-align: right;\">0.324472</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2360\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2360\n",
      "    num_agent_steps_trained: 2360\n",
      "    num_env_steps_sampled: 2360\n",
      "    num_env_steps_trained: 2360\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-02\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.0006075006053513121\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 786\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -1.3231911901968751\n",
      "    episode_reward_mean: -1.3231911901968751\n",
      "    episode_reward_min: -1.3231911901968751\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3231911901968751\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14105673586384634\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 883.1882503595245\n",
      "      mean_inference_ms: 1.4373733756247533\n",
      "      mean_raw_obs_processing_ms: 0.6632711110490093\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3703354597091675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.892847068229457e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.337157279253006\n",
      "          total_loss: 5.721447467803955\n",
      "          vf_explained_var: -0.00028949975967407227\n",
      "          vf_loss: 5.384288311004639\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2360\n",
      "    num_agent_steps_trained: 2360\n",
      "    num_env_steps_sampled: 2360\n",
      "    num_env_steps_trained: 2360\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2360\n",
      "  num_agent_steps_trained: 2360\n",
      "  num_env_steps_sampled: 2360\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2360\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.15555555555555\n",
      "    ram_util_percent: 41.22222222222223\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14189175223703002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.4577854320581\n",
      "    mean_inference_ms: 1.1840290862156961\n",
      "    mean_raw_obs_processing_ms: 1.7398302884486094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.0006075006053513121\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3817358231484118\n",
      "      - -0.06274999001054704\n",
      "      - 28.749809534674355\n",
      "      - 37.73681409490054\n",
      "      - 4.960499267033214\n",
      "      - 0.07924874817575045\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14189175223703002\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.4577854320581\n",
      "      mean_inference_ms: 1.1840290862156961\n",
      "      mean_raw_obs_processing_ms: 1.7398302884486094\n",
      "  time_since_restore: 1169.653624534607\n",
      "  time_this_iter_s: 6.2816901206970215\n",
      "  time_total_s: 1169.653624534607\n",
      "  timers:\n",
      "    learn_throughput: 173.451\n",
      "    learn_time_ms: 46.122\n",
      "    load_throughput: 42690.117\n",
      "    load_time_ms: 0.187\n",
      "    training_iteration_time_ms: 3790.69\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1657050302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2360\n",
      "  training_iteration: 295\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2376\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2376\n",
      "    num_agent_steps_trained: 2376\n",
      "    num_env_steps_sampled: 2376\n",
      "    num_env_steps_trained: 2376\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.39091538554616023\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 792\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3811366558074951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.780022315273527e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.30954688787460327\n",
      "          total_loss: -0.2747458517551422\n",
      "          vf_explained_var: 0.08680801093578339\n",
      "          vf_loss: 0.03480105102062225\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2376\n",
      "    num_agent_steps_trained: 2376\n",
      "    num_env_steps_sampled: 2376\n",
      "    num_env_steps_trained: 2376\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2376\n",
      "  num_agent_steps_trained: 2376\n",
      "  num_env_steps_sampled: 2376\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2376\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.933333333333326\n",
      "    ram_util_percent: 41.333333333333336\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1418826295167804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 724.9336007995103\n",
      "    mean_inference_ms: 1.183858306331283\n",
      "    mean_raw_obs_processing_ms: 1.747538053578645\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.39091538554616023\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -33.73646410334929\n",
      "      - 0.05099228669775213\n",
      "      - -11.904979788584921\n",
      "      - 3.8219621740223175\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1418826295167804\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 724.9336007995103\n",
      "      mean_inference_ms: 1.183858306331283\n",
      "      mean_raw_obs_processing_ms: 1.747538053578645\n",
      "  time_since_restore: 1177.0892050266266\n",
      "  time_this_iter_s: 3.9627010822296143\n",
      "  time_total_s: 1177.0892050266266\n",
      "  timers:\n",
      "    learn_throughput: 172.338\n",
      "    learn_time_ms: 46.42\n",
      "    load_throughput: 43312.808\n",
      "    load_time_ms: 0.185\n",
      "    training_iteration_time_ms: 3701.46\n",
      "    update_time_ms: 1.975\n",
      "  timestamp: 1657050309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2376\n",
      "  training_iteration: 297\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:09 (running for 00:20:04.89)<br>Memory usage on this node: 12.7/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         1177.09</td><td style=\"text-align: right;\">2376</td><td style=\"text-align: right;\">-0.390915</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2392\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2392\n",
      "    num_agent_steps_trained: 2392\n",
      "    num_env_steps_sampled: 2392\n",
      "    num_env_steps_trained: 2392\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.020230075849545944\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 796\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3651820421218872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.259370325598866e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.24429260194301605\n",
      "          total_loss: 2.85448956489563\n",
      "          vf_explained_var: -0.0021964549086987972\n",
      "          vf_loss: 2.6101975440979004\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2392\n",
      "    num_agent_steps_trained: 2392\n",
      "    num_env_steps_sampled: 2392\n",
      "    num_env_steps_trained: 2392\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2392\n",
      "  num_agent_steps_trained: 2392\n",
      "  num_env_steps_sampled: 2392\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2392\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.05\n",
      "    ram_util_percent: 41.4\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1418373475107447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.0683346814311\n",
      "    mean_inference_ms: 1.1835402742206365\n",
      "    mean_raw_obs_processing_ms: 1.7452713080303568\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.020230075849545944\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.05211507764389145\n",
      "      - -29.663488640782553\n",
      "      - 0.02461842397798497\n",
      "      - -0.0844884058348534\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1418373475107447\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.0683346814311\n",
      "      mean_inference_ms: 1.1835402742206365\n",
      "      mean_raw_obs_processing_ms: 1.7452713080303568\n",
      "  time_since_restore: 1183.821673631668\n",
      "  time_this_iter_s: 3.204864501953125\n",
      "  time_total_s: 1183.821673631668\n",
      "  timers:\n",
      "    learn_throughput: 172.104\n",
      "    learn_time_ms: 46.483\n",
      "    load_throughput: 44052.031\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3632.844\n",
      "    update_time_ms: 1.985\n",
      "  timestamp: 1657050316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2392\n",
      "  training_iteration: 299\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:16 (running for 00:20:11.68)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1183.82</td><td style=\"text-align: right;\">2392</td><td style=\"text-align: right;\">0.0202301</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:21 (running for 00:20:16.69)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1183.82</td><td style=\"text-align: right;\">2392</td><td style=\"text-align: right;\">0.0202301</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2400\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2400\n",
      "    num_agent_steps_trained: 2400\n",
      "    num_env_steps_sampled: 2400\n",
      "    num_env_steps_trained: 2400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.012464612674121595\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 800\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.05067898072733712\n",
      "    episode_reward_mean: 0.05067898072733712\n",
      "    episode_reward_min: 0.05067898072733712\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.05067898072733712\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14083161538477101\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 878.5508956698424\n",
      "      mean_inference_ms: 1.4339270512702056\n",
      "      mean_raw_obs_processing_ms: 0.6627364711866853\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3801264762878418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.847638021805324e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.49347248673439026\n",
      "          total_loss: 0.49667853116989136\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0032060437370091677\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2400\n",
      "    num_agent_steps_trained: 2400\n",
      "    num_env_steps_sampled: 2400\n",
      "    num_env_steps_trained: 2400\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2400\n",
      "  num_agent_steps_trained: 2400\n",
      "  num_env_steps_sampled: 2400\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2400\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.78888888888889\n",
      "    ram_util_percent: 41.46666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14179066552175057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.2072343955485\n",
      "    mean_inference_ms: 1.183231106869619\n",
      "    mean_raw_obs_processing_ms: 1.7430657908143619\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.012464612674121595\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.00496458569691427\n",
      "      - -0.025789918258625644\n",
      "      - 0.06935723452759235\n",
      "      - 37.42123245232729\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14179066552175057\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.2072343955485\n",
      "      mean_inference_ms: 1.183231106869619\n",
      "      mean_raw_obs_processing_ms: 1.7430657908143619\n",
      "  time_since_restore: 1189.8651280403137\n",
      "  time_this_iter_s: 6.04345440864563\n",
      "  time_total_s: 1189.8651280403137\n",
      "  timers:\n",
      "    learn_throughput: 173.786\n",
      "    learn_time_ms: 46.034\n",
      "    load_throughput: 43942.42\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3689.009\n",
      "    update_time_ms: 2.021\n",
      "  timestamp: 1657050322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2400\n",
      "  training_iteration: 300\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2416\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2416\n",
      "    num_agent_steps_trained: 2416\n",
      "    num_env_steps_sampled: 2416\n",
      "    num_env_steps_trained: 2416\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-29\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.002396419870809554\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 804\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3712666034698486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2029164483683417e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.20191030204296112\n",
      "          total_loss: 4.205993175506592\n",
      "          vf_explained_var: 0.004761735443025827\n",
      "          vf_loss: 4.004083156585693\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2416\n",
      "    num_agent_steps_trained: 2416\n",
      "    num_env_steps_sampled: 2416\n",
      "    num_env_steps_trained: 2416\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2416\n",
      "  num_agent_steps_trained: 2416\n",
      "  num_env_steps_sampled: 2416\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2416\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.8\n",
      "    ram_util_percent: 41.58\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1417453233893557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.3244528026243\n",
      "    mean_inference_ms: 1.1829317055194606\n",
      "    mean_raw_obs_processing_ms: 1.7408979184216398\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.002396419870809554\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.005334674582588406\n",
      "      - -37.4207362661434\n",
      "      - 0.018861487763879747\n",
      "      - 30.32895827590915\n",
      "      - -0.05947774435164721\n",
      "      - 0.02388638920413433\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1417453233893557\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.3244528026243\n",
      "      mean_inference_ms: 1.1829317055194606\n",
      "      mean_raw_obs_processing_ms: 1.7408979184216398\n",
      "  time_since_restore: 1196.5550932884216\n",
      "  time_this_iter_s: 3.4966750144958496\n",
      "  time_total_s: 1196.5550932884216\n",
      "  timers:\n",
      "    learn_throughput: 174.39\n",
      "    learn_time_ms: 45.874\n",
      "    load_throughput: 49395.601\n",
      "    load_time_ms: 0.162\n",
      "    training_iteration_time_ms: 3678.511\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1657050329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2416\n",
      "  training_iteration: 302\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:29 (running for 00:20:24.54)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         1196.56</td><td style=\"text-align: right;\">2416</td><td style=\"text-align: right;\">0.00239642</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2432\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2432\n",
      "    num_agent_steps_trained: 2432\n",
      "    num_env_steps_sampled: 2432\n",
      "    num_env_steps_trained: 2432\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.0743738803464005\n",
      "  episode_reward_min: -37.87087980744289\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 810\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3760864734649658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.272712423400662e-07\n",
      "          model: {}\n",
      "          policy_loss: -0.3500292897224426\n",
      "          total_loss: -0.33961138129234314\n",
      "          vf_explained_var: -0.42713743448257446\n",
      "          vf_loss: 0.010417887941002846\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2432\n",
      "    num_agent_steps_trained: 2432\n",
      "    num_env_steps_sampled: 2432\n",
      "    num_env_steps_trained: 2432\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2432\n",
      "  num_agent_steps_trained: 2432\n",
      "  num_env_steps_sampled: 2432\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2432\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.333333333333332\n",
      "    ram_util_percent: 41.65\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1416301492020754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.1206400031514\n",
      "    mean_inference_ms: 1.182214499352868\n",
      "    mean_raw_obs_processing_ms: 1.7269226239497248\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.0743738803464005\n",
      "    episode_reward_min: -37.87087980744289\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 7.537960735243978\n",
      "      - -37.87087980744289\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1416301492020754\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.1206400031514\n",
      "      mean_inference_ms: 1.182214499352868\n",
      "      mean_raw_obs_processing_ms: 1.7269226239497248\n",
      "  time_since_restore: 1203.8139367103577\n",
      "  time_this_iter_s: 4.098573446273804\n",
      "  time_total_s: 1203.8139367103577\n",
      "  timers:\n",
      "    learn_throughput: 173.974\n",
      "    learn_time_ms: 45.984\n",
      "    load_throughput: 45166.822\n",
      "    load_time_ms: 0.177\n",
      "    training_iteration_time_ms: 3594.67\n",
      "    update_time_ms: 1.996\n",
      "  timestamp: 1657050336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2432\n",
      "  training_iteration: 304\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:36 (running for 00:20:31.91)<br>Memory usage on this node: 12.8/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         1203.81</td><td style=\"text-align: right;\">2432</td><td style=\"text-align: right;\">0.0743739</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:41 (running for 00:20:36.92)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         1203.81</td><td style=\"text-align: right;\">2432</td><td style=\"text-align: right;\">0.0743739</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -37.8709</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2440\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2440\n",
      "    num_agent_steps_trained: 2440\n",
      "    num_env_steps_sampled: 2440\n",
      "    num_env_steps_trained: 2440\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-42\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.3733474764040077\n",
      "  episode_reward_min: -37.54095571297353\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 812\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.004272588470566552\n",
      "    episode_reward_mean: -0.004272588470566552\n",
      "    episode_reward_min: -0.004272588470566552\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.004272588470566552\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14075507288393768\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 876.9998045071312\n",
      "      mean_inference_ms: 1.4294670975726584\n",
      "      mean_raw_obs_processing_ms: 0.6628217904464059\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3800321817398071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0120261322299484e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.5797659158706665\n",
      "          total_loss: 1.4219509363174438\n",
      "          vf_explained_var: 0.009756859391927719\n",
      "          vf_loss: 2.0017170906066895\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2440\n",
      "    num_agent_steps_trained: 2440\n",
      "    num_env_steps_sampled: 2440\n",
      "    num_env_steps_trained: 2440\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2440\n",
      "  num_agent_steps_trained: 2440\n",
      "  num_env_steps_sampled: 2440\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2440\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.3875\n",
      "    ram_util_percent: 41.712500000000006\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14166556717723347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.5468620793254\n",
      "    mean_inference_ms: 1.1823586106706119\n",
      "    mean_raw_obs_processing_ms: 1.7366326842668003\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.3733474764040077\n",
      "    episode_reward_min: -37.54095571297353\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.048070981563091975\n",
      "      - 0.018164556866107073\n",
      "      - -1.3043160379002607\n",
      "      - -0.030448861423717943\n",
      "      - 32.26184608943509\n",
      "      - -32.25496289114023\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14166556717723347\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.5468620793254\n",
      "      mean_inference_ms: 1.1823586106706119\n",
      "      mean_raw_obs_processing_ms: 1.7366326842668003\n",
      "  time_since_restore: 1209.354828596115\n",
      "  time_this_iter_s: 5.540891885757446\n",
      "  time_total_s: 1209.354828596115\n",
      "  timers:\n",
      "    learn_throughput: 173.629\n",
      "    learn_time_ms: 46.075\n",
      "    load_throughput: 45051.6\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3547.697\n",
      "    update_time_ms: 1.947\n",
      "  timestamp: 1657050342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2440\n",
      "  training_iteration: 305\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2456\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2456\n",
      "    num_agent_steps_trained: 2456\n",
      "    num_env_steps_sampled: 2456\n",
      "    num_env_steps_trained: 2456\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.682237572812451\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 818\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3844494819641113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.546743027982302e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.45288288593292236\n",
      "          total_loss: 7.593577861785889\n",
      "          vf_explained_var: -0.001686994219198823\n",
      "          vf_loss: 8.046462059020996\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2456\n",
      "    num_agent_steps_trained: 2456\n",
      "    num_env_steps_sampled: 2456\n",
      "    num_env_steps_trained: 2456\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2456\n",
      "  num_agent_steps_trained: 2456\n",
      "  num_env_steps_sampled: 2456\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2456\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.025\n",
      "    ram_util_percent: 41.8\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14155048844633206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.2741537032178\n",
      "    mean_inference_ms: 1.1816677518606145\n",
      "    mean_raw_obs_processing_ms: 1.7228895710719159\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.682237572812451\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 30.153964706934183\n",
      "      - 7.544172886205963\n",
      "      - 0.036202880801405546\n",
      "      - -19.449558238659783\n",
      "      - 1.2621486434502955\n",
      "      - 14.060873097717788\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14155048844633206\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.2741537032178\n",
      "      mean_inference_ms: 1.1816677518606145\n",
      "      mean_raw_obs_processing_ms: 1.7228895710719159\n",
      "  time_since_restore: 1214.7179493904114\n",
      "  time_this_iter_s: 2.654165744781494\n",
      "  time_total_s: 1214.7179493904114\n",
      "  timers:\n",
      "    learn_throughput: 175.114\n",
      "    learn_time_ms: 45.685\n",
      "    load_throughput: 46009.094\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3340.137\n",
      "    update_time_ms: 2.011\n",
      "  timestamp: 1657050347\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2456\n",
      "  training_iteration: 307\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:47 (running for 00:20:42.94)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         1214.72</td><td style=\"text-align: right;\">2456</td><td style=\"text-align: right;\">0.682238</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2472\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2472\n",
      "    num_agent_steps_trained: 2472\n",
      "    num_env_steps_sampled: 2472\n",
      "    num_env_steps_trained: 2472\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-53\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.027275562305967557\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 824\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3685966730117798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.553452400723472e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.13002605736255646\n",
      "          total_loss: 5.870068073272705\n",
      "          vf_explained_var: -0.0023862600792199373\n",
      "          vf_loss: 6.000093460083008\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2472\n",
      "    num_agent_steps_trained: 2472\n",
      "    num_env_steps_sampled: 2472\n",
      "    num_env_steps_trained: 2472\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2472\n",
      "  num_agent_steps_trained: 2472\n",
      "  num_env_steps_sampled: 2472\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2472\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.625\n",
      "    ram_util_percent: 41.875\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14154358599008943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.710200711292\n",
      "    mean_inference_ms: 1.1814949631499463\n",
      "    mean_raw_obs_processing_ms: 1.7304261381081016\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.027275562305967557\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.007187821662879124\n",
      "      - -1.3127700529911306\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14154358599008943\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.710200711292\n",
      "      mean_inference_ms: 1.1814949631499463\n",
      "      mean_raw_obs_processing_ms: 1.7304261381081016\n",
      "  time_since_restore: 1220.5478591918945\n",
      "  time_this_iter_s: 3.205458641052246\n",
      "  time_total_s: 1220.5478591918945\n",
      "  timers:\n",
      "    learn_throughput: 173.608\n",
      "    learn_time_ms: 46.081\n",
      "    load_throughput: 44168.003\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3249.758\n",
      "    update_time_ms: 1.992\n",
      "  timestamp: 1657050353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2472\n",
      "  training_iteration: 309\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:53 (running for 00:20:48.81)<br>Memory usage on this node: 12.9/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         1220.55</td><td style=\"text-align: right;\">2472</td><td style=\"text-align: right;\">-0.0272756</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:45:58 (running for 00:20:53.84)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         1220.55</td><td style=\"text-align: right;\">2472</td><td style=\"text-align: right;\">-0.0272756</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2480\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2480\n",
      "    num_agent_steps_trained: 2480\n",
      "    num_env_steps_sampled: 2480\n",
      "    num_env_steps_trained: 2480\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-45-59\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.013744609916330985\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 826\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.0233698022732709\n",
      "    episode_reward_mean: 0.0233698022732709\n",
      "    episode_reward_min: 0.0233698022732709\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0233698022732709\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14053181530957554\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 876.864851477312\n",
      "      mean_inference_ms: 1.4272832615490265\n",
      "      mean_raw_obs_processing_ms: 0.6628801478421624\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3799097537994385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.843346793379169e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.19248394668102264\n",
      "          total_loss: 3.6754887104034424\n",
      "          vf_explained_var: -0.024429507553577423\n",
      "          vf_loss: 3.483004570007324\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2480\n",
      "    num_agent_steps_trained: 2480\n",
      "    num_env_steps_sampled: 2480\n",
      "    num_env_steps_trained: 2480\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2480\n",
      "  num_agent_steps_trained: 2480\n",
      "  num_env_steps_sampled: 2480\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2480\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.166666666666668\n",
      "    ram_util_percent: 40.68888888888889\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14147230134794145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.318102018517\n",
      "    mean_inference_ms: 1.1811004330810932\n",
      "    mean_raw_obs_processing_ms: 1.718897729337715\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.013744609916330985\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -33.61930889997982\n",
      "      - -0.04946354677279663\n",
      "      - -0.006982031326388083\n",
      "      - 1.3098222858050246\n",
      "      - 0.0013332976312081524\n",
      "      - -1.3035124815741825\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14147230134794145\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.318102018517\n",
      "      mean_inference_ms: 1.1811004330810932\n",
      "      mean_raw_obs_processing_ms: 1.718897729337715\n",
      "  time_since_restore: 1226.532657623291\n",
      "  time_this_iter_s: 5.984798431396484\n",
      "  time_total_s: 1226.532657623291\n",
      "  timers:\n",
      "    learn_throughput: 173.72\n",
      "    learn_time_ms: 46.051\n",
      "    load_throughput: 42979.931\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3164.306\n",
      "    update_time_ms: 1.96\n",
      "  timestamp: 1657050359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2480\n",
      "  training_iteration: 310\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2496\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2496\n",
      "    num_agent_steps_trained: 2496\n",
      "    num_env_steps_sampled: 2496\n",
      "    num_env_steps_trained: 2496\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.6818851231656166\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 832\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3771986961364746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.55454472405836e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.15876370668411255\n",
      "          total_loss: 1.8475741147994995\n",
      "          vf_explained_var: 0.0010291497455909848\n",
      "          vf_loss: 2.006337881088257\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2496\n",
      "    num_agent_steps_trained: 2496\n",
      "    num_env_steps_sampled: 2496\n",
      "    num_env_steps_trained: 2496\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2496\n",
      "  num_agent_steps_trained: 2496\n",
      "  num_env_steps_sampled: 2496\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2496\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.099999999999998\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14147320354342405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.6678671984206\n",
      "    mean_inference_ms: 1.180957372947501\n",
      "    mean_raw_obs_processing_ms: 1.7263698029048953\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.6818851231656166\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.03916679680339896\n",
      "      - 32.410272894161395\n",
      "      - 1.3169565771559077\n",
      "      - 0.07140156983939594\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14147320354342405\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.6678671984206\n",
      "      mean_inference_ms: 1.180957372947501\n",
      "      mean_raw_obs_processing_ms: 1.7263698029048953\n",
      "  time_since_restore: 1232.2369265556335\n",
      "  time_this_iter_s: 3.185915470123291\n",
      "  time_total_s: 1232.2369265556335\n",
      "  timers:\n",
      "    learn_throughput: 171.951\n",
      "    learn_time_ms: 46.525\n",
      "    load_throughput: 43151.276\n",
      "    load_time_ms: 0.185\n",
      "    training_iteration_time_ms: 3065.936\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1657050365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2496\n",
      "  training_iteration: 312\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:05 (running for 00:21:00.65)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         1232.24</td><td style=\"text-align: right;\">2496</td><td style=\"text-align: right;\">0.681885</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2512\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2512\n",
      "    num_agent_steps_trained: 2512\n",
      "    num_env_steps_sampled: 2512\n",
      "    num_env_steps_trained: 2512\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-12\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.33823282601868526\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 836\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.373751163482666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.275049574673176e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.3118062913417816\n",
      "          total_loss: -0.31077632308006287\n",
      "          vf_explained_var: -0.05623684450984001\n",
      "          vf_loss: 0.0010299355490133166\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2512\n",
      "    num_agent_steps_trained: 2512\n",
      "    num_env_steps_sampled: 2512\n",
      "    num_env_steps_trained: 2512\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2512\n",
      "  num_agent_steps_trained: 2512\n",
      "  num_env_steps_sampled: 2512\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2512\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.37142857142857\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14144407634232214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.6175418216491\n",
      "    mean_inference_ms: 1.1807230877484176\n",
      "    mean_raw_obs_processing_ms: 1.7243911212862049\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.33823282601868526\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.37937526730004123\n",
      "      - 37.55798860432809\n",
      "      - -32.02259439898468\n",
      "      - -37.54095571297353\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14144407634232214\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.6175418216491\n",
      "      mean_inference_ms: 1.1807230877484176\n",
      "      mean_raw_obs_processing_ms: 1.7243911212862049\n",
      "  time_since_restore: 1239.5385794639587\n",
      "  time_this_iter_s: 4.378368854522705\n",
      "  time_total_s: 1239.5385794639587\n",
      "  timers:\n",
      "    learn_throughput: 168.985\n",
      "    learn_time_ms: 47.342\n",
      "    load_throughput: 46571.037\n",
      "    load_time_ms: 0.172\n",
      "    training_iteration_time_ms: 3070.217\n",
      "    update_time_ms: 2.136\n",
      "  timestamp: 1657050372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2512\n",
      "  training_iteration: 314\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:12 (running for 00:21:08.09)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         1239.54</td><td style=\"text-align: right;\">2512</td><td style=\"text-align: right;\">-0.338233</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:17 (running for 00:21:13.09)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         1239.54</td><td style=\"text-align: right;\">2512</td><td style=\"text-align: right;\">-0.338233</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2520\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2520\n",
      "    num_agent_steps_trained: 2520\n",
      "    num_env_steps_sampled: 2520\n",
      "    num_env_steps_trained: 2520\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.0010932979409148124\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 840\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 36.45620516916407\n",
      "    episode_reward_mean: 36.45620516916407\n",
      "    episode_reward_min: 36.45620516916407\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 36.45620516916407\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14059920060007197\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 873.3567626852739\n",
      "      mean_inference_ms: 1.4289743021914834\n",
      "      mean_raw_obs_processing_ms: 0.6634950637817383\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3768669366836548\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1441005881351884e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.26785358786582947\n",
      "          total_loss: 0.0812302753329277\n",
      "          vf_explained_var: 0.03408237546682358\n",
      "          vf_loss: 0.349083811044693\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2520\n",
      "    num_agent_steps_trained: 2520\n",
      "    num_env_steps_sampled: 2520\n",
      "    num_env_steps_trained: 2520\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2520\n",
      "  num_agent_steps_trained: 2520\n",
      "  num_env_steps_sampled: 2520\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2520\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.6375\n",
      "    ram_util_percent: 39.925\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14141760673024187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.6151458199613\n",
      "    mean_inference_ms: 1.1805412525535235\n",
      "    mean_raw_obs_processing_ms: 1.7224422990090558\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.0010932979409148124\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03626554130160409\n",
      "      - 33.300371485033175\n",
      "      - -0.01797221048755704\n",
      "      - -0.0019462057455257309\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14141760673024187\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.6151458199613\n",
      "      mean_inference_ms: 1.1805412525535235\n",
      "      mean_raw_obs_processing_ms: 1.7224422990090558\n",
      "  time_since_restore: 1245.1961975097656\n",
      "  time_this_iter_s: 5.657618045806885\n",
      "  time_total_s: 1245.1961975097656\n",
      "  timers:\n",
      "    learn_throughput: 169.29\n",
      "    learn_time_ms: 47.256\n",
      "    load_throughput: 47716.769\n",
      "    load_time_ms: 0.168\n",
      "    training_iteration_time_ms: 3120.362\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1657050378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2520\n",
      "  training_iteration: 315\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2536\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2536\n",
      "    num_agent_steps_trained: 2536\n",
      "    num_env_steps_sampled: 2536\n",
      "    num_env_steps_trained: 2536\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-25\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: -0.12715030818424067\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 844\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.383437991142273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8065427866531536e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.45317745208740234\n",
      "          total_loss: 4.799981594085693\n",
      "          vf_explained_var: 0.00371731910854578\n",
      "          vf_loss: 4.346804141998291\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2536\n",
      "    num_agent_steps_trained: 2536\n",
      "    num_env_steps_sampled: 2536\n",
      "    num_env_steps_trained: 2536\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2536\n",
      "  num_agent_steps_trained: 2536\n",
      "  num_env_steps_sampled: 2536\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2536\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.339999999999996\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14138846729747684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.6202300518063\n",
      "    mean_inference_ms: 1.1803355081144098\n",
      "    mean_raw_obs_processing_ms: 1.7205354634241323\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: -0.12715030818424067\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -4.8339210949778995\n",
      "      - 0.014136058378072747\n",
      "      - 6.8943433144806825\n",
      "      - -0.008793347011782249\n",
      "      - -0.17204371078010183\n",
      "      - -33.83507884919145\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14138846729747684\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.6202300518063\n",
      "      mean_inference_ms: 1.1803355081144098\n",
      "      mean_raw_obs_processing_ms: 1.7205354634241323\n",
      "  time_since_restore: 1252.3222970962524\n",
      "  time_this_iter_s: 3.2416014671325684\n",
      "  time_total_s: 1252.3222970962524\n",
      "  timers:\n",
      "    learn_throughput: 165.948\n",
      "    learn_time_ms: 48.208\n",
      "    load_throughput: 47113.777\n",
      "    load_time_ms: 0.17\n",
      "    training_iteration_time_ms: 3296.91\n",
      "    update_time_ms: 2.122\n",
      "  timestamp: 1657050385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2536\n",
      "  training_iteration: 317\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:25 (running for 00:21:20.99)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         1252.32</td><td style=\"text-align: right;\">2536</td><td style=\"text-align: right;\">-0.12715</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2552\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2552\n",
      "    num_agent_steps_trained: 2552\n",
      "    num_env_steps_sampled: 2552\n",
      "    num_env_steps_trained: 2552\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.951700149884786\n",
      "  episode_reward_mean: 0.2544532240796423\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 850\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3766499757766724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.6944213611132e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.376300185918808\n",
      "          total_loss: 0.4074722230434418\n",
      "          vf_explained_var: 0.1295095831155777\n",
      "          vf_loss: 0.031172115355730057\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2552\n",
      "    num_agent_steps_trained: 2552\n",
      "    num_env_steps_sampled: 2552\n",
      "    num_env_steps_trained: 2552\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2552\n",
      "  num_agent_steps_trained: 2552\n",
      "  num_env_steps_sampled: 2552\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2552\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.680000000000001\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1412881602065228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.2850822532595\n",
      "    mean_inference_ms: 1.179789078359017\n",
      "    mean_raw_obs_processing_ms: 1.7075412055949015\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.951700149884786\n",
      "    episode_reward_mean: 0.2544532240796423\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 37.951700149884786\n",
      "      - -0.861562962216162\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1412881602065228\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.2850822532595\n",
      "      mean_inference_ms: 1.179789078359017\n",
      "      mean_raw_obs_processing_ms: 1.7075412055949015\n",
      "  time_since_restore: 1258.8723027706146\n",
      "  time_this_iter_s: 3.09757661819458\n",
      "  time_total_s: 1258.8723027706146\n",
      "  timers:\n",
      "    learn_throughput: 163.76\n",
      "    learn_time_ms: 48.852\n",
      "    load_throughput: 42749.945\n",
      "    load_time_ms: 0.187\n",
      "    training_iteration_time_ms: 3369.068\n",
      "    update_time_ms: 2.133\n",
      "  timestamp: 1657050392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2552\n",
      "  training_iteration: 319\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:32 (running for 00:21:27.58)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         1258.87</td><td style=\"text-align: right;\">2552</td><td style=\"text-align: right;\">0.254453</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:37 (running for 00:21:32.61)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         1258.87</td><td style=\"text-align: right;\">2552</td><td style=\"text-align: right;\">0.254453</td><td style=\"text-align: right;\">             37.9517</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2560\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2560\n",
      "    num_agent_steps_trained: 2560\n",
      "    num_env_steps_sampled: 2560\n",
      "    num_env_steps_trained: 2560\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.03106692828413\n",
      "  episode_reward_mean: -0.06002280631307391\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 852\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.8127308330997636\n",
      "    episode_reward_mean: -0.8127308330997636\n",
      "    episode_reward_min: -0.8127308330997636\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.8127308330997636\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14072008083521392\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 870.7543506523488\n",
      "      mean_inference_ms: 1.4262335288092263\n",
      "      mean_raw_obs_processing_ms: 0.6631754840593881\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3786766529083252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.355282039294252e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5719661116600037\n",
      "          total_loss: -0.5089476704597473\n",
      "          vf_explained_var: -0.10531339049339294\n",
      "          vf_loss: 0.06301844865083694\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2560\n",
      "    num_agent_steps_trained: 2560\n",
      "    num_env_steps_sampled: 2560\n",
      "    num_env_steps_trained: 2560\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2560\n",
      "  num_agent_steps_trained: 2560\n",
      "  num_env_steps_sampled: 2560\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2560\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.114285714285714\n",
      "    ram_util_percent: 39.91428571428571\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14132508818473627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.659649333047\n",
      "    mean_inference_ms: 1.179940118767335\n",
      "    mean_raw_obs_processing_ms: 1.7167821444210623\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.03106692828413\n",
      "    episode_reward_mean: -0.06002280631307391\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 33.706967609645965\n",
      "      - 0.8081310900888425\n",
      "      - -33.66711862881224\n",
      "      - -5.269194473185557\n",
      "      - -0.08941183926154128\n",
      "      - 0.006818855342129115\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14132508818473627\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.659649333047\n",
      "      mean_inference_ms: 1.179940118767335\n",
      "      mean_raw_obs_processing_ms: 1.7167821444210623\n",
      "  time_since_restore: 1264.1556208133698\n",
      "  time_this_iter_s: 5.283318042755127\n",
      "  time_total_s: 1264.1556208133698\n",
      "  timers:\n",
      "    learn_throughput: 164.645\n",
      "    learn_time_ms: 48.589\n",
      "    load_throughput: 44555.082\n",
      "    load_time_ms: 0.18\n",
      "    training_iteration_time_ms: 3347.534\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1657050397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2560\n",
      "  training_iteration: 320\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2576\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2576\n",
      "    num_agent_steps_trained: 2576\n",
      "    num_env_steps_sampled: 2576\n",
      "    num_env_steps_trained: 2576\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.03106692828413\n",
      "  episode_reward_mean: -0.3528508084836179\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 858\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3717395067214966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.060875527211465e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3754853904247284\n",
      "          total_loss: 0.3778219223022461\n",
      "          vf_explained_var: -0.6625022292137146\n",
      "          vf_loss: 0.0023364881053566933\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2576\n",
      "    num_agent_steps_trained: 2576\n",
      "    num_env_steps_sampled: 2576\n",
      "    num_env_steps_trained: 2576\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2576\n",
      "  num_agent_steps_trained: 2576\n",
      "  num_env_steps_sampled: 2576\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2576\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.380000000000003\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14122592023150704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.3545828351773\n",
      "    mean_inference_ms: 1.1793499554732532\n",
      "    mean_raw_obs_processing_ms: 1.7038759053460992\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.03106692828413\n",
      "    episode_reward_mean: -0.3528508084836179\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -8.466173668062744\n",
      "      - 8.667879238474896\n",
      "      - -1.3235782459195011\n",
      "      - -34.18819073332091\n",
      "      - 0.06816356245365052\n",
      "      - 0.009840204949302267\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14122592023150704\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.3545828351773\n",
      "      mean_inference_ms: 1.1793499554732532\n",
      "      mean_raw_obs_processing_ms: 1.7038759053460992\n",
      "  time_since_restore: 1271.0896158218384\n",
      "  time_this_iter_s: 3.843618154525757\n",
      "  time_total_s: 1271.0896158218384\n",
      "  timers:\n",
      "    learn_throughput: 162.464\n",
      "    learn_time_ms: 49.242\n",
      "    load_throughput: 45577.876\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3469.991\n",
      "    update_time_ms: 2.317\n",
      "  timestamp: 1657050404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2576\n",
      "  training_iteration: 322\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:44 (running for 00:21:39.94)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         1271.09</td><td style=\"text-align: right;\">2576</td><td style=\"text-align: right;\">-0.352851</td><td style=\"text-align: right;\">             35.0311</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2592\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2592\n",
      "    num_agent_steps_trained: 2592\n",
      "    num_env_steps_sampled: 2592\n",
      "    num_env_steps_trained: 2592\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.03106692828413\n",
      "  episode_reward_mean: 0.013065003650521132\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 864\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.376616358757019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.560476438404294e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.5292600393295288\n",
      "          total_loss: 0.9012609124183655\n",
      "          vf_explained_var: -0.045191440731287\n",
      "          vf_loss: 0.37200072407722473\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2592\n",
      "    num_agent_steps_trained: 2592\n",
      "    num_env_steps_sampled: 2592\n",
      "    num_env_steps_trained: 2592\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2592\n",
      "  num_agent_steps_trained: 2592\n",
      "  num_env_steps_sampled: 2592\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2592\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.375\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14123435275448132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.8246440669122\n",
      "    mean_inference_ms: 1.1793223138505247\n",
      "    mean_raw_obs_processing_ms: 1.7111958757754229\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.03106692828413\n",
      "    episode_reward_mean: 0.013065003650521132\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.2577428341324604\n",
      "      - -0.004053856318729565\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14123435275448132\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.8246440669122\n",
      "      mean_inference_ms: 1.1793223138505247\n",
      "      mean_raw_obs_processing_ms: 1.7111958757754229\n",
      "  time_since_restore: 1277.8879146575928\n",
      "  time_this_iter_s: 3.127802610397339\n",
      "  time_total_s: 1277.8879146575928\n",
      "  timers:\n",
      "    learn_throughput: 163.224\n",
      "    learn_time_ms: 49.012\n",
      "    load_throughput: 45889.54\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3419.707\n",
      "    update_time_ms: 2.295\n",
      "  timestamp: 1657050411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2592\n",
      "  training_iteration: 324\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:51 (running for 00:21:46.81)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         1277.89</td><td style=\"text-align: right;\">2592</td><td style=\"text-align: right;\">0.013065</td><td style=\"text-align: right;\">             35.0311</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:46:56 (running for 00:21:51.82)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         1277.89</td><td style=\"text-align: right;\">2592</td><td style=\"text-align: right;\">0.013065</td><td style=\"text-align: right;\">             35.0311</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2600\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2600\n",
      "    num_agent_steps_trained: 2600\n",
      "    num_env_steps_sampled: 2600\n",
      "    num_env_steps_trained: 2600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.03106692828413\n",
      "  episode_reward_mean: -0.012486179091601395\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 866\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.5150395596983728\n",
      "    episode_reward_mean: 0.5150395596983728\n",
      "    episode_reward_min: 0.5150395596983728\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.5150395596983728\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14100999248271084\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 867.6193307857124\n",
      "      mean_inference_ms: 1.4273293164311622\n",
      "      mean_raw_obs_processing_ms: 0.6643229601334553\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3608379364013672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.4410175430821255e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.339664101600647\n",
      "          total_loss: 0.9602957367897034\n",
      "          vf_explained_var: -0.011384558863937855\n",
      "          vf_loss: 0.620631754398346\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2600\n",
      "    num_agent_steps_trained: 2600\n",
      "    num_env_steps_sampled: 2600\n",
      "    num_env_steps_trained: 2600\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2600\n",
      "  num_agent_steps_trained: 2600\n",
      "  num_env_steps_sampled: 2600\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2600\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.7125\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14116975082622918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.4456305935986\n",
      "    mean_inference_ms: 1.1789812008567713\n",
      "    mean_raw_obs_processing_ms: 1.7003158513882028\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.03106692828413\n",
      "    episode_reward_mean: -0.012486179091601395\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.020175398483416762\n",
      "      - 33.77727851874431\n",
      "      - -1.3156359268179445\n",
      "      - 0.13928185567376516\n",
      "      - 0.0027904946649693896\n",
      "      - -33.895904948826846\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14116975082622918\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.4456305935986\n",
      "      mean_inference_ms: 1.1789812008567713\n",
      "      mean_raw_obs_processing_ms: 1.7003158513882028\n",
      "  time_since_restore: 1283.217895746231\n",
      "  time_this_iter_s: 5.329981088638306\n",
      "  time_total_s: 1283.217895746231\n",
      "  timers:\n",
      "    learn_throughput: 163.789\n",
      "    learn_time_ms: 48.843\n",
      "    load_throughput: 45429.775\n",
      "    load_time_ms: 0.176\n",
      "    training_iteration_time_ms: 3383.217\n",
      "    update_time_ms: 2.284\n",
      "  timestamp: 1657050417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2600\n",
      "  training_iteration: 325\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2616\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2616\n",
      "    num_agent_steps_trained: 2616\n",
      "    num_env_steps_sampled: 2616\n",
      "    num_env_steps_trained: 2616\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-04\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 35.03106692828413\n",
      "  episode_reward_mean: 0.02766559400551956\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 872\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3726470470428467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.944848504033871e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.23472139239311218\n",
      "          total_loss: 0.1309686154127121\n",
      "          vf_explained_var: -0.004403595346957445\n",
      "          vf_loss: 0.3656899631023407\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2616\n",
      "    num_agent_steps_trained: 2616\n",
      "    num_env_steps_sampled: 2616\n",
      "    num_env_steps_trained: 2616\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2616\n",
      "  num_agent_steps_trained: 2616\n",
      "  num_env_steps_sampled: 2616\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2616\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.919999999999998\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14117311538133767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.906331783918\n",
      "    mean_inference_ms: 1.178951352266334\n",
      "    mean_raw_obs_processing_ms: 1.7076435422913188\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 35.03106692828413\n",
      "    episode_reward_mean: 0.02766559400551956\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0046790011247196395\n",
      "      - -0.0006958757967101548\n",
      "      - -0.05910992321519837\n",
      "      - 0.003135240903364478\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14117311538133767\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.906331783918\n",
      "      mean_inference_ms: 1.178951352266334\n",
      "      mean_raw_obs_processing_ms: 1.7076435422913188\n",
      "  time_since_restore: 1290.2434685230255\n",
      "  time_this_iter_s: 3.4685676097869873\n",
      "  time_total_s: 1290.2434685230255\n",
      "  timers:\n",
      "    learn_throughput: 165.624\n",
      "    learn_time_ms: 48.302\n",
      "    load_throughput: 43223.537\n",
      "    load_time_ms: 0.185\n",
      "    training_iteration_time_ms: 3373.092\n",
      "    update_time_ms: 2.267\n",
      "  timestamp: 1657050424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2616\n",
      "  training_iteration: 327\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:04 (running for 00:21:59.29)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         1290.24</td><td style=\"text-align: right;\">2616</td><td style=\"text-align: right;\">0.0276656</td><td style=\"text-align: right;\">             35.0311</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2632\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2632\n",
      "    num_agent_steps_trained: 2632\n",
      "    num_env_steps_sampled: 2632\n",
      "    num_env_steps_trained: 2632\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-10\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: 0.4077076149598098\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 876\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3809226751327515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.105670192686375e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.1880999058485031\n",
      "          total_loss: 2.430574655532837\n",
      "          vf_explained_var: -0.0028036197181791067\n",
      "          vf_loss: 2.6186745166778564\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2632\n",
      "    num_agent_steps_trained: 2632\n",
      "    num_env_steps_sampled: 2632\n",
      "    num_env_steps_trained: 2632\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2632\n",
      "  num_agent_steps_trained: 2632\n",
      "  num_env_steps_sampled: 2632\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2632\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.666666666666668\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1411428098802215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.9259703839546\n",
      "    mean_inference_ms: 1.1787620585989969\n",
      "    mean_raw_obs_processing_ms: 1.705910328404539\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: 0.4077076149598098\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0150329724645768\n",
      "      - 1.3277936547416944\n",
      "      - 1.3035861884128037\n",
      "      - -0.010568446263941045\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1411428098802215\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.9259703839546\n",
      "      mean_inference_ms: 1.1787620585989969\n",
      "      mean_raw_obs_processing_ms: 1.705910328404539\n",
      "  time_since_restore: 1296.998957157135\n",
      "  time_this_iter_s: 3.7802979946136475\n",
      "  time_total_s: 1296.998957157135\n",
      "  timers:\n",
      "    learn_throughput: 167.563\n",
      "    learn_time_ms: 47.743\n",
      "    load_throughput: 48587.362\n",
      "    load_time_ms: 0.165\n",
      "    training_iteration_time_ms: 3393.596\n",
      "    update_time_ms: 2.331\n",
      "  timestamp: 1657050430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2632\n",
      "  training_iteration: 329\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:11 (running for 00:22:06.18)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">            1297</td><td style=\"text-align: right;\">2632</td><td style=\"text-align: right;\">0.407708</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:16 (running for 00:22:11.18)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">            1297</td><td style=\"text-align: right;\">2632</td><td style=\"text-align: right;\">0.407708</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2640\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2640\n",
      "    num_agent_steps_trained: 2640\n",
      "    num_env_steps_sampled: 2640\n",
      "    num_env_steps_trained: 2640\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: 0.386202967764229\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 880\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -36.204312325504254\n",
      "    episode_reward_mean: -36.204312325504254\n",
      "    episode_reward_min: -36.204312325504254\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -36.204312325504254\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14164579573588157\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 862.1268020802407\n",
      "      mean_inference_ms: 1.4311907878473178\n",
      "      mean_raw_obs_processing_ms: 0.6648679474490372\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3722094297409058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.3134513159893686e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.19140036404132843\n",
      "          total_loss: 0.20839105546474457\n",
      "          vf_explained_var: -0.09458360821008682\n",
      "          vf_loss: 0.016990693286061287\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2640\n",
      "    num_agent_steps_trained: 2640\n",
      "    num_env_steps_sampled: 2640\n",
      "    num_env_steps_trained: 2640\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2640\n",
      "  num_agent_steps_trained: 2640\n",
      "  num_env_steps_sampled: 2640\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2640\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.07142857142857\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14111704540709186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.9459572472397\n",
      "    mean_inference_ms: 1.1785903024613922\n",
      "    mean_raw_obs_processing_ms: 1.7041725738025275\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: 0.386202967764229\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.059291492783123845\n",
      "      - 30.978295173026588\n",
      "      - 0.04420639516052871\n",
      "      - 0.032268248614831574\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14111704540709186\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.9459572472397\n",
      "      mean_inference_ms: 1.1785903024613922\n",
      "      mean_raw_obs_processing_ms: 1.7041725738025275\n",
      "  time_since_restore: 1302.4306197166443\n",
      "  time_this_iter_s: 5.431662559509277\n",
      "  time_total_s: 1302.4306197166443\n",
      "  timers:\n",
      "    learn_throughput: 165.324\n",
      "    learn_time_ms: 48.39\n",
      "    load_throughput: 45695.808\n",
      "    load_time_ms: 0.175\n",
      "    training_iteration_time_ms: 3469.204\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1657050436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2640\n",
      "  training_iteration: 330\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2656\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2656\n",
      "    num_agent_steps_trained: 2656\n",
      "    num_env_steps_sampled: 2656\n",
      "    num_env_steps_trained: 2656\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: -0.32267191040607174\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 884\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3719059228897095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.147359737136867e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.4111434519290924\n",
      "          total_loss: -0.3941144049167633\n",
      "          vf_explained_var: -0.1286425143480301\n",
      "          vf_loss: 0.01702902466058731\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2656\n",
      "    num_agent_steps_trained: 2656\n",
      "    num_env_steps_sampled: 2656\n",
      "    num_env_steps_trained: 2656\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2656\n",
      "  num_agent_steps_trained: 2656\n",
      "  num_env_steps_sampled: 2656\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2656\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.62\n",
      "    ram_util_percent: 40.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14109110470571146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.9487202478118\n",
      "    mean_inference_ms: 1.1784373809756261\n",
      "    mean_raw_obs_processing_ms: 1.7024595741686648\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: -0.32267191040607174\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -32.31273398054924\n",
      "      - -0.08857623344486498\n",
      "      - -0.004881146963805816\n",
      "      - -1.3185964351813442\n",
      "      - 35.03106692828413\n",
      "      - 0.08520354682396203\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14109110470571146\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.9487202478118\n",
      "      mean_inference_ms: 1.1784373809756261\n",
      "      mean_raw_obs_processing_ms: 1.7024595741686648\n",
      "  time_since_restore: 1309.042380809784\n",
      "  time_this_iter_s: 3.4818685054779053\n",
      "  time_total_s: 1309.042380809784\n",
      "  timers:\n",
      "    learn_throughput: 165.456\n",
      "    learn_time_ms: 48.351\n",
      "    load_throughput: 43046.096\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3437.285\n",
      "    update_time_ms: 2.131\n",
      "  timestamp: 1657050443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2656\n",
      "  training_iteration: 332\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:23 (running for 00:22:18.33)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         1309.04</td><td style=\"text-align: right;\">2656</td><td style=\"text-align: right;\">-0.322672</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2672\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2672\n",
      "    num_agent_steps_trained: 2672\n",
      "    num_env_steps_sampled: 2672\n",
      "    num_env_steps_trained: 2672\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-30\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: -0.3374231612808138\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 890\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3717153072357178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3407242477114778e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.11568405479192734\n",
      "          total_loss: 0.8376340270042419\n",
      "          vf_explained_var: 0.005604161880910397\n",
      "          vf_loss: 0.9533180594444275\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2672\n",
      "    num_agent_steps_trained: 2672\n",
      "    num_env_steps_sampled: 2672\n",
      "    num_env_steps_trained: 2672\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2672\n",
      "  num_agent_steps_trained: 2672\n",
      "  num_env_steps_sampled: 2672\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2672\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.7\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14100717140389224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.6237004742508\n",
      "    mean_inference_ms: 1.1780455979766247\n",
      "    mean_raw_obs_processing_ms: 1.6902797418109514\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: -0.3374231612808138\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.019051531612318984\n",
      "      - 0.002724559265561033\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14100717140389224\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.6237004742508\n",
      "      mean_inference_ms: 1.1780455979766247\n",
      "      mean_raw_obs_processing_ms: 1.6902797418109514\n",
      "  time_since_restore: 1316.5016903877258\n",
      "  time_this_iter_s: 3.7257168292999268\n",
      "  time_total_s: 1316.5016903877258\n",
      "  timers:\n",
      "    learn_throughput: 166.729\n",
      "    learn_time_ms: 47.982\n",
      "    load_throughput: 43919.414\n",
      "    load_time_ms: 0.182\n",
      "    training_iteration_time_ms: 3503.452\n",
      "    update_time_ms: 2.135\n",
      "  timestamp: 1657050450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2672\n",
      "  training_iteration: 334\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:30 (running for 00:22:25.82)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">          1316.5</td><td style=\"text-align: right;\">2672</td><td style=\"text-align: right;\">-0.337423</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:35 (running for 00:22:30.85)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">          1316.5</td><td style=\"text-align: right;\">2672</td><td style=\"text-align: right;\">-0.337423</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2680\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2680\n",
      "    num_agent_steps_trained: 2680\n",
      "    num_env_steps_sampled: 2680\n",
      "    num_env_steps_trained: 2680\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: -0.35138525710847973\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 892\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.039973863210367\n",
      "    episode_reward_mean: 0.039973863210367\n",
      "    episode_reward_min: 0.039973863210367\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.039973863210367\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14157224409651048\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 861.4346320086187\n",
      "      mean_inference_ms: 1.429459836223338\n",
      "      mean_raw_obs_processing_ms: 0.6651193788736174\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.377001404762268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4566171557817142e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.4066767394542694\n",
      "          total_loss: 0.7693714499473572\n",
      "          vf_explained_var: 0.04502290487289429\n",
      "          vf_loss: 0.36269471049308777\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2680\n",
      "    num_agent_steps_trained: 2680\n",
      "    num_env_steps_sampled: 2680\n",
      "    num_env_steps_trained: 2680\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2680\n",
      "  num_agent_steps_trained: 2680\n",
      "  num_env_steps_sampled: 2680\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2680\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.011111111111113\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14104805159168746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.0032082471724\n",
      "    mean_inference_ms: 1.1782401153920257\n",
      "    mean_raw_obs_processing_ms: 1.6991267230565164\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: -0.35138525710847973\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -33.77434485942513\n",
      "      - 1.3167367453439622\n",
      "      - 33.12363697121001\n",
      "      - -1.3199721487723939\n",
      "      - -33.12342593036962\n",
      "      - 0.02094769961975551\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14104805159168746\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.0032082471724\n",
      "      mean_inference_ms: 1.1782401153920257\n",
      "      mean_raw_obs_processing_ms: 1.6991267230565164\n",
      "  time_since_restore: 1322.7525165081024\n",
      "  time_this_iter_s: 6.250826120376587\n",
      "  time_total_s: 1322.7525165081024\n",
      "  timers:\n",
      "    learn_throughput: 165.802\n",
      "    learn_time_ms: 48.25\n",
      "    load_throughput: 44104.143\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3551.067\n",
      "    update_time_ms: 2.191\n",
      "  timestamp: 1657050456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2680\n",
      "  training_iteration: 335\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:40 (running for 00:22:35.95)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         1326.53</td><td style=\"text-align: right;\">2688</td><td style=\"text-align: right;\">-0.00152691</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2696\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2696\n",
      "    num_agent_steps_trained: 2696\n",
      "    num_env_steps_sampled: 2696\n",
      "    num_env_steps_trained: 2696\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-44\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: 0.33388078378366887\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 898\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3770434856414795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4498463315248955e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.32064083218574524\n",
      "          total_loss: 0.38327133655548096\n",
      "          vf_explained_var: 0.032514140009880066\n",
      "          vf_loss: 0.06263049691915512\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2696\n",
      "    num_agent_steps_trained: 2696\n",
      "    num_env_steps_sampled: 2696\n",
      "    num_env_steps_trained: 2696\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2696\n",
      "  num_agent_steps_trained: 2696\n",
      "  num_env_steps_sampled: 2696\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2696\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.16\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1409603845624893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.7067225195913\n",
      "    mean_inference_ms: 1.1778522162138592\n",
      "    mean_raw_obs_processing_ms: 1.6871061425929939\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: 0.33388078378366887\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.04630499982446423\n",
      "      - 0.01123067827537172\n",
      "      - -0.08583883046524332\n",
      "      - 33.88927608196249\n",
      "      - 0.040911729855442\n",
      "      - 5.111518627433583\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1409603845624893\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.7067225195913\n",
      "      mean_inference_ms: 1.1778522162138592\n",
      "      mean_raw_obs_processing_ms: 1.6871061425929939\n",
      "  time_since_restore: 1329.9513359069824\n",
      "  time_this_iter_s: 3.418370008468628\n",
      "  time_total_s: 1329.9513359069824\n",
      "  timers:\n",
      "    learn_throughput: 163.793\n",
      "    learn_time_ms: 48.842\n",
      "    load_throughput: 43118.006\n",
      "    load_time_ms: 0.186\n",
      "    training_iteration_time_ms: 3568.432\n",
      "    update_time_ms: 2.281\n",
      "  timestamp: 1657050464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2696\n",
      "  training_iteration: 337\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:48 (running for 00:22:43.25)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         1333.72</td><td style=\"text-align: right;\">2704</td><td style=\"text-align: right;\">0.283397</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2712\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2712\n",
      "    num_agent_steps_trained: 2712\n",
      "    num_env_steps_sampled: 2712\n",
      "    num_env_steps_trained: 2712\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-51\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: -0.10699576148304998\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 904\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3743927478790283\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.596910730469972e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.06072460487484932\n",
      "          total_loss: 5.9442596435546875\n",
      "          vf_explained_var: -0.00017376343021169305\n",
      "          vf_loss: 6.004985332489014\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2712\n",
      "    num_agent_steps_trained: 2712\n",
      "    num_env_steps_sampled: 2712\n",
      "    num_env_steps_trained: 2712\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2712\n",
      "  num_agent_steps_trained: 2712\n",
      "  num_env_steps_sampled: 2712\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2712\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.779999999999998\n",
      "    ram_util_percent: 39.96\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14098055449594799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.0835038042101\n",
      "    mean_inference_ms: 1.1779621652079961\n",
      "    mean_raw_obs_processing_ms: 1.694274753609042\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: -0.10699576148304998\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.03287474931862144\n",
      "      - 0.011603315708885287\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14098055449594799\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.0835038042101\n",
      "      mean_inference_ms: 1.1779621652079961\n",
      "      mean_raw_obs_processing_ms: 1.694274753609042\n",
      "  time_since_restore: 1336.8303589820862\n",
      "  time_this_iter_s: 3.1082303524017334\n",
      "  time_total_s: 1336.8303589820862\n",
      "  timers:\n",
      "    learn_throughput: 161.578\n",
      "    learn_time_ms: 49.512\n",
      "    load_throughput: 40436.77\n",
      "    load_time_ms: 0.198\n",
      "    training_iteration_time_ms: 3580.711\n",
      "    update_time_ms: 2.232\n",
      "  timestamp: 1657050471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2712\n",
      "  training_iteration: 339\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:47:56 (running for 00:22:51.36)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         1336.83</td><td style=\"text-align: right;\">2712</td><td style=\"text-align: right;\">-0.106996</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2720\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2720\n",
      "    num_agent_steps_trained: 2720\n",
      "    num_env_steps_sampled: 2720\n",
      "    num_env_steps_trained: 2720\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: -0.4033419526156997\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 906\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.05023742083916383\n",
      "    episode_reward_mean: -0.05023742083916383\n",
      "    episode_reward_min: -0.05023742083916383\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.05023742083916383\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14236961922994473\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 859.9312759027248\n",
      "      mean_inference_ms: 1.4265432590391578\n",
      "      mean_raw_obs_processing_ms: 0.6649738404808975\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3807650804519653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.156004474178189e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.27800819277763367\n",
      "          total_loss: 4.503990173339844\n",
      "          vf_explained_var: -0.001005796599201858\n",
      "          vf_loss: 4.22598123550415\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2720\n",
      "    num_agent_steps_trained: 2720\n",
      "    num_env_steps_sampled: 2720\n",
      "    num_env_steps_trained: 2720\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2720\n",
      "  num_agent_steps_trained: 2720\n",
      "  num_env_steps_sampled: 2720\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2720\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.6625\n",
      "    ram_util_percent: 39.912499999999994\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1409154695900912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 725.71887183422\n",
      "    mean_inference_ms: 1.1776858724588535\n",
      "    mean_raw_obs_processing_ms: 1.6840154102679512\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: -0.4033419526156997\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.3717838122199666\n",
      "      - -0.292913187902073\n",
      "      - 0.05275516323877971\n",
      "      - -0.02645083858829622\n",
      "      - -0.04408237270517823\n",
      "      - -0.3914770937329948\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1409154695900912\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 725.71887183422\n",
      "      mean_inference_ms: 1.1776858724588535\n",
      "      mean_raw_obs_processing_ms: 1.6840154102679512\n",
      "  time_since_restore: 1342.3461332321167\n",
      "  time_this_iter_s: 5.515774250030518\n",
      "  time_total_s: 1342.3461332321167\n",
      "  timers:\n",
      "    learn_throughput: 161.856\n",
      "    learn_time_ms: 49.427\n",
      "    load_throughput: 41692.883\n",
      "    load_time_ms: 0.192\n",
      "    training_iteration_time_ms: 3512.736\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1657050476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2720\n",
      "  training_iteration: 340\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2736\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2736\n",
      "    num_agent_steps_trained: 2736\n",
      "    num_env_steps_sampled: 2736\n",
      "    num_env_steps_trained: 2736\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 37.967849081178755\n",
      "  episode_reward_mean: -0.39982308818634793\n",
      "  episode_reward_min: -38.671865721941174\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 912\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3667174577713013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.641048220335506e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.003489746479317546\n",
      "          total_loss: 0.6891491413116455\n",
      "          vf_explained_var: 0.021526465192437172\n",
      "          vf_loss: 0.6856594085693359\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2736\n",
      "    num_agent_steps_trained: 2736\n",
      "    num_env_steps_sampled: 2736\n",
      "    num_env_steps_trained: 2736\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2736\n",
      "  num_agent_steps_trained: 2736\n",
      "  num_env_steps_sampled: 2736\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2736\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.119999999999997\n",
      "    ram_util_percent: 39.9\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1409413386399822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.170802148886\n",
      "    mean_inference_ms: 1.1778424095259863\n",
      "    mean_raw_obs_processing_ms: 1.6912352288456658\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 37.967849081178755\n",
      "    episode_reward_mean: -0.39982308818634793\n",
      "    episode_reward_min: -38.671865721941174\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.04949504507206948\n",
      "      - 26.898991781873953\n",
      "      - -38.671865721941174\n",
      "      - 0.009050545819682232\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1409413386399822\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.170802148886\n",
      "      mean_inference_ms: 1.1778424095259863\n",
      "      mean_raw_obs_processing_ms: 1.6912352288456658\n",
      "  time_since_restore: 1349.655588388443\n",
      "  time_this_iter_s: 3.795313596725464\n",
      "  time_total_s: 1349.655588388443\n",
      "  timers:\n",
      "    learn_throughput: 164.528\n",
      "    learn_time_ms: 48.624\n",
      "    load_throughput: 44237.88\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3582.692\n",
      "    update_time_ms: 2.266\n",
      "  timestamp: 1657050484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2736\n",
      "  training_iteration: 342\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:04 (running for 00:22:59.30)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         1349.66</td><td style=\"text-align: right;\">2736</td><td style=\"text-align: right;\">-0.399823</td><td style=\"text-align: right;\">             37.9678</td><td style=\"text-align: right;\">            -38.6719</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2752\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2752\n",
      "    num_agent_steps_trained: 2752\n",
      "    num_env_steps_sampled: 2752\n",
      "    num_env_steps_trained: 2752\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-10\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.395293382185145\n",
      "  episode_reward_mean: 0.44164261079552875\n",
      "  episode_reward_min: -38.45515890975497\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 916\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3675113916397095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.047679794894066e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.36794957518577576\n",
      "          total_loss: 7.6372151374816895\n",
      "          vf_explained_var: -0.0005023002740927041\n",
      "          vf_loss: 8.005166053771973\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2752\n",
      "    num_agent_steps_trained: 2752\n",
      "    num_env_steps_sampled: 2752\n",
      "    num_env_steps_trained: 2752\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2752\n",
      "  num_agent_steps_trained: 2752\n",
      "  num_env_steps_sampled: 2752\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2752\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.85\n",
      "    ram_util_percent: 39.95\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14092749715772684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.25911569553\n",
      "    mean_inference_ms: 1.1777870324106505\n",
      "    mean_raw_obs_processing_ms: 1.6897529215056104\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.395293382185145\n",
      "    episode_reward_mean: 0.44164261079552875\n",
      "    episode_reward_min: -38.45515890975497\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 7.295035874874706\n",
      "      - 33.95051398941895\n",
      "      - -9.750849129626477\n",
      "      - 0.06036949806506442\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14092749715772684\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.25911569553\n",
      "      mean_inference_ms: 1.1777870324106505\n",
      "      mean_raw_obs_processing_ms: 1.6897529215056104\n",
      "  time_since_restore: 1356.2737436294556\n",
      "  time_this_iter_s: 2.8779330253601074\n",
      "  time_total_s: 1356.2737436294556\n",
      "  timers:\n",
      "    learn_throughput: 163.303\n",
      "    learn_time_ms: 48.989\n",
      "    load_throughput: 44156.378\n",
      "    load_time_ms: 0.181\n",
      "    training_iteration_time_ms: 3498.412\n",
      "    update_time_ms: 2.232\n",
      "  timestamp: 1657050490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2752\n",
      "  training_iteration: 344\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:10 (running for 00:23:06.03)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         1356.27</td><td style=\"text-align: right;\">2752</td><td style=\"text-align: right;\">0.441643</td><td style=\"text-align: right;\">             38.3953</td><td style=\"text-align: right;\">            -38.4552</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:15 (running for 00:23:11.04)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         1356.27</td><td style=\"text-align: right;\">2752</td><td style=\"text-align: right;\">0.441643</td><td style=\"text-align: right;\">             38.3953</td><td style=\"text-align: right;\">            -38.4552</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2760\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2760\n",
      "    num_agent_steps_trained: 2760\n",
      "    num_env_steps_sampled: 2760\n",
      "    num_env_steps_trained: 2760\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.395293382185145\n",
      "  episode_reward_mean: -0.584801601742675\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 920\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.05592191435664129\n",
      "    episode_reward_mean: 0.05592191435664129\n",
      "    episode_reward_min: 0.05592191435664129\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.05592191435664129\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14206767082214355\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 860.9692144852418\n",
      "      mean_inference_ms: 1.4229233448322003\n",
      "      mean_raw_obs_processing_ms: 0.6676006775635939\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3789174556732178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.2688596851076e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.030224760994315147\n",
      "          total_loss: 0.03732433542609215\n",
      "          vf_explained_var: -0.3383151888847351\n",
      "          vf_loss: 0.007099578622728586\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2760\n",
      "    num_agent_steps_trained: 2760\n",
      "    num_env_steps_sampled: 2760\n",
      "    num_env_steps_trained: 2760\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2760\n",
      "  num_agent_steps_trained: 2760\n",
      "  num_env_steps_sampled: 2760\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2760\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.41111111111111\n",
      "    ram_util_percent: 39.93333333333333\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14091644270977305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.3708163484847\n",
      "    mean_inference_ms: 1.177725026230893\n",
      "    mean_raw_obs_processing_ms: 1.6882954179642116\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.395293382185145\n",
      "    episode_reward_mean: -0.584801601742675\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 6.794041753809655\n",
      "      - -0.4374966519587602\n",
      "      - -34.02368474893066\n",
      "      - 0.014109743249188011\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14091644270977305\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.3708163484847\n",
      "      mean_inference_ms: 1.177725026230893\n",
      "      mean_raw_obs_processing_ms: 1.6882954179642116\n",
      "  time_since_restore: 1362.3287181854248\n",
      "  time_this_iter_s: 6.054974555969238\n",
      "  time_total_s: 1362.3287181854248\n",
      "  timers:\n",
      "    learn_throughput: 163.137\n",
      "    learn_time_ms: 49.038\n",
      "    load_throughput: 42153.809\n",
      "    load_time_ms: 0.19\n",
      "    training_iteration_time_ms: 3443.945\n",
      "    update_time_ms: 2.238\n",
      "  timestamp: 1657050496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2760\n",
      "  training_iteration: 345\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:20 (running for 00:23:16.11)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         1366.25</td><td style=\"text-align: right;\">2768</td><td style=\"text-align: right;\">-0.32031</td><td style=\"text-align: right;\">             38.3953</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2776\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2776\n",
      "    num_agent_steps_trained: 2776\n",
      "    num_env_steps_sampled: 2776\n",
      "    num_env_steps_trained: 2776\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-23\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.52516388071549\n",
      "  episode_reward_mean: 0.4037291904074011\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 924\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3795825242996216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8748447675752686e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.3268587291240692\n",
      "          total_loss: 1.8238646984100342\n",
      "          vf_explained_var: -0.0003317117807455361\n",
      "          vf_loss: 2.150723457336426\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2776\n",
      "    num_agent_steps_trained: 2776\n",
      "    num_env_steps_sampled: 2776\n",
      "    num_env_steps_trained: 2776\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2776\n",
      "  num_agent_steps_trained: 2776\n",
      "  num_env_steps_sampled: 2776\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2776\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.68\n",
      "    ram_util_percent: 39.96\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1409064384973459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.5097340849716\n",
      "    mean_inference_ms: 1.1776707815776868\n",
      "    mean_raw_obs_processing_ms: 1.686878605995406\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.52516388071549\n",
      "    episode_reward_mean: 0.4037291904074011\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.05392830490109901\n",
      "      - -0.006415297265690123\n",
      "      - 0.34684193047184664\n",
      "      - 34.38178358740462\n",
      "      - -32.5300373163346\n",
      "      - 33.634705799821425\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1409064384973459\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.5097340849716\n",
      "      mean_inference_ms: 1.1776707815776868\n",
      "      mean_raw_obs_processing_ms: 1.686878605995406\n",
      "  time_since_restore: 1369.2308044433594\n",
      "  time_this_iter_s: 2.980952501296997\n",
      "  time_total_s: 1369.2308044433594\n",
      "  timers:\n",
      "    learn_throughput: 164.571\n",
      "    learn_time_ms: 48.611\n",
      "    load_throughput: 42538.58\n",
      "    load_time_ms: 0.188\n",
      "    training_iteration_time_ms: 3413.855\n",
      "    update_time_ms: 2.208\n",
      "  timestamp: 1657050503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2776\n",
      "  training_iteration: 347\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:26 (running for 00:23:21.91)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         1371.97</td><td style=\"text-align: right;\">2784</td><td style=\"text-align: right;\">-0.063614</td><td style=\"text-align: right;\">             38.5252</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2792\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2792\n",
      "    num_agent_steps_trained: 2792\n",
      "    num_env_steps_sampled: 2792\n",
      "    num_env_steps_trained: 2792\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-29\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.52516388071549\n",
      "  episode_reward_mean: 0.00714078348589041\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 930\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3552151918411255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4009793630975764e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.19002743065357208\n",
      "          total_loss: 10.190028190612793\n",
      "          vf_explained_var: -0.0037580609787255526\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2792\n",
      "    num_agent_steps_trained: 2792\n",
      "    num_env_steps_sampled: 2792\n",
      "    num_env_steps_trained: 2792\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2792\n",
      "  num_agent_steps_trained: 2792\n",
      "  num_env_steps_sampled: 2792\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2792\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.4\n",
      "    ram_util_percent: 39.96666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14085069674409315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.342395665175\n",
      "    mean_inference_ms: 1.1774282793086954\n",
      "    mean_raw_obs_processing_ms: 1.6756012495874435\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.52516388071549\n",
      "    episode_reward_mean: 0.00714078348589041\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.06164585715500692\n",
      "      - 0.12321378776957914\n",
      "      - -33.63726033451079\n",
      "      - -34.483267920843964\n",
      "      - -0.056985556350398814\n",
      "      - 0.003516731235034709\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14085069674409315\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.342395665175\n",
      "      mean_inference_ms: 1.1774282793086954\n",
      "      mean_raw_obs_processing_ms: 1.6756012495874435\n",
      "  time_since_restore: 1374.4339561462402\n",
      "  time_this_iter_s: 2.459052324295044\n",
      "  time_total_s: 1374.4339561462402\n",
      "  timers:\n",
      "    learn_throughput: 164.734\n",
      "    learn_time_ms: 48.563\n",
      "    load_throughput: 37663.522\n",
      "    load_time_ms: 0.212\n",
      "    training_iteration_time_ms: 3246.308\n",
      "    update_time_ms: 2.306\n",
      "  timestamp: 1657050509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2792\n",
      "  training_iteration: 349\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:33 (running for 00:23:29.10)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         1379.03</td><td style=\"text-align: right;\">2800</td><td style=\"text-align: right;\">-0.0740395</td><td style=\"text-align: right;\">             38.5252</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2808\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2808\n",
      "    num_agent_steps_trained: 2808\n",
      "    num_env_steps_sampled: 2808\n",
      "    num_env_steps_trained: 2808\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-37\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.52516388071549\n",
      "  episode_reward_mean: 0.28752607013004094\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 936\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3781784772872925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8595406800159253e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.569390594959259\n",
      "          total_loss: -0.524886965751648\n",
      "          vf_explained_var: 0.0834096297621727\n",
      "          vf_loss: 0.04450361058115959\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2808\n",
      "    num_agent_steps_trained: 2808\n",
      "    num_env_steps_sampled: 2808\n",
      "    num_env_steps_trained: 2808\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2808\n",
      "  num_agent_steps_trained: 2808\n",
      "  num_env_steps_sampled: 2808\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2808\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1\n",
      "    ram_util_percent: 39.940000000000005\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14089835580599355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.8729874483937\n",
      "    mean_inference_ms: 1.177626781538213\n",
      "    mean_raw_obs_processing_ms: 1.6828402984768636\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.52516388071549\n",
      "    episode_reward_mean: 0.28752607013004094\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0012723416855446867\n",
      "      - 1.3424751740951224\n",
      "      - -0.012689920653816134\n",
      "      - -0.0020415622800152855\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14089835580599355\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.8729874483937\n",
      "      mean_inference_ms: 1.177626781538213\n",
      "      mean_raw_obs_processing_ms: 1.6828402984768636\n",
      "  time_since_restore: 1382.7527167797089\n",
      "  time_this_iter_s: 3.727158308029175\n",
      "  time_total_s: 1382.7527167797089\n",
      "  timers:\n",
      "    learn_throughput: 165.159\n",
      "    learn_time_ms: 48.438\n",
      "    load_throughput: 37187.667\n",
      "    load_time_ms: 0.215\n",
      "    training_iteration_time_ms: 3219.215\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1657050517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2808\n",
      "  training_iteration: 351\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:40 (running for 00:23:35.66)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         1385.54</td><td style=\"text-align: right;\">2816</td><td style=\"text-align: right;\">0.255557</td><td style=\"text-align: right;\">             38.5252</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2824\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2824\n",
      "    num_agent_steps_trained: 2824\n",
      "    num_env_steps_sampled: 2824\n",
      "    num_env_steps_trained: 2824\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-43\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.52516388071549\n",
      "  episode_reward_mean: 0.27785988338209533\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 940\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.38079833984375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7198036630361457e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.3901858627796173\n",
      "          total_loss: 2.4936447143554688\n",
      "          vf_explained_var: -0.004393311217427254\n",
      "          vf_loss: 2.1034586429595947\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2824\n",
      "    num_agent_steps_trained: 2824\n",
      "    num_env_steps_sampled: 2824\n",
      "    num_env_steps_trained: 2824\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2824\n",
      "  num_agent_steps_trained: 2824\n",
      "  num_env_steps_sampled: 2824\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2824\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.419999999999998\n",
      "    ram_util_percent: 39.940000000000005\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1408939938631891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.9231362758244\n",
      "    mean_inference_ms: 1.1775506715780804\n",
      "    mean_raw_obs_processing_ms: 1.6815360763152214\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.52516388071549\n",
      "    episode_reward_mean: 0.27785988338209533\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -1.3310329163789663\n",
      "      - 34.30983906176145\n",
      "      - 0.05414717621663234\n",
      "      - -12.394466818433237\n",
      "      - -0.03909258736914978\n",
      "      - -0.009507053242304386\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1408939938631891\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.9231362758244\n",
      "      mean_inference_ms: 1.1775506715780804\n",
      "      mean_raw_obs_processing_ms: 1.6815360763152214\n",
      "  time_since_restore: 1388.808762550354\n",
      "  time_this_iter_s: 3.268329381942749\n",
      "  time_total_s: 1388.808762550354\n",
      "  timers:\n",
      "    learn_throughput: 167.149\n",
      "    learn_time_ms: 47.862\n",
      "    load_throughput: 37299.28\n",
      "    load_time_ms: 0.214\n",
      "    training_iteration_time_ms: 3071.366\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1657050523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2824\n",
      "  training_iteration: 353\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:47 (running for 00:23:42.33)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         1392.08</td><td style=\"text-align: right;\">2832</td><td style=\"text-align: right;\">0.0418995</td><td style=\"text-align: right;\">             38.5252</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:48:52 (running for 00:23:47.34)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         1392.08</td><td style=\"text-align: right;\">2832</td><td style=\"text-align: right;\">0.0418995</td><td style=\"text-align: right;\">             38.5252</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2840\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2840\n",
      "    num_agent_steps_trained: 2840\n",
      "    num_env_steps_sampled: 2840\n",
      "    num_env_steps_trained: 2840\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-52\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 38.52516388071549\n",
      "  episode_reward_mean: 0.10997135672959288\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 946\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -0.8212619151261578\n",
      "    episode_reward_mean: -0.8212619151261578\n",
      "    episode_reward_min: -0.8212619151261578\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.8212619151261578\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14281941351489486\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 855.6263379961531\n",
      "      mean_inference_ms: 1.4240886563452606\n",
      "      mean_raw_obs_processing_ms: 0.668676100044607\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3703625202178955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.359221399383387e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.42684897780418396\n",
      "          total_loss: 3.577515125274658\n",
      "          vf_explained_var: -0.002918676473200321\n",
      "          vf_loss: 4.004364490509033\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2840\n",
      "    num_agent_steps_trained: 2840\n",
      "    num_env_steps_sampled: 2840\n",
      "    num_env_steps_trained: 2840\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2840\n",
      "  num_agent_steps_trained: 2840\n",
      "  num_env_steps_sampled: 2840\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2840\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 39.925\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14083423239002463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.5773637755025\n",
      "    mean_inference_ms: 1.177189557855965\n",
      "    mean_raw_obs_processing_ms: 1.6705472171967157\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 38.52516388071549\n",
      "    episode_reward_mean: 0.10997135672959288\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 10.036070213403484\n",
      "      - -4.2840138014051945\n",
      "      - 0.039196501265114314\n",
      "      - 0.4763423246338938\n",
      "      - -0.04087419645463963\n",
      "      - 5.683408344851653\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14083423239002463\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.5773637755025\n",
      "      mean_inference_ms: 1.177189557855965\n",
      "      mean_raw_obs_processing_ms: 1.6705472171967157\n",
      "  time_since_restore: 1397.887286901474\n",
      "  time_this_iter_s: 5.810958623886108\n",
      "  time_total_s: 1397.887286901474\n",
      "  timers:\n",
      "    learn_throughput: 168.619\n",
      "    learn_time_ms: 47.444\n",
      "    load_throughput: 37701.609\n",
      "    load_time_ms: 0.212\n",
      "    training_iteration_time_ms: 3146.114\n",
      "    update_time_ms: 2.34\n",
      "  timestamp: 1657050532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2840\n",
      "  training_iteration: 355\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2856\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2856\n",
      "    num_agent_steps_trained: 2856\n",
      "    num_env_steps_sampled: 2856\n",
      "    num_env_steps_trained: 2856\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: 0.05874278874886109\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 952\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3793543577194214\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00010130159353138879\n",
      "          model: {}\n",
      "          policy_loss: -0.18089167773723602\n",
      "          total_loss: 1.8198000192642212\n",
      "          vf_explained_var: -0.0003892739478033036\n",
      "          vf_loss: 2.0006916522979736\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2856\n",
      "    num_agent_steps_trained: 2856\n",
      "    num_env_steps_sampled: 2856\n",
      "    num_env_steps_trained: 2856\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2856\n",
      "  num_agent_steps_trained: 2856\n",
      "  num_env_steps_sampled: 2856\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2856\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.439999999999998\n",
      "    ram_util_percent: 40.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14087924718419254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.0506242457254\n",
      "    mean_inference_ms: 1.1773456839948349\n",
      "    mean_raw_obs_processing_ms: 1.6776588722049786\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: 0.05874278874886109\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.039919671480795005\n",
      "      - -0.0328455525699628\n",
      "      - -33.76736419272222\n",
      "      - 0.005520170434646032\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14087924718419254\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.0506242457254\n",
      "      mean_inference_ms: 1.1773456839948349\n",
      "      mean_raw_obs_processing_ms: 1.6776588722049786\n",
      "  time_since_restore: 1404.7588715553284\n",
      "  time_this_iter_s: 3.466261386871338\n",
      "  time_total_s: 1404.7588715553284\n",
      "  timers:\n",
      "    learn_throughput: 165.107\n",
      "    learn_time_ms: 48.453\n",
      "    load_throughput: 38692.841\n",
      "    load_time_ms: 0.207\n",
      "    training_iteration_time_ms: 3143.505\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1657050539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2856\n",
      "  training_iteration: 357\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:00 (running for 00:23:55.16)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         1404.76</td><td style=\"text-align: right;\">2856</td><td style=\"text-align: right;\">0.0587428</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2872\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2872\n",
      "    num_agent_steps_trained: 2872\n",
      "    num_env_steps_sampled: 2872\n",
      "    num_env_steps_trained: 2872\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-06\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: 0.40184423041479733\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 956\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3749558925628662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.9919696569559164e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.04807257279753685\n",
      "          total_loss: 5.952366352081299\n",
      "          vf_explained_var: -0.0012754718773066998\n",
      "          vf_loss: 6.000439643859863\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2872\n",
      "    num_agent_steps_trained: 2872\n",
      "    num_env_steps_sampled: 2872\n",
      "    num_env_steps_trained: 2872\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2872\n",
      "  num_agent_steps_trained: 2872\n",
      "  num_env_steps_sampled: 2872\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2872\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.816666666666665\n",
      "    ram_util_percent: 39.95\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14087440749337335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.0918512428658\n",
      "    mean_inference_ms: 1.1772949913158954\n",
      "    mean_raw_obs_processing_ms: 1.676402340988606\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: 0.40184423041479733\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.02912364686848201\n",
      "      - -0.060961346728546695\n",
      "      - 32.70337148828292\n",
      "      - 0.06988833050714993\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14087440749337335\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.0918512428658\n",
      "      mean_inference_ms: 1.1772949913158954\n",
      "      mean_raw_obs_processing_ms: 1.676402340988606\n",
      "  time_since_restore: 1411.4476075172424\n",
      "  time_this_iter_s: 3.7545852661132812\n",
      "  time_total_s: 1411.4476075172424\n",
      "  timers:\n",
      "    learn_throughput: 166.705\n",
      "    learn_time_ms: 47.989\n",
      "    load_throughput: 46791.845\n",
      "    load_time_ms: 0.171\n",
      "    training_iteration_time_ms: 3292.143\n",
      "    update_time_ms: 2.309\n",
      "  timestamp: 1657050546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2872\n",
      "  training_iteration: 359\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:06 (running for 00:24:01.90)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         1411.45</td><td style=\"text-align: right;\">2872</td><td style=\"text-align: right;\">0.401844</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:11 (running for 00:24:06.94)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         1411.45</td><td style=\"text-align: right;\">2872</td><td style=\"text-align: right;\">0.401844</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2880\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2880\n",
      "    num_agent_steps_trained: 2880\n",
      "    num_env_steps_sampled: 2880\n",
      "    num_env_steps_trained: 2880\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-11\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.32706529308027027\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 960\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 7.3915241023930776\n",
      "    episode_reward_mean: 7.3915241023930776\n",
      "    episode_reward_min: 7.3915241023930776\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 7.3915241023930776\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14264352859989288\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 850.2894535592073\n",
      "      mean_inference_ms: 1.4214801348848825\n",
      "      mean_raw_obs_processing_ms: 0.6685971115041988\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3667479753494263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.367544417822501e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.5730162262916565\n",
      "          total_loss: -0.41027480363845825\n",
      "          vf_explained_var: -0.10150335729122162\n",
      "          vf_loss: 0.16274137794971466\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2880\n",
      "    num_agent_steps_trained: 2880\n",
      "    num_env_steps_sampled: 2880\n",
      "    num_env_steps_trained: 2880\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2880\n",
      "  num_agent_steps_trained: 2880\n",
      "  num_env_steps_sampled: 2880\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2880\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 39.95714285714286\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14086727608305483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.1130848945074\n",
      "    mean_inference_ms: 1.177236802330197\n",
      "    mean_raw_obs_processing_ms: 1.675173865934804\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.32706529308027027\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -32.70543288651766\n",
      "      - 0.003538542381320231\n",
      "      - 1.274815614388514\n",
      "      - 0.013340482946360588\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14086727608305483\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.1130848945074\n",
      "      mean_inference_ms: 1.177236802330197\n",
      "      mean_raw_obs_processing_ms: 1.675173865934804\n",
      "  time_since_restore: 1416.5473954677582\n",
      "  time_this_iter_s: 5.099787950515747\n",
      "  time_total_s: 1416.5473954677582\n",
      "  timers:\n",
      "    learn_throughput: 167.831\n",
      "    learn_time_ms: 47.667\n",
      "    load_throughput: 46034.342\n",
      "    load_time_ms: 0.174\n",
      "    training_iteration_time_ms: 3385.675\n",
      "    update_time_ms: 2.27\n",
      "  timestamp: 1657050551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2880\n",
      "  training_iteration: 360\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2896\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2896\n",
      "    num_agent_steps_trained: 2896\n",
      "    num_env_steps_sampled: 2896\n",
      "    num_env_steps_trained: 2896\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.014059490292216337\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 964\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3825801610946655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.4529150323360227e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.5244465470314026\n",
      "          total_loss: 0.5258172154426575\n",
      "          vf_explained_var: -0.3362450897693634\n",
      "          vf_loss: 0.0013707225443795323\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2896\n",
      "    num_agent_steps_trained: 2896\n",
      "    num_env_steps_sampled: 2896\n",
      "    num_env_steps_trained: 2896\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2896\n",
      "  num_agent_steps_trained: 2896\n",
      "  num_env_steps_sampled: 2896\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2896\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.14\n",
      "    ram_util_percent: 39.980000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14086151168589306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.140957192468\n",
      "    mean_inference_ms: 1.1771805240678652\n",
      "    mean_raw_obs_processing_ms: 1.6739176537702622\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.014059490292216337\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.0015759672047055417\n",
      "      - -1.2998533291938101\n",
      "      - -0.06008952115886457\n",
      "      - -0.04374260324497026\n",
      "      - 1.4041597571904707\n",
      "      - -0.021574254049088193\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14086151168589306\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.140957192468\n",
      "      mean_inference_ms: 1.1771805240678652\n",
      "      mean_raw_obs_processing_ms: 1.6739176537702622\n",
      "  time_since_restore: 1423.3397467136383\n",
      "  time_this_iter_s: 3.6721339225769043\n",
      "  time_total_s: 1423.3397467136383\n",
      "  timers:\n",
      "    learn_throughput: 167.609\n",
      "    learn_time_ms: 47.73\n",
      "    load_throughput: 43759.04\n",
      "    load_time_ms: 0.183\n",
      "    training_iteration_time_ms: 3412.689\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1657050558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2896\n",
      "  training_iteration: 362\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:18 (running for 00:24:13.94)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         1423.34</td><td style=\"text-align: right;\">2896</td><td style=\"text-align: right;\">-0.0140595</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2912\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2912\n",
      "    num_agent_steps_trained: 2912\n",
      "    num_env_steps_sampled: 2912\n",
      "    num_env_steps_trained: 2912\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-26\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.0006720760715381813\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 970\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3708910942077637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.043789198884042e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.2093314826488495\n",
      "          total_loss: 0.41649696230888367\n",
      "          vf_explained_var: -0.0213793832808733\n",
      "          vf_loss: 0.6258284449577332\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2912\n",
      "    num_agent_steps_trained: 2912\n",
      "    num_env_steps_sampled: 2912\n",
      "    num_env_steps_trained: 2912\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2912\n",
      "  num_agent_steps_trained: 2912\n",
      "  num_env_steps_sampled: 2912\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2912\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.516666666666666\n",
      "    ram_util_percent: 39.983333333333334\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14081066470525178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.8169660777384\n",
      "    mean_inference_ms: 1.1769055486489544\n",
      "    mean_raw_obs_processing_ms: 1.6633091969855287\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.0006720760715381813\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 1.3153162766144755\n",
      "      - 0.10874224931489618\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14081066470525178\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.8169660777384\n",
      "      mean_inference_ms: 1.1769055486489544\n",
      "      mean_raw_obs_processing_ms: 1.6633091969855287\n",
      "  time_since_restore: 1430.9850795269012\n",
      "  time_this_iter_s: 3.8046350479125977\n",
      "  time_total_s: 1430.9850795269012\n",
      "  timers:\n",
      "    learn_throughput: 166.053\n",
      "    learn_time_ms: 48.177\n",
      "    load_throughput: 46345.901\n",
      "    load_time_ms: 0.173\n",
      "    training_iteration_time_ms: 3523.87\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1657050566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2912\n",
      "  training_iteration: 364\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:26 (running for 00:24:21.70)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         1430.99</td><td style=\"text-align: right;\">2912</td><td style=\"text-align: right;\">-0.000672076</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:31 (running for 00:24:26.70)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         1430.99</td><td style=\"text-align: right;\">2912</td><td style=\"text-align: right;\">-0.000672076</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2920\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2920\n",
      "    num_agent_steps_trained: 2920\n",
      "    num_env_steps_sampled: 2920\n",
      "    num_env_steps_trained: 2920\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-32\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.0148147377579992\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 972\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -10.885323613313066\n",
      "    episode_reward_mean: -10.885323613313066\n",
      "    episode_reward_min: -10.885323613313066\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -10.885323613313066\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14269351959228516\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 846.6302362355319\n",
      "      mean_inference_ms: 1.4190153642134233\n",
      "      mean_raw_obs_processing_ms: 0.6675416773015803\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.370984673500061\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2216871002456173e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.23582978546619415\n",
      "          total_loss: 0.23741115629673004\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015813391655683517\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2920\n",
      "    num_agent_steps_trained: 2920\n",
      "    num_env_steps_sampled: 2920\n",
      "    num_env_steps_trained: 2920\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2920\n",
      "  num_agent_steps_trained: 2920\n",
      "  num_env_steps_sampled: 2920\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2920\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.666666666666666\n",
      "    ram_util_percent: 39.96666666666667\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14086098350843693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.2306220075236\n",
      "    mean_inference_ms: 1.1771485666501693\n",
      "    mean_raw_obs_processing_ms: 1.6715127278260864\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.0148147377579992\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.025105494452573485\n",
      "      - -0.03448237916122876\n",
      "      - -0.01561966027434103\n",
      "      - 37.967849081178755\n",
      "      - -0.0562424115259228\n",
      "      - 0.054750769817856426\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14086098350843693\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.2306220075236\n",
      "      mean_inference_ms: 1.1771485666501693\n",
      "      mean_raw_obs_processing_ms: 1.6715127278260864\n",
      "  time_since_restore: 1437.214032649994\n",
      "  time_this_iter_s: 6.228953123092651\n",
      "  time_total_s: 1437.214032649994\n",
      "  timers:\n",
      "    learn_throughput: 166.101\n",
      "    learn_time_ms: 48.163\n",
      "    load_throughput: 44691.572\n",
      "    load_time_ms: 0.179\n",
      "    training_iteration_time_ms: 3610.337\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1657050572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2920\n",
      "  training_iteration: 365\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2936\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2936\n",
      "    num_agent_steps_trained: 2936\n",
      "    num_env_steps_sampled: 2936\n",
      "    num_env_steps_trained: 2936\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.040870975041069214\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 978\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3772214651107788\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2378347491903696e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.36363154649734497\n",
      "          total_loss: 0.3648739457130432\n",
      "          vf_explained_var: 0.11835417151451111\n",
      "          vf_loss: 0.001242441008798778\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2936\n",
      "    num_agent_steps_trained: 2936\n",
      "    num_env_steps_sampled: 2936\n",
      "    num_env_steps_trained: 2936\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2936\n",
      "  num_agent_steps_trained: 2936\n",
      "  num_env_steps_sampled: 2936\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2936\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.88333333333333\n",
      "    ram_util_percent: 40.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14080900101624177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 726.9259502122752\n",
      "    mean_inference_ms: 1.17686738656354\n",
      "    mean_raw_obs_processing_ms: 1.6610609058523003\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.040870975041069214\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.1340011470557272\n",
      "      - 0.35287014444939047\n",
      "      - -1.376732946402108\n",
      "      - -38.45515890975497\n",
      "      - 0.010778139978341206\n",
      "      - 0.04768720873370924\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "      - -1.3041000294869252\n",
      "      - 1.325166096168275\n",
      "      - -0.01754093513736177\n",
      "      - 0.06452423208333857\n",
      "      - 0.059297212106966146\n",
      "      - 35.20839059044639\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14080900101624177\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 726.9259502122752\n",
      "      mean_inference_ms: 1.17686738656354\n",
      "      mean_raw_obs_processing_ms: 1.6610609058523003\n",
      "  time_since_restore: 1444.0343668460846\n",
      "  time_this_iter_s: 3.668341636657715\n",
      "  time_total_s: 1444.0343668460846\n",
      "  timers:\n",
      "    learn_throughput: 170.762\n",
      "    learn_time_ms: 46.849\n",
      "    load_throughput: 46487.16\n",
      "    load_time_ms: 0.172\n",
      "    training_iteration_time_ms: 3605.295\n",
      "    update_time_ms: 2.319\n",
      "  timestamp: 1657050579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2936\n",
      "  training_iteration: 367\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:39 (running for 00:24:34.89)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         1444.03</td><td style=\"text-align: right;\">2936</td><td style=\"text-align: right;\">-0.040871</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2952\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2952\n",
      "    num_agent_steps_trained: 2952\n",
      "    num_env_steps_sampled: 2952\n",
      "    num_env_steps_trained: 2952\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.014580134197786945\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 984\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3698456287384033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.0236476504796883e-06\n",
      "          model: {}\n",
      "          policy_loss: -0.36420345306396484\n",
      "          total_loss: 0.22489997744560242\n",
      "          vf_explained_var: 0.007012289948761463\n",
      "          vf_loss: 0.5891034007072449\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2952\n",
      "    num_agent_steps_trained: 2952\n",
      "    num_env_steps_sampled: 2952\n",
      "    num_env_steps_trained: 2952\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2952\n",
      "  num_agent_steps_trained: 2952\n",
      "  num_env_steps_sampled: 2952\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2952\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.939999999999998\n",
      "    ram_util_percent: 39.980000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1408582081751033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.4101748181108\n",
      "    mean_inference_ms: 1.1771217944374839\n",
      "    mean_raw_obs_processing_ms: 1.6680982777616509\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.014580134197786945\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.01470069568751442\n",
      "      - 1.3800872916056575\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "      - -1.3041000294869252\n",
      "      - 1.325166096168275\n",
      "      - -0.01754093513736177\n",
      "      - 0.06452423208333857\n",
      "      - 0.059297212106966146\n",
      "      - 35.20839059044639\n",
      "      - -0.019912469859647786\n",
      "      - -35.21464682397455\n",
      "      - -1.3738192078291143\n",
      "      - 0.007595184053263537\n",
      "      - -0.04865296300833288\n",
      "      - -0.00803485099329282\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1408582081751033\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.4101748181108\n",
      "      mean_inference_ms: 1.1771217944374839\n",
      "      mean_raw_obs_processing_ms: 1.6680982777616509\n",
      "  time_since_restore: 1450.9987738132477\n",
      "  time_this_iter_s: 3.2808640003204346\n",
      "  time_total_s: 1450.9987738132477\n",
      "  timers:\n",
      "    learn_throughput: 170.887\n",
      "    learn_time_ms: 46.815\n",
      "    load_throughput: 44924.932\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3632.903\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1657050586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2952\n",
      "  training_iteration: 369\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:46 (running for 00:24:41.93)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">            1451</td><td style=\"text-align: right;\">2952</td><td style=\"text-align: right;\">-0.0145801</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:51 (running for 00:24:46.94)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">            1451</td><td style=\"text-align: right;\">2952</td><td style=\"text-align: right;\">-0.0145801</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2960\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2960\n",
      "    num_agent_steps_trained: 2960\n",
      "    num_env_steps_sampled: 2960\n",
      "    num_env_steps_trained: 2960\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-52\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.027727020439700496\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 986\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -20.699643907845267\n",
      "    episode_reward_mean: -20.699643907845267\n",
      "    episode_reward_min: -20.699643907845267\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -20.699643907845267\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14291108990998544\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 844.5158389651721\n",
      "      mean_inference_ms: 1.4165303097712083\n",
      "      mean_raw_obs_processing_ms: 0.6680659649083432\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3714823722839355\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6193815781662124e-06\n",
      "          model: {}\n",
      "          policy_loss: 0.35094013810157776\n",
      "          total_loss: 0.7235927581787109\n",
      "          vf_explained_var: 0.0505913570523262\n",
      "          vf_loss: 0.37265264987945557\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2960\n",
      "    num_agent_steps_trained: 2960\n",
      "    num_env_steps_sampled: 2960\n",
      "    num_env_steps_trained: 2960\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2960\n",
      "  num_agent_steps_trained: 2960\n",
      "  num_env_steps_sampled: 2960\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2960\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1375\n",
      "    ram_util_percent: 40.0\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14080678586425907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.0392802500106\n",
      "    mean_inference_ms: 1.1768346968648304\n",
      "    mean_raw_obs_processing_ms: 1.658892164828019\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.027727020439700496\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -0.04761395755567133\n",
      "      - -1.3978640630179537\n",
      "      - -1.328472472751074\n",
      "      - 1.3249214889011935\n",
      "      - -0.036384650175767086\n",
      "      - -1.338048841712956\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "      - -1.3041000294869252\n",
      "      - 1.325166096168275\n",
      "      - -0.01754093513736177\n",
      "      - 0.06452423208333857\n",
      "      - 0.059297212106966146\n",
      "      - 35.20839059044639\n",
      "      - -0.019912469859647786\n",
      "      - -35.21464682397455\n",
      "      - -1.3738192078291143\n",
      "      - 0.007595184053263537\n",
      "      - -0.04865296300833288\n",
      "      - -0.00803485099329282\n",
      "      - 0.042419820071677994\n",
      "      - 0.008278151655125576\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14080678586425907\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.0392802500106\n",
      "      mean_inference_ms: 1.1768346968648304\n",
      "      mean_raw_obs_processing_ms: 1.658892164828019\n",
      "  time_since_restore: 1456.7581856250763\n",
      "  time_this_iter_s: 5.759411811828613\n",
      "  time_total_s: 1456.7581856250763\n",
      "  timers:\n",
      "    learn_throughput: 168.175\n",
      "    learn_time_ms: 47.569\n",
      "    load_throughput: 44864.864\n",
      "    load_time_ms: 0.178\n",
      "    training_iteration_time_ms: 3632.827\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1657050592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2960\n",
      "  training_iteration: 370\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n",
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2976\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2976\n",
      "    num_agent_steps_trained: 2976\n",
      "    num_env_steps_sampled: 2976\n",
      "    num_env_steps_trained: 2976\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: 0.0010727648945192203\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 992\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3659281730651855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2172226888651494e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.4186999201774597\n",
      "          total_loss: 0.4226571321487427\n",
      "          vf_explained_var: -0.9964399933815002\n",
      "          vf_loss: 0.0039572035893797874\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2976\n",
      "    num_agent_steps_trained: 2976\n",
      "    num_env_steps_sampled: 2976\n",
      "    num_env_steps_trained: 2976\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2976\n",
      "  num_agent_steps_trained: 2976\n",
      "  num_env_steps_sampled: 2976\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2976\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.46\n",
      "    ram_util_percent: 39.980000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14084972741998247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.4709095391571\n",
      "    mean_inference_ms: 1.1770262912070693\n",
      "    mean_raw_obs_processing_ms: 1.6658819934911961\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: 0.0010727648945192203\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.03386743196539144\n",
      "      - 32.85422156238313\n",
      "      - 1.3347192658888019\n",
      "      - 0.10908297816237456\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "      - -1.3041000294869252\n",
      "      - 1.325166096168275\n",
      "      - -0.01754093513736177\n",
      "      - 0.06452423208333857\n",
      "      - 0.059297212106966146\n",
      "      - 35.20839059044639\n",
      "      - -0.019912469859647786\n",
      "      - -35.21464682397455\n",
      "      - -1.3738192078291143\n",
      "      - 0.007595184053263537\n",
      "      - -0.04865296300833288\n",
      "      - -0.00803485099329282\n",
      "      - 0.042419820071677994\n",
      "      - 0.008278151655125576\n",
      "      - 32.90745322353588\n",
      "      - -0.0037428876563662117\n",
      "      - -32.909013010973844\n",
      "      - 0.0003420369270911161\n",
      "      - -0.003528109182636574\n",
      "      - 0.06500478445961311\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14084972741998247\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.4709095391571\n",
      "      mean_inference_ms: 1.1770262912070693\n",
      "      mean_raw_obs_processing_ms: 1.6658819934911961\n",
      "  time_since_restore: 1463.3250877857208\n",
      "  time_this_iter_s: 3.3351736068725586\n",
      "  time_total_s: 1463.3250877857208\n",
      "  timers:\n",
      "    learn_throughput: 168.689\n",
      "    learn_time_ms: 47.424\n",
      "    load_throughput: 47928.056\n",
      "    load_time_ms: 0.167\n",
      "    training_iteration_time_ms: 3610.81\n",
      "    update_time_ms: 2.313\n",
      "  timestamp: 1657050599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2976\n",
      "  training_iteration: 372\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:49:59 (running for 00:24:54.37)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         1463.33</td><td style=\"text-align: right;\">2976</td><td style=\"text-align: right;\">0.00107276</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 2992\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 2992\n",
      "    num_agent_steps_trained: 2992\n",
      "    num_env_steps_sampled: 2992\n",
      "    num_env_steps_trained: 2992\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-50-05\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: -0.3294347366740775\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 996\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.379133939743042\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2190264897071756e-05\n",
      "          model: {}\n",
      "          policy_loss: -0.2349664568901062\n",
      "          total_loss: 0.11464598774909973\n",
      "          vf_explained_var: 0.04200105741620064\n",
      "          vf_loss: 0.34961244463920593\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 2992\n",
      "    num_agent_steps_trained: 2992\n",
      "    num_env_steps_sampled: 2992\n",
      "    num_env_steps_trained: 2992\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 2992\n",
      "  num_agent_steps_trained: 2992\n",
      "  num_env_steps_sampled: 2992\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 2992\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.18\n",
      "    ram_util_percent: 39.980000000000004\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14084875776852315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.4772929468319\n",
      "    mean_inference_ms: 1.1769835244832398\n",
      "    mean_raw_obs_processing_ms: 1.6648144125827338\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: -0.3294347366740775\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 2\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.4762788380705345\n",
      "      - -0.03798750964874387\n",
      "      - -5.052160368808888\n",
      "      - 0.0613107317187489\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "      - -1.3041000294869252\n",
      "      - 1.325166096168275\n",
      "      - -0.01754093513736177\n",
      "      - 0.06452423208333857\n",
      "      - 0.059297212106966146\n",
      "      - 35.20839059044639\n",
      "      - -0.019912469859647786\n",
      "      - -35.21464682397455\n",
      "      - -1.3738192078291143\n",
      "      - 0.007595184053263537\n",
      "      - -0.04865296300833288\n",
      "      - -0.00803485099329282\n",
      "      - 0.042419820071677994\n",
      "      - 0.008278151655125576\n",
      "      - 32.90745322353588\n",
      "      - -0.0037428876563662117\n",
      "      - -32.909013010973844\n",
      "      - 0.0003420369270911161\n",
      "      - -0.003528109182636574\n",
      "      - 0.06500478445961311\n",
      "      - -0.00037797485265378405\n",
      "      - 34.42589103334861\n",
      "      - 1.2898310335776533\n",
      "      - -34.434203010533594\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14084875776852315\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.4772929468319\n",
      "      mean_inference_ms: 1.1769835244832398\n",
      "      mean_raw_obs_processing_ms: 1.6648144125827338\n",
      "  time_since_restore: 1469.983618736267\n",
      "  time_this_iter_s: 3.825957775115967\n",
      "  time_total_s: 1469.983618736267\n",
      "  timers:\n",
      "    learn_throughput: 168.452\n",
      "    learn_time_ms: 47.491\n",
      "    load_throughput: 46831.029\n",
      "    load_time_ms: 0.171\n",
      "    training_iteration_time_ms: 3512.244\n",
      "    update_time_ms: 2.289\n",
      "  timestamp: 1657050605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2992\n",
      "  training_iteration: 374\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:50:06 (running for 00:25:01.15)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         1469.98</td><td style=\"text-align: right;\">2992</td><td style=\"text-align: right;\">-0.329435</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:50:11 (running for 00:25:06.15)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>RUNNING </td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         1469.98</td><td style=\"text-align: right;\">2992</td><td style=\"text-align: right;\">-0.329435</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_compiler_gym_2c379_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-05_15-50-12\n",
      "  done: true\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.58496904304526\n",
      "  episode_reward_mean: 0.0710466199061537\n",
      "  episode_reward_min: -38.49390607994264\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1000\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.02068836848704647\n",
      "    episode_reward_mean: 0.02068836848704647\n",
      "    episode_reward_min: 0.02068836848704647\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.02068836848704647\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1427589264591183\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 846.4168985333063\n",
      "      mean_inference_ms: 1.4121574638164147\n",
      "      mean_raw_obs_processing_ms: 0.6698950202064177\n",
      "    timesteps_this_iter: 3\n",
      "  experiment_id: 1ca6e38840c340479addb8dd7cbbc3be\n",
      "  hostname: codah\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3674664497375488\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.212892857438419e-05\n",
      "          model: {}\n",
      "          policy_loss: 0.2160187065601349\n",
      "          total_loss: 4.217809200286865\n",
      "          vf_explained_var: -0.0014845529804006219\n",
      "          vf_loss: 4.001791477203369\n",
      "        num_agent_steps_trained: 5.0\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_env_steps_sampled: 3000\n",
      "    num_env_steps_trained: 3000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 100.37.253.28\n",
      "  num_agent_steps_sampled: 3000\n",
      "  num_agent_steps_trained: 3000\n",
      "  num_env_steps_sampled: 3000\n",
      "  num_env_steps_sampled_this_iter: 8\n",
      "  num_env_steps_trained: 3000\n",
      "  num_env_steps_trained_this_iter: 8\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.633333333333335\n",
      "    ram_util_percent: 39.98888888888889\n",
      "  pid: 560340\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1408521375872397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 727.4987701746368\n",
      "    mean_inference_ms: 1.1769565865631826\n",
      "    mean_raw_obs_processing_ms: 1.6637760301615356\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 3.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 39.58496904304526\n",
      "    episode_reward_mean: 0.0710466199061537\n",
      "    episode_reward_min: -38.49390607994264\n",
      "    episodes_this_iter: 4\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - -28.274448889884017\n",
      "      - 28.274448889884017\n",
      "      - -0.12771379802995364\n",
      "      - 0.04431219533427644\n",
      "      - -28.277712310474023\n",
      "      - -1.3781782364006596\n",
      "      - 0.015524995713031453\n",
      "      - 0.05630916717111345\n",
      "      - -0.01654672436606841\n",
      "      - 0.002577610008163256\n",
      "      - 1.2917698419202885\n",
      "      - -1.328132964981123\n",
      "      - -0.04696392891533452\n",
      "      - 1.3904813045599158\n",
      "      - 32.69343079118246\n",
      "      - 38.395293382185145\n",
      "      - -32.65710827499505\n",
      "      - 0.027126964197371817\n",
      "      - -38.49390607994264\n",
      "      - 0.034536369652189425\n",
      "      - 32.816735138739865\n",
      "      - -0.010983453898576467\n",
      "      - -0.13086625437974675\n",
      "      - 38.52516388071549\n",
      "      - -0.11132383788342537\n",
      "      - -4.3847408027994454\n",
      "      - -11.760707572221907\n",
      "      - 4.298588195213714\n",
      "      - 0.9360005491829995\n",
      "      - 7.244149785355667\n",
      "      - -0.2213983525183778\n",
      "      - -7.835066855813352\n",
      "      - -27.586535797431562\n",
      "      - 0.042527914552278556\n",
      "      - 0.44564484527435866\n",
      "      - -4.919072239503819\n",
      "      - 25.422814161880375\n",
      "      - -27.275928755615304\n",
      "      - 2.2536795490951107\n",
      "      - -0.03816759730790664\n",
      "      - -2.590439719155107\n",
      "      - -0.4243332663589996\n",
      "      - 0.0015459450616269743\n",
      "      - 0.055678660206795905\n",
      "      - 6.775819658812651\n",
      "      - -0.017237081262770793\n",
      "      - -32.82134387270565\n",
      "      - -0.006197939474312908\n",
      "      - -0.00019445747749824438\n",
      "      - 0.025820003983741402\n",
      "      - 0.004219810849593886\n",
      "      - 39.58496904304526\n",
      "      - -0.015037665104721043\n",
      "      - -5.2947688850044585\n",
      "      - -0.034239910582481214\n",
      "      - 5.8994207239085625\n",
      "      - -0.015854989575401324\n",
      "      - 0.05980638991158882\n",
      "      - -8.422472031462227\n",
      "      - -31.771009599450707\n",
      "      - -0.008252066444377704\n",
      "      - 0.008552447570821187\n",
      "      - -0.050881143987170474\n",
      "      - -0.06257720513536358\n",
      "      - 0.039444725377922096\n",
      "      - 0.01626575398401464\n",
      "      - 3.164408953493236e-05\n",
      "      - 0.010960887133441455\n",
      "      - -0.06459164736252154\n",
      "      - 1.313954141184476\n",
      "      - 1.3188812440739646\n",
      "      - -1.309088886790704\n",
      "      - -1.3041000294869252\n",
      "      - 1.325166096168275\n",
      "      - -0.01754093513736177\n",
      "      - 0.06452423208333857\n",
      "      - 0.059297212106966146\n",
      "      - 35.20839059044639\n",
      "      - -0.019912469859647786\n",
      "      - -35.21464682397455\n",
      "      - -1.3738192078291143\n",
      "      - 0.007595184053263537\n",
      "      - -0.04865296300833288\n",
      "      - -0.00803485099329282\n",
      "      - 0.042419820071677994\n",
      "      - 0.008278151655125576\n",
      "      - 32.90745322353588\n",
      "      - -0.0037428876563662117\n",
      "      - -32.909013010973844\n",
      "      - 0.0003420369270911161\n",
      "      - -0.003528109182636574\n",
      "      - 0.06500478445961311\n",
      "      - -0.00037797485265378405\n",
      "      - 34.42589103334861\n",
      "      - 1.2898310335776533\n",
      "      - -34.434203010533594\n",
      "      - 0.09162608477779477\n",
      "      - -0.07023841282739429\n",
      "      - 34.147943676566456\n",
      "      - 1.3262460008379051\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1408521375872397\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 727.4987701746368\n",
      "      mean_inference_ms: 1.1769565865631826\n",
      "      mean_raw_obs_processing_ms: 1.6637760301615356\n",
      "  time_since_restore: 1476.0843658447266\n",
      "  time_this_iter_s: 6.100747108459473\n",
      "  time_total_s: 1476.0843658447266\n",
      "  timers:\n",
      "    learn_throughput: 170.226\n",
      "    learn_time_ms: 46.996\n",
      "    load_throughput: 47067.516\n",
      "    load_time_ms: 0.17\n",
      "    training_iteration_time_ms: 3377.587\n",
      "    update_time_ms: 2.238\n",
      "  timestamp: 1657050612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 375\n",
      "  trial_id: 2c379_00000\n",
      "  warmup_time: 10.31270980834961\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-05 15:50:12 (running for 00:25:07.35)<br>Memory usage on this node: 12.3/30.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/13.27 GiB heap, 0.0/6.63 GiB objects<br>Result logdir: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_compiler_gym_2c379_00000</td><td>TERMINATED</td><td>100.37.253.28:560340</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         1476.08</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">0.0710466</td><td style=\"text-align: right;\">              39.585</td><td style=\"text-align: right;\">            -38.4939</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 15:50:12,878\tINFO tune.py:747 -- Total run time: 1508.04 seconds (1507.31 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    PPOTrainer,\n",
    "    fail_fast=True,\n",
    "    checkpoint_at_end=True,\n",
    "    # stop=TimeStopper(),\n",
    "    stop={\n",
    "        \"episodes_total\": 1000,\n",
    "        # \"episode_reward_mean\": 30\n",
    "    },\n",
    "    config=PPO_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = analysis.get_best_checkpoint(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    trial=analysis.trials[0]s\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>num_agent_steps_sampled</th>\n",
       "      <th>num_agent_steps_trained</th>\n",
       "      <th>num_env_steps_sampled</th>\n",
       "      <th>num_env_steps_trained</th>\n",
       "      <th>...</th>\n",
       "      <th>config/log_level</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/model</th>\n",
       "      <th>config/num_workers</th>\n",
       "      <th>config/rollout_fragment_length</th>\n",
       "      <th>config/seed</th>\n",
       "      <th>config/sgd_minibatch_size</th>\n",
       "      <th>config/soft_horizon</th>\n",
       "      <th>config/train_batch_size</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.584969</td>\n",
       "      <td>-38.493906</td>\n",
       "      <td>0.071047</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'fcnet_hiddens': [5, 5]}</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>/home/dejang/ray_results/PPOTrainer_2022-07-05...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0           39.584969          -38.493906             0.071047   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
       "0               3.0                   4                    2   \n",
       "\n",
       "   num_agent_steps_sampled  num_agent_steps_trained  num_env_steps_sampled  \\\n",
       "0                     3000                     3000                   3000   \n",
       "\n",
       "   num_env_steps_trained  ...  config/log_level  config/lr  \\\n",
       "0                   3000  ...             ERROR     0.0001   \n",
       "\n",
       "                config/model  config/num_workers  \\\n",
       "0  {'fcnet_hiddens': [5, 5]}                   2   \n",
       "\n",
       "   config/rollout_fragment_length  config/seed  config/sgd_minibatch_size  \\\n",
       "0                               5          204                          5   \n",
       "\n",
       "  config/soft_horizon config/train_batch_size  \\\n",
       "0                True                       5   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/dejang/ray_results/PPOTrainer_2022-07-05...  \n",
       "\n",
       "[1 rows x 90 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04/PPOTrainer_compiler_gym_2c379_00000_0_2022-07-05_15-25-05\n"
     ]
    }
   ],
   "source": [
    "trial = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "log_dir = analysis.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episodes_total': 1000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.stopping_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': {'max': 39.58496904304526,\n",
       "  'min': 0.016922388863661375,\n",
       "  'avg': 37.35342323484236,\n",
       "  'last': 39.58496904304526,\n",
       "  'last-5-avg': 39.58496904304526,\n",
       "  'last-10-avg': 39.584969043045255},\n",
       " 'episode_reward_min': {'max': 0.0034574530022073446,\n",
       "  'min': -39.82938783525944,\n",
       "  'avg': -37.717964688154716,\n",
       "  'last': -38.49390607994264,\n",
       "  'last-5-avg': -38.49390607994264,\n",
       "  'last-10-avg': -38.49390607994264},\n",
       " 'episode_reward_mean': {'max': 7.186053079470235,\n",
       "  'min': -0.6798548492744083,\n",
       "  'avg': 0.08435085538460046,\n",
       "  'last': 0.0710466199061537,\n",
       "  'last-5-avg': 0.014979303357484,\n",
       "  'last-10-avg': -0.07999296632692823},\n",
       " 'episode_len_mean': {'max': 3.0,\n",
       "  'min': 3.0,\n",
       "  'avg': 3.0000000000000004,\n",
       "  'last': 3.0,\n",
       "  'last-5-avg': 3.0,\n",
       "  'last-10-avg': 3.0},\n",
       " 'episodes_this_iter': {'max': 4,\n",
       "  'min': 2,\n",
       "  'avg': 2.6666666666666665,\n",
       "  'last': 4,\n",
       "  'last-5-avg': 2.8,\n",
       "  'last-10-avg': 2.8},\n",
       " 'num_healthy_workers': {'max': 2,\n",
       "  'min': 2,\n",
       "  'avg': 1.9999999999999962,\n",
       "  'last': 2,\n",
       "  'last-5-avg': 2.0,\n",
       "  'last-10-avg': 2.0},\n",
       " 'num_agent_steps_sampled': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'num_agent_steps_trained': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'num_env_steps_sampled': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'num_env_steps_trained': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'num_env_steps_sampled_this_iter': {'max': 8,\n",
       "  'min': 8,\n",
       "  'avg': 7.999999999999985,\n",
       "  'last': 8,\n",
       "  'last-5-avg': 8.0,\n",
       "  'last-10-avg': 8.0},\n",
       " 'num_env_steps_trained_this_iter': {'max': 8,\n",
       "  'min': 8,\n",
       "  'avg': 7.999999999999985,\n",
       "  'last': 8,\n",
       "  'last-5-avg': 8.0,\n",
       "  'last-10-avg': 8.0},\n",
       " 'timesteps_total': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'agent_timesteps_total': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'done': {'max': True,\n",
       "  'min': False,\n",
       "  'avg': 0.0026666666666666666,\n",
       "  'last': True,\n",
       "  'last-5-avg': 0.2,\n",
       "  'last-10-avg': 0.1},\n",
       " 'episodes_total': {'max': 1000,\n",
       "  'min': 2,\n",
       "  'avg': 500.66666666666663,\n",
       "  'last': 1000,\n",
       "  'last-5-avg': 994.0,\n",
       "  'last-10-avg': 987.4},\n",
       " 'training_iteration': {'max': 375,\n",
       "  'min': 1,\n",
       "  'avg': 188.00000000000003,\n",
       "  'last': 375,\n",
       "  'last-5-avg': 373.0,\n",
       "  'last-10-avg': 370.5},\n",
       " 'time_this_iter_s': {'max': 7.490169286727905,\n",
       "  'min': 2.1016476154327393,\n",
       "  'avg': 3.9362249755859415,\n",
       "  'last': 6.100747108459473,\n",
       "  'last-5-avg': 3.8652360439300537,\n",
       "  'last-10-avg': 3.8870333194732667},\n",
       " 'time_total_s': {'max': 1476.0843658447266,\n",
       "  'min': 4.42792820930481,\n",
       "  'avg': 744.3294534244537,\n",
       "  'last': 1476.0843658447266,\n",
       "  'last-5-avg': 1467.1081295013428,\n",
       "  'last-10-avg': 1457.5415908813477},\n",
       " 'time_since_restore': {'max': 1476.0843658447266,\n",
       "  'min': 4.42792820930481,\n",
       "  'avg': 744.3294534244537,\n",
       "  'last': 1476.0843658447266,\n",
       "  'last-5-avg': 1467.1081295013428,\n",
       "  'last-10-avg': 1457.5415908813477},\n",
       " 'timesteps_since_restore': {'max': 0,\n",
       "  'min': 0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0},\n",
       " 'iterations_since_restore': {'max': 375,\n",
       "  'min': 1,\n",
       "  'avg': 188.00000000000003,\n",
       "  'last': 375,\n",
       "  'last-5-avg': 373.0,\n",
       "  'last-10-avg': 370.5},\n",
       " 'warmup_time': {'max': 10.31270980834961,\n",
       "  'min': 10.31270980834961,\n",
       "  'avg': 10.312709808349606,\n",
       "  'last': 10.31270980834961,\n",
       "  'last-5-avg': 10.31270980834961,\n",
       "  'last-10-avg': 10.31270980834961},\n",
       " 'info/num_env_steps_sampled': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'info/num_env_steps_trained': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'info/num_agent_steps_sampled': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'info/num_agent_steps_trained': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'sampler_results/episode_reward_max': {'max': 39.58496904304526,\n",
       "  'min': 0.016922388863661375,\n",
       "  'avg': 37.35342323484236,\n",
       "  'last': 39.58496904304526,\n",
       "  'last-5-avg': 39.58496904304526,\n",
       "  'last-10-avg': 39.584969043045255},\n",
       " 'sampler_results/episode_reward_min': {'max': 0.0034574530022073446,\n",
       "  'min': -39.82938783525944,\n",
       "  'avg': -37.717964688154716,\n",
       "  'last': -38.49390607994264,\n",
       "  'last-5-avg': -38.49390607994264,\n",
       "  'last-10-avg': -38.49390607994264},\n",
       " 'sampler_results/episode_reward_mean': {'max': 7.186053079470235,\n",
       "  'min': -0.6798548492744083,\n",
       "  'avg': 0.08435085538460046,\n",
       "  'last': 0.0710466199061537,\n",
       "  'last-5-avg': 0.014979303357484,\n",
       "  'last-10-avg': -0.07999296632692823},\n",
       " 'sampler_results/episode_len_mean': {'max': 3.0,\n",
       "  'min': 3.0,\n",
       "  'avg': 3.0000000000000004,\n",
       "  'last': 3.0,\n",
       "  'last-5-avg': 3.0,\n",
       "  'last-10-avg': 3.0},\n",
       " 'sampler_results/episodes_this_iter': {'max': 4,\n",
       "  'min': 2,\n",
       "  'avg': 2.6666666666666665,\n",
       "  'last': 4,\n",
       "  'last-5-avg': 2.8,\n",
       "  'last-10-avg': 2.8},\n",
       " 'sampler_perf/mean_raw_obs_processing_ms': {'max': 15.568337565374733,\n",
       "  'min': 1.1372354295518663,\n",
       "  'avg': 3.3898867180051973,\n",
       "  'last': 1.6637760301615356,\n",
       "  'last-5-avg': 1.663645332968544,\n",
       "  'last-10-avg': 1.6645854036570937},\n",
       " 'sampler_perf/mean_inference_ms': {'max': 5.604004859924316,\n",
       "  'min': 1.1767428474956594,\n",
       "  'avg': 1.3353771587102004,\n",
       "  'last': 1.1769565865631826,\n",
       "  'last-5-avg': 1.1769552009096675,\n",
       "  'last-10-avg': 1.176986338520111},\n",
       " 'sampler_perf/mean_action_processing_ms': {'max': 0.1645803451538086,\n",
       "  'min': 0.14079944377531242,\n",
       "  'avg': 0.14388504828387239,\n",
       "  'last': 0.1408521375872397,\n",
       "  'last-5-avg': 0.14084066073781104,\n",
       "  'last-10-avg': 0.14083954905563983},\n",
       " 'sampler_perf/mean_env_wait_ms': {'max': 743.5877197967773,\n",
       "  'min': 701.4995519931499,\n",
       "  'avg': 727.1063702951581,\n",
       "  'last': 727.4987701746368,\n",
       "  'last-5-avg': 727.3965868071524,\n",
       "  'last-10-avg': 727.30054450305},\n",
       " 'sampler_perf/mean_env_render_ms': {'max': 0.0,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0},\n",
       " 'timers/training_iteration_time_ms': {'max': 4424.835,\n",
       "  'min': 2722.842,\n",
       "  'avg': 3426.128842666665,\n",
       "  'last': 3377.587,\n",
       "  'last-5-avg': 3530.9346000000005,\n",
       "  'last-10-avg': 3579.1040000000003},\n",
       " 'timers/load_time_ms': {'max': 0.26,\n",
       "  'min': 0.154,\n",
       "  'avg': 0.188336,\n",
       "  'last': 0.17,\n",
       "  'last-5-avg': 0.17040000000000002,\n",
       "  'last-10-avg': 0.17339999999999997},\n",
       " 'timers/load_throughput': {'max': 52030.442,\n",
       "  'min': 30766.947,\n",
       "  'avg': 42801.89345066665,\n",
       "  'last': 47067.516,\n",
       "  'last-5-avg': 47001.8818,\n",
       "  'last-10-avg': 46170.757},\n",
       " 'timers/learn_time_ms': {'max': 388.894,\n",
       "  'min': 45.303,\n",
       "  'avg': 50.50051466666658,\n",
       "  'last': 46.996,\n",
       "  'last-5-avg': 47.3946,\n",
       "  'last-10-avg': 47.260299999999994},\n",
       " 'timers/learn_throughput': {'max': 176.589,\n",
       "  'min': 20.571,\n",
       "  'avg': 164.49651733333346,\n",
       "  'last': 170.226,\n",
       "  'last-5-avg': 168.7984,\n",
       "  'last-10-avg': 169.2819},\n",
       " 'timers/update_time_ms': {'max': 2.801,\n",
       "  'min': 1.947,\n",
       "  'avg': 2.2434933333333347,\n",
       "  'last': 2.238,\n",
       "  'last-5-avg': 2.2874,\n",
       "  'last-10-avg': 2.3032999999999997},\n",
       " 'counters/num_env_steps_sampled': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'counters/num_env_steps_trained': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'counters/num_agent_steps_sampled': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'counters/num_agent_steps_trained': {'max': 3000,\n",
       "  'min': 8,\n",
       "  'avg': 1504.0000000000002,\n",
       "  'last': 3000,\n",
       "  'last-5-avg': 2984.0,\n",
       "  'last-10-avg': 2964.0},\n",
       " 'perf/cpu_util_percent': {'max': 51.4,\n",
       "  'min': 13.45,\n",
       "  'avg': 20.35610656084655,\n",
       "  'last': 15.633333333333335,\n",
       "  'last-5-avg': 17.44466666666667,\n",
       "  'last-10-avg': 17.58291666666667},\n",
       " 'perf/ram_util_percent': {'max': 43.0,\n",
       "  'min': 39.4,\n",
       "  'avg': 40.296078095238094,\n",
       "  'last': 39.98888888888889,\n",
       "  'last-5-avg': 39.98577777777778,\n",
       "  'last-10-avg': 39.98688888888889},\n",
       " 'sampler_results/sampler_perf/mean_raw_obs_processing_ms': {'max': 15.568337565374733,\n",
       "  'min': 1.1372354295518663,\n",
       "  'avg': 3.3898867180051973,\n",
       "  'last': 1.6637760301615356,\n",
       "  'last-5-avg': 1.663645332968544,\n",
       "  'last-10-avg': 1.6645854036570937},\n",
       " 'sampler_results/sampler_perf/mean_inference_ms': {'max': 5.604004859924316,\n",
       "  'min': 1.1767428474956594,\n",
       "  'avg': 1.3353771587102004,\n",
       "  'last': 1.1769565865631826,\n",
       "  'last-5-avg': 1.1769552009096675,\n",
       "  'last-10-avg': 1.176986338520111},\n",
       " 'sampler_results/sampler_perf/mean_action_processing_ms': {'max': 0.1645803451538086,\n",
       "  'min': 0.14079944377531242,\n",
       "  'avg': 0.14388504828387239,\n",
       "  'last': 0.1408521375872397,\n",
       "  'last-5-avg': 0.14084066073781104,\n",
       "  'last-10-avg': 0.14083954905563983},\n",
       " 'sampler_results/sampler_perf/mean_env_wait_ms': {'max': 743.5877197967773,\n",
       "  'min': 701.4995519931499,\n",
       "  'avg': 727.1063702951581,\n",
       "  'last': 727.4987701746368,\n",
       "  'last-5-avg': 727.3965868071524,\n",
       "  'last-10-avg': 727.30054450305},\n",
       " 'sampler_results/sampler_perf/mean_env_render_ms': {'max': 0.0,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0},\n",
       " 'info/learner/default_policy/num_agent_steps_trained': {'max': 5.0,\n",
       "  'min': 5.0,\n",
       "  'avg': 5.0,\n",
       "  'last': 5.0,\n",
       "  'last-5-avg': 5.0,\n",
       "  'last-10-avg': 5.0},\n",
       " 'info/learner/default_policy/learner_stats/cur_kl_coeff': {'max': 0.20000000298023224,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0010666666825612378,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0},\n",
       " 'info/learner/default_policy/learner_stats/cur_lr': {'max': 9.999999747378752e-05,\n",
       "  'min': 9.999999747378752e-05,\n",
       "  'avg': 9.999999747378753e-05,\n",
       "  'last': 9.999999747378752e-05,\n",
       "  'last-5-avg': 9.999999747378752e-05,\n",
       "  'last-10-avg': 9.999999747378752e-05},\n",
       " 'info/learner/default_policy/learner_stats/total_loss': {'max': 10.435163,\n",
       "  'min': -0.6289432,\n",
       "  'avg': 2.8818352914042737,\n",
       "  'last': 4.217809,\n",
       "  'last-5-avg': 2.9808701813220977,\n",
       "  'last-10-avg': 1.9407112419605255},\n",
       " 'info/learner/default_policy/learner_stats/policy_loss': {'max': 0.75160986,\n",
       "  'min': -0.6299847,\n",
       "  'avg': -0.0027191900306691654,\n",
       "  'last': 0.2160187,\n",
       "  'last-5-avg': 0.08992252945899963,\n",
       "  'last-10-avg': 0.10312783187255263},\n",
       " 'info/learner/default_policy/learner_stats/vf_loss': {'max': 10.0,\n",
       "  'min': 9.656266e-05,\n",
       "  'avg': 2.8845544752196686,\n",
       "  'last': 4.0017915,\n",
       "  'last-5-avg': 2.890947989933193,\n",
       "  'last-10-avg': 1.8375836018356495},\n",
       " 'info/learner/default_policy/learner_stats/vf_explained_var': {'max': 0.7118089,\n",
       "  'min': -1.0,\n",
       "  'avg': -0.044632918251225805,\n",
       "  'last': -0.001484553,\n",
       "  'last-5-avg': -0.1909868337912485,\n",
       "  'last-10-avg': -0.07643933827348519},\n",
       " 'info/learner/default_policy/learner_stats/kl': {'max': 0.00010130159,\n",
       "  'min': 4.7803127e-07,\n",
       "  'avg': 1.0297173803640662e-05,\n",
       "  'last': 1.2128929e-05,\n",
       "  'last-5-avg': 1.7193452003994024e-05,\n",
       "  'last-10-avg': 1.267292933562203e-05},\n",
       " 'info/learner/default_policy/learner_stats/entropy': {'max': 1.386293,\n",
       "  'min': 1.3552152,\n",
       "  'avg': 1.3814027684529597,\n",
       "  'last': 1.3674664,\n",
       "  'last-5-avg': 1.3714501142501831,\n",
       "  'last-10-avg': 1.3735279440879822},\n",
       " 'info/learner/default_policy/learner_stats/entropy_coeff': {'max': 0.0,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0},\n",
       " 'evaluation/episode_reward_max': {'max': 36.45620516916407,\n",
       "  'min': -36.204312325504254,\n",
       "  'avg': -0.5835721941816135,\n",
       "  'last': 0.02068836848704647,\n",
       "  'last-5-avg': -4.998803393080873,\n",
       "  'last-10-avg': -3.4778530122479667},\n",
       " 'evaluation/episode_reward_min': {'max': 36.45620516916407,\n",
       "  'min': -36.204312325504254,\n",
       "  'avg': -0.5835721941816135,\n",
       "  'last': 0.02068836848704647,\n",
       "  'last-5-avg': -4.998803393080873,\n",
       "  'last-10-avg': -3.4778530122479667},\n",
       " 'evaluation/episode_reward_mean': {'max': 36.45620516916407,\n",
       "  'min': -36.204312325504254,\n",
       "  'avg': -0.5835721941816135,\n",
       "  'last': 0.02068836848704647,\n",
       "  'last-5-avg': -4.998803393080873,\n",
       "  'last-10-avg': -3.4778530122479667},\n",
       " 'evaluation/episode_len_mean': {'max': 3.0,\n",
       "  'min': 3.0,\n",
       "  'avg': 3.0000000000000004,\n",
       "  'last': 3.0,\n",
       "  'last-5-avg': 3.0,\n",
       "  'last-10-avg': 3.0},\n",
       " 'evaluation/episodes_this_iter': {'max': 1,\n",
       "  'min': 1,\n",
       "  'avg': 1.0,\n",
       "  'last': 1,\n",
       "  'last-5-avg': 1.0,\n",
       "  'last-10-avg': 1.0},\n",
       " 'evaluation/timesteps_this_iter': {'max': 3,\n",
       "  'min': 3,\n",
       "  'avg': 3.0000000000000004,\n",
       "  'last': 3,\n",
       "  'last-5-avg': 3.0,\n",
       "  'last-10-avg': 3.0},\n",
       " 'evaluation/sampler_perf/mean_raw_obs_processing_ms': {'max': 0.8798837661743164,\n",
       "  'min': 0.6609730098558508,\n",
       "  'avg': 0.7763125199868013,\n",
       "  'last': 0.6698950202064177,\n",
       "  'last-5-avg': 0.6685551747930294,\n",
       "  'last-10-avg': 0.6674458895607891},\n",
       " 'evaluation/sampler_perf/mean_inference_ms': {'max': 10.521352291107178,\n",
       "  'min': 1.4121574638164147,\n",
       "  'avg': 6.255287382440282,\n",
       "  'last': 1.4121574638164147,\n",
       "  'last-5-avg': 1.418654385806238,\n",
       "  'last-10-avg': 1.4226209090028339},\n",
       " 'evaluation/sampler_perf/mean_action_processing_ms': {'max': 0.18727779388427734,\n",
       "  'min': 0.14053181530957554,\n",
       "  'avg': 0.166180715288759,\n",
       "  'last': 0.1427589264591183,\n",
       "  'last-5-avg': 0.14276529561523535,\n",
       "  'last-10-avg': 0.14235679816464628},\n",
       " 'evaluation/sampler_perf/mean_env_wait_ms': {'max': 905.4532701318914,\n",
       "  'min': 698.5506415367126,\n",
       "  'avg': 784.3136514353153,\n",
       "  'last': 846.4168985333063,\n",
       "  'last-5-avg': 848.6957530578741,\n",
       "  'last-10-avg': 854.5359663065195},\n",
       " 'evaluation/sampler_perf/mean_env_render_ms': {'max': 0.0,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.metric_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04/PPOTrainer_compiler_gym_2c379_00000_0_2022-07-05_15-25-05/checkpoint_000375/checkpoint-375'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.checkpoint.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 15:50:13,696\tWARNING deprecation.py:46 -- DeprecationWarning: `ray.rllib.agents.ppo.ppo.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.agents.ppo.ppo.PPOConfig(...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "ppo.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04/PPOTrainer_compiler_gym_2c379_00000_0_2022-07-05_15-25-05/checkpoint_000375/checkpoint-375'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.checkpoint.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_level': 'ERROR',\n",
       " 'seed': 204,\n",
       " 'num_workers': 2,\n",
       " 'env': 'compiler_gym',\n",
       " 'rollout_fragment_length': 5,\n",
       " 'train_batch_size': 5,\n",
       " 'sgd_minibatch_size': 5,\n",
       " 'gamma': 0.8,\n",
       " 'lr': 0.0001,\n",
       " 'horizon': 3,\n",
       " 'soft_horizon': True,\n",
       " 'evaluation_interval': 5,\n",
       " 'evaluation_num_episodes': 1,\n",
       " 'model': {'fcnet_hiddens': [5, 5]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/dejang/ray_results/PPOTrainer_compiler_gym_2022-07-05_15-50-14w1lcst39\n",
      "2022-07-05 15:50:14,025\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-07-05 15:50:14,027\tWARNING deprecation.py:46 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "2022-07-05 15:50:14,028\tWARNING ppo.py:386 -- `train_batch_size` (5) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=5)! Auto-adjusting `rollout_fragment_length` to 2.\n",
      "2022-07-05 15:50:14,030\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-07-05 15:50:14,031\tINFO trainer.py:903 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654916)\u001b[0m E0705 15:50:19.217818 140569179321920 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654916)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654916)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T155018-182728-0236\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654916)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654915)\u001b[0m E0705 15:50:19.182927 139973638477376 example_service.py:263] CRITICAL - \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654915)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654915)\u001b[0m Working_dir = /dev/shm/compiler_gym_dejang/s/0705T155018-143285-6f35\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=654915)\u001b[0m \n",
      "E0705 15:50:22.649786 140127627957824 example_service.py:263] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0705T155021-585428-659b\n",
      "\n",
      "2022-07-05 15:50:24,694\tINFO trainable.py:159 -- Trainable.setup took 10.672 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-05 15:50:24,696\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-07-05 15:50:24,748\tINFO trainable.py:588 -- Restored on 100.37.253.28 from checkpoint: /home/dejang/ray_results/PPOTrainer_2022-07-05_15-25-04/PPOTrainer_compiler_gym_2c379_00000_0_2022-07-05_15-25-05/checkpoint_000375/checkpoint-375\n",
      "2022-07-05 15:50:24,750\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 375, '_timesteps_total': None, '_time_total': 1476.0843658447266, '_episodes_total': 1000}\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(config=trial.config, env=\"compiler_gym\")\n",
    "trainer.restore(trial.checkpoint.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_policy().model\n",
    "policy = trainer.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_use_default_native_models': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " 'fcnet_hiddens': [5, 5],\n",
       " 'fcnet_activation': 'tanh',\n",
       " 'conv_filters': None,\n",
       " 'conv_activation': 'relu',\n",
       " 'post_fcnet_hiddens': [],\n",
       " 'post_fcnet_activation': 'relu',\n",
       " 'free_log_std': False,\n",
       " 'no_final_linear': False,\n",
       " 'vf_share_layers': False,\n",
       " 'use_lstm': False,\n",
       " 'max_seq_len': 20,\n",
       " 'lstm_cell_size': 256,\n",
       " 'lstm_use_prev_action': False,\n",
       " 'lstm_use_prev_reward': False,\n",
       " '_time_major': False,\n",
       " 'use_attention': False,\n",
       " 'attention_num_transformer_units': 1,\n",
       " 'attention_dim': 64,\n",
       " 'attention_num_heads': 1,\n",
       " 'attention_head_dim': 32,\n",
       " 'attention_memory_inference': 50,\n",
       " 'attention_memory_training': 50,\n",
       " 'attention_position_wise_mlp_dim': 32,\n",
       " 'attention_init_gru_gate_bias': 2.0,\n",
       " 'attention_use_n_prev_actions': 0,\n",
       " 'attention_use_n_prev_rewards': 0,\n",
       " 'framestack': True,\n",
       " 'dim': 84,\n",
       " 'grayscale': False,\n",
       " 'zero_mean': True,\n",
       " 'custom_model': None,\n",
       " 'custom_model_config': {},\n",
       " 'custom_action_dist': None,\n",
       " 'custom_preprocessor': None,\n",
       " 'lstm_use_prev_action_reward': -1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rollout (agent, env, n_iter=1, max_steps=5, verbose=False):\n",
    "    \"\"\"\n",
    "    iterate through `n_iter` episodes in a rollout to emulate deployment in a production use case\n",
    "    \"\"\"\n",
    "    for episode in range(n_iter):\n",
    "        state = env.reset()\n",
    "        sum_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            try:\n",
    "                \n",
    "                action = int(agent.compute_single_action(state, explore=False))\n",
    "                print(f\"Compute action = {env.action_space.to_string(action)}\")\n",
    "\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sum_reward += reward\n",
    "                print(f\"Compute reward = {reward}\")\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"reward {:6.3f}  sum {:6.3f}\".format(reward, sum_reward))\n",
    "                    env.render()\n",
    "            except Exception as e:\n",
    "                print(f'----------------------------> Exception = {e}')\n",
    "                break\n",
    "\n",
    "\n",
    "        # report at the end of each episode\n",
    "        print(\"CUMULATIVE REWARD:\", round(sum_reward, 3), \"\\n\")\n",
    "        yield sum_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0705 15:50:25.025571 140184553244224 example_service.py:263] CRITICAL - \n",
      "\n",
      "Working_dir = /dev/shm/compiler_gym_dejang/s/0705T152501-249505-dbfd\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute action = swap_down\n",
      "Compute reward = -0.10795806245708439\n",
      "Compute action = swap_down\n",
      "Compute reward = -1.2608128186439285\n",
      "Compute action = swap_down\n",
      "Compute reward = -0.06702328541776437\n",
      "Compute action = swap_down\n",
      "Compute reward = -0.0016316470605532984\n",
      "Compute action = swap_down\n",
      "Compute reward = 0.006835741497085546\n",
      "CUMULATIVE REWARD: -1.431 \n",
      "\n",
      "average reward: -1.431\n"
     ]
    }
   ],
   "source": [
    "# trainer.restore(trial.checkpoint.value)\n",
    "history = []\n",
    "for episode_reward in run_rollout(trainer, env, verbose=False):\n",
    "    history.append(episode_reward)\n",
    "    \n",
    "print(\"average reward:\", round(sum(history) / len(history), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/agent_timesteps_total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/counters/num_agent_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/counters/num_agent_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/counters/num_env_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/counters/num_env_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/done</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/episode_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/episode_reward_max</td><td>▁▄▄▄▅▅███▇████▇▇▇▇▄▄▄▅▅▅████▇▇▇▇▇▇▅▇▇▇██</td></tr><tr><td>ray/tune/episode_reward_mean</td><td>█▅▂▄▁▃▂▃▁▁▂▁▄▃▂▁▃▁▁▄▁▄▂▂▂▂▁▂▁▃▃▂▂▄▂▁▁▃▁▂</td></tr><tr><td>ray/tune/episode_reward_min</td><td>██▅▅▄▄▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▅▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>ray/tune/episodes_this_iter</td><td>▁█▁▁▁██▁▁▁█▁▁▁▁█▁▁▁██▁▁▁█▁▁▁▁█▁▁▁██▁▁▁██</td></tr><tr><td>ray/tune/episodes_total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/evaluation/episode_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/evaluation/episode_reward_max</td><td>▂▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▃▃▃▃▃▂▃█▃▃▃▂▁▃</td></tr><tr><td>ray/tune/evaluation/episode_reward_mean</td><td>▂▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▃▃▃▃▃▂▃█▃▃▃▂▁▃</td></tr><tr><td>ray/tune/evaluation/episode_reward_min</td><td>▂▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▃▃▃▃▃▂▃█▃▃▃▂▁▃</td></tr><tr><td>ray/tune/evaluation/episodes_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_action_processing_ms</td><td>█▅▃▂▂▂▂▂▂▁▁▁▂▂▂▂▁▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_env_wait_ms</td><td>▁▂▆▆▆█▆▆▇▇▇▇██████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_inference_ms</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_raw_obs_processing_ms</td><td>█▅▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/evaluation/timesteps_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/cur_lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/entropy</td><td>█████████████▇▇██▇▇██▇▆▅▆▆▅▇▂▅▇▃▄▅▅▃▆▆▁▁</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/entropy_coeff</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/kl</td><td>▁▂▄▁▁▂▂▁▁▁▁▁▄▂▁▁▃▂▁█▄▂▃▆▂▃▄▂▁▂▅█▁▄▂▅▂▃▂▃</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/policy_loss</td><td>▆▄▇▂▆▆█▃▇▇█▁▃▇▇▇▃▇▃▆▆▃▆▆▆█▆▄▃▃▆▃▂▄█▂▆▂▁▇</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/total_loss</td><td>▅▃▂█▁▄▃▇▂▄▂▅▄▆▃▃█▂▁▄▂▃▃▇▃▂▃▄▂▃▇▃▁▃▂▁▄▆▁▂</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/vf_explained_var</td><td>▇▇▇▇█▇▇▇▄▇▇▇▇▇▇▇▇▇▅▇▇▇▇▇▇█▇▇▇▇▇▇▄▇▇▆▇▇▆▁</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/vf_loss</td><td>▅▃▁█▁▄▂▇▁▄▁▅▄▆▃▂█▂▁▄▁▂▂▇▂▁▂▄▁▃▇▃▁▂▁▁▄▇▁▁</td></tr><tr><td>ray/tune/info/learner/default_policy/num_agent_steps_trained</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/info/num_agent_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/info/num_agent_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/info/num_env_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/info/num_env_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/iterations_since_restore</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/num_agent_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/num_agent_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/num_env_steps_sampled</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/num_env_steps_sampled_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/num_env_steps_trained</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/num_env_steps_trained_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/num_healthy_workers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/perf/cpu_util_percent</td><td>▁▂▁▃▁▂▁▂▂▂▂▂▂▆▄▃▃▆▆▂▃▁▂▄▆▆▆▆▆▆▆█▅▂▃▂▂▃▁▃</td></tr><tr><td>ray/tune/perf/ram_util_percent</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▄▅▂▂▂▂▃▄▅▆▆▇█▄▅▅▂▂▂▂▂▂▂</td></tr><tr><td>ray/tune/sampler_perf/mean_action_processing_ms</td><td>█▅▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_perf/mean_env_wait_ms</td><td>▁▄▇█▇█▆▆▆▆▇▇▇▇▅▅▄▃▃▃▃▄▄▄▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅</td></tr><tr><td>ray/tune/sampler_perf/mean_inference_ms</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_perf/mean_raw_obs_processing_ms</td><td>▄█▇▆▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_results/episode_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_results/episode_reward_max</td><td>▁▄▄▄▅▅███▇████▇▇▇▇▄▄▄▅▅▅████▇▇▇▇▇▇▅▇▇▇██</td></tr><tr><td>ray/tune/sampler_results/episode_reward_mean</td><td>█▅▂▄▁▃▂▃▁▁▂▁▄▃▂▁▃▁▁▄▁▄▂▂▂▂▁▂▁▃▃▂▂▄▂▁▁▃▁▂</td></tr><tr><td>ray/tune/sampler_results/episode_reward_min</td><td>██▅▅▄▄▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▅▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>ray/tune/sampler_results/episodes_this_iter</td><td>▁█▁▁▁██▁▁▁█▁▁▁▁█▁▁▁██▁▁▁█▁▁▁▁█▁▁▁██▁▁▁██</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_action_processing_ms</td><td>█▅▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_env_wait_ms</td><td>▁▄▇█▇█▆▆▆▆▇▇▇▇▅▅▄▃▃▃▃▄▄▄▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_inference_ms</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_raw_obs_processing_ms</td><td>▄█▇▆▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/time_since_restore</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/time_this_iter_s</td><td>▇▃█▁▇▂█▂▃▂▃▃▁▁▃▃▂▃▃▆▄▇▅▇▃▂▃▂▃▃▂▃▄▂▂▃▇▁▆▃</td></tr><tr><td>ray/tune/timers/learn_throughput</td><td>▁█▇▇▇▇▇▇▇███▇▇▇█▇███▇▇█▇▇██▇██▇███▇▇▇▇██</td></tr><tr><td>ray/tune/timers/learn_time_ms</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/timers/load_throughput</td><td>▇▆▅▄▅▄▃▆▅▇▆▅▁▅▆█▆▄▇▆▄▆▆▅▂▄▅▄▇▇▆▆▆▅▇▅▅▃▇▇</td></tr><tr><td>ray/tune/timers/load_time_ms</td><td>▂▂▃▅▄▅▅▂▃▂▃▃█▃▂▁▃▄▂▂▄▃▂▄▆▅▃▄▁▁▂▂▂▃▂▃▃▅▂▁</td></tr><tr><td>ray/tune/timers/training_iteration_time_ms</td><td>█▆▅▃▄▆▆▅▃▆▆▅▁▃▄▃▅▅▇▃▅▆▅▄▂▄▅▅▄▅▃▆▅▃▄▅▅▃▄▅</td></tr><tr><td>ray/tune/timers/update_time_ms</td><td>▅▄▆▅█▆▅▃▃▃▅▃▄▄▅▅█▂▃▃▇▅▃▆▁▁▂▂▂▂▃▁▁▁▅▃▅▅▄▅</td></tr><tr><td>ray/tune/timesteps_since_restore</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ray/tune/timesteps_total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ray/tune/warmup_time</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>3000</td></tr><tr><td>ray/tune/agent_timesteps_total</td><td>3000.0</td></tr><tr><td>ray/tune/counters/num_agent_steps_sampled</td><td>3000.0</td></tr><tr><td>ray/tune/counters/num_agent_steps_trained</td><td>3000.0</td></tr><tr><td>ray/tune/counters/num_env_steps_sampled</td><td>3000.0</td></tr><tr><td>ray/tune/counters/num_env_steps_trained</td><td>3000.0</td></tr><tr><td>ray/tune/done</td><td>1.0</td></tr><tr><td>ray/tune/episode_len_mean</td><td>3.0</td></tr><tr><td>ray/tune/episode_reward_max</td><td>39.58497</td></tr><tr><td>ray/tune/episode_reward_mean</td><td>0.07105</td></tr><tr><td>ray/tune/episode_reward_min</td><td>-38.49391</td></tr><tr><td>ray/tune/episodes_this_iter</td><td>4.0</td></tr><tr><td>ray/tune/episodes_total</td><td>1000.0</td></tr><tr><td>ray/tune/evaluation/episode_len_mean</td><td>3.0</td></tr><tr><td>ray/tune/evaluation/episode_reward_max</td><td>0.02069</td></tr><tr><td>ray/tune/evaluation/episode_reward_mean</td><td>0.02069</td></tr><tr><td>ray/tune/evaluation/episode_reward_min</td><td>0.02069</td></tr><tr><td>ray/tune/evaluation/episodes_this_iter</td><td>1.0</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_action_processing_ms</td><td>0.14276</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_env_wait_ms</td><td>846.41687</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_inference_ms</td><td>1.41216</td></tr><tr><td>ray/tune/evaluation/sampler_perf/mean_raw_obs_processing_ms</td><td>0.66989</td></tr><tr><td>ray/tune/evaluation/timesteps_this_iter</td><td>3.0</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>0.0</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/cur_lr</td><td>0.0001</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/entropy</td><td>1.36747</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/entropy_coeff</td><td>0.0</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/kl</td><td>1e-05</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/policy_loss</td><td>0.21602</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/total_loss</td><td>4.21781</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/vf_explained_var</td><td>-0.00148</td></tr><tr><td>ray/tune/info/learner/default_policy/learner_stats/vf_loss</td><td>4.00179</td></tr><tr><td>ray/tune/info/learner/default_policy/num_agent_steps_trained</td><td>5.0</td></tr><tr><td>ray/tune/info/num_agent_steps_sampled</td><td>3000.0</td></tr><tr><td>ray/tune/info/num_agent_steps_trained</td><td>3000.0</td></tr><tr><td>ray/tune/info/num_env_steps_sampled</td><td>3000.0</td></tr><tr><td>ray/tune/info/num_env_steps_trained</td><td>3000.0</td></tr><tr><td>ray/tune/iterations_since_restore</td><td>375.0</td></tr><tr><td>ray/tune/num_agent_steps_sampled</td><td>3000.0</td></tr><tr><td>ray/tune/num_agent_steps_trained</td><td>3000.0</td></tr><tr><td>ray/tune/num_env_steps_sampled</td><td>3000.0</td></tr><tr><td>ray/tune/num_env_steps_sampled_this_iter</td><td>8.0</td></tr><tr><td>ray/tune/num_env_steps_trained</td><td>3000.0</td></tr><tr><td>ray/tune/num_env_steps_trained_this_iter</td><td>8.0</td></tr><tr><td>ray/tune/num_healthy_workers</td><td>2.0</td></tr><tr><td>ray/tune/perf/cpu_util_percent</td><td>15.63333</td></tr><tr><td>ray/tune/perf/ram_util_percent</td><td>39.98889</td></tr><tr><td>ray/tune/sampler_perf/mean_action_processing_ms</td><td>0.14085</td></tr><tr><td>ray/tune/sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>ray/tune/sampler_perf/mean_env_wait_ms</td><td>727.49878</td></tr><tr><td>ray/tune/sampler_perf/mean_inference_ms</td><td>1.17696</td></tr><tr><td>ray/tune/sampler_perf/mean_raw_obs_processing_ms</td><td>1.66378</td></tr><tr><td>ray/tune/sampler_results/episode_len_mean</td><td>3.0</td></tr><tr><td>ray/tune/sampler_results/episode_reward_max</td><td>39.58497</td></tr><tr><td>ray/tune/sampler_results/episode_reward_mean</td><td>0.07105</td></tr><tr><td>ray/tune/sampler_results/episode_reward_min</td><td>-38.49391</td></tr><tr><td>ray/tune/sampler_results/episodes_this_iter</td><td>4.0</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_action_processing_ms</td><td>0.14085</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_env_wait_ms</td><td>727.49878</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_inference_ms</td><td>1.17696</td></tr><tr><td>ray/tune/sampler_results/sampler_perf/mean_raw_obs_processing_ms</td><td>1.66378</td></tr><tr><td>ray/tune/time_since_restore</td><td>1476.08435</td></tr><tr><td>ray/tune/time_this_iter_s</td><td>6.10075</td></tr><tr><td>ray/tune/timers/learn_throughput</td><td>170.226</td></tr><tr><td>ray/tune/timers/learn_time_ms</td><td>46.996</td></tr><tr><td>ray/tune/timers/load_throughput</td><td>47067.51562</td></tr><tr><td>ray/tune/timers/load_time_ms</td><td>0.17</td></tr><tr><td>ray/tune/timers/training_iteration_time_ms</td><td>3377.58691</td></tr><tr><td>ray/tune/timers/update_time_ms</td><td>2.238</td></tr><tr><td>ray/tune/timesteps_since_restore</td><td>0.0</td></tr><tr><td>ray/tune/timesteps_total</td><td>3000.0</td></tr><tr><td>ray/tune/warmup_time</td><td>10.31271</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">worthy-gorge-38</strong>: <a href=\"https://wandb.ai/dejang/loop_tool/runs/g7fxgziu\" target=\"_blank\">https://wandb.ai/dejang/loop_tool/runs/g7fxgziu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220705_152453-g7fxgziu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If running in a notebook, finish the wandb run to upload the tensorboard logs to W&B\n",
    "wandb.finish()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e804d18dc74ce1dc9e76e68b7cf0aefb2bc0afdfbb2c1892ec3bac3a66589459"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('compiler_gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
