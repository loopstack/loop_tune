> /home/dejang/loop_tool_env/loop_tool_service/demo_loopnest_mm_td_ir_tensor.py(69)main()
-> agent = q_agents.QAgentTensor(
(Pdb) **************************** 0 ******************************
<<<<< Evaluate Policy >>>>>>>
Available_actions = ['down', 'swap_down']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.04860299080610275
swap_down = 0.09031523764133453
-------------------------------------------
down = 1
swap_down = 0
====================================================================
Reward = 0.0
Available_actions = ['down', 'swap_down', 'swap_up', 'up']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
down = 0.005914375185966492
swap_down = 0.13802479207515717
swap_up = 0.003245206316933036
up = 0.1343853920698166
-------------------------------------------
down = 0
swap_down = 0
swap_up = 1
up = 0
====================================================================
Reward = -0.08010687999999977
Available_actions = ['down', 'swap_down']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.06229643523693085
swap_down = 0.09175612777471542
-------------------------------------------
down = 1
swap_down = 0
====================================================================
Reward = 0.0
Available_actions = ['down', 'swap_down', 'swap_up', 'up']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.011139363050460815
swap_down = 0.12167900055646896
swap_up = 0.009229954332113266
up = 0.13639430701732635
-------------------------------------------
down = 1
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1563, -0.0486,  0.0085,  0.0903]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.04860299080610275
swap_down = 0.09031523764133453
-------------------------------------------
down = 2
swap_down = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.15049728
**************************** 1 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[ 1.3221e-01,  2.2530e-03, -1.3069e-05,  1.3424e-01]],
       grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.0022529661655426025
swap_down = 0.1342390775680542
swap_up = -1.306924968957901e-05
up = 0.13220565021038055
-------------------------------------------
down = 0
swap_down = 0
swap_up = 2
up = 0
====================================================================
Reward = 29.04667648
Reward = 29.04667648

Flops = 31.19717376
**************************** 2 ******************************
Available_actions = ['swap_up', 'up']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[ 0.1444, -0.0331, -0.0063,  0.1056]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = -0.006266207434237003
up = 0.1443583369255066
-------------------------------------------
swap_up = 1
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 31.19717376
**************************** 3 ******************************
Available_actions = ['down', 'swap_down', 'swap_up', 'up']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1619, -0.0626,  0.0034,  0.1018]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.0625840499997139
swap_down = 0.10183258354663849
swap_up = 0.00338589190505445
up = 0.16191668808460236
-------------------------------------------
down = 1
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 31.19717376
**************************** 4 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.046811044216156006
swap_down = 0.09167468547821045
-------------------------------------------
down = 3
swap_down = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
down = 0.009045630693435669
swap_down = 0.141436368227005
swap_up = 0.006400959566235542
up = 0.1362418830394745
-------------------------------------------
down = 0
swap_down = 0
swap_up = 3
up = 0
====================================================================
Reward = -0.08581491200000002
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.05994313955307007
swap_down = 0.09495407342910767
-------------------------------------------
down = 2
swap_down = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.008397921919822693
swap_down = 0.12384231388568878
swap_up = 0.010673392564058304
up = 0.13764652609825134
-------------------------------------------
down = 2
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Explore <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_down

_____________________________Actions from network: tensor([[ 0.1574, -0.0468,  0.0096,  0.0917]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.046811044216156006
swap_down = 0.09167468547821045
-------------------------------------------
down = 3
swap_down = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.150960384
**************************** 5 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1371, 0.0103, 0.0077, 0.1428]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.010269537568092346
swap_down = 0.14283330738544464
swap_up = 0.0077478522434830666
up = 0.13711592555046082
-------------------------------------------
down = 0
swap_down = 0
swap_up = 4
up = 0
====================================================================
Reward = 35.034563328
Reward = 35.034563328

Flops = 37.185523712
**************************** 6 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[ 0.1487, -0.0253, -0.0010,  0.1129]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = -0.001024570781737566
up = 0.14872020483016968
-------------------------------------------
swap_up = 2
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.185523712
**************************** 7 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1661, -0.0558,  0.0100,  0.1097]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.055799052119255066
swap_down = 0.10970716923475266
swap_up = 0.009952617809176445
up = 0.16606096923351288
-------------------------------------------
down = 2
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.185523712
**************************** 8 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.04135790467262268
swap_down = 0.09684007614850998
-------------------------------------------
down = 4
swap_down = 1
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
down = 0.015564769506454468
swap_down = 0.14908269047737122
swap_up = 0.013642054982483387
up = 0.141250878572464
-------------------------------------------
down = 0
swap_down = 0
swap_up = 5
up = 0
====================================================================
Reward = -0.080248192
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.05381688475608826
swap_down = 0.10325364023447037
-------------------------------------------
down = 3
swap_down = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.0017930269241333008
swap_down = 0.1303955465555191
swap_up = 0.01580747961997986
up = 0.14143233001232147
-------------------------------------------
down = 3
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Explore <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_down

_____________________________Actions from network: tensor([[ 0.1609, -0.0414,  0.0138,  0.0968]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.04135790467262268
swap_down = 0.09684007614850998
-------------------------------------------
down = 4
swap_down = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.15119104
**************************** 9 ******************************
Explore <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1423, 0.0169, 0.0151, 0.1506]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.016863584518432617
swap_down = 0.15063124895095825
swap_up = 0.015116652473807335
up = 0.14228513836860657
-------------------------------------------
down = 1
swap_down = 0
swap_up = 5
up = 0
====================================================================
Reward = 35.359575552
Reward = 35.359575552

Flops = 37.510766592
**************************** 10 ******************************
Explore <<<<<<<<<<<<<<<<<<<<<
Chosen Action = up

_____________________________Actions from network: tensor([[ 0.1531, -0.0173,  0.0043,  0.1201]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.004304246045649052
up = 0.153109610080719
-------------------------------------------
swap_up = 2
up = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.510766592
**************************** 11 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1705, -0.0492,  0.0166,  0.1175]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.04916941374540329
swap_down = 0.11752195656299591
swap_up = 0.016637148335576057
up = 0.17045941948890686
-------------------------------------------
down = 3
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.510766592
**************************** 12 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.036198392510414124
swap_down = 0.10193890333175659
-------------------------------------------
down = 5
swap_down = 2
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
down = 0.022029191255569458
swap_down = 0.15691187977790833
swap_up = 0.02105094864964485
up = 0.14657631516456604
-------------------------------------------
down = 1
swap_down = 0
swap_up = 6
up = 0
====================================================================
Reward = -0.083930112
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.047692038118839264
swap_down = 0.11135102808475494
-------------------------------------------
down = 4
swap_down = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.004802137613296509
swap_down = 0.13704745471477509
swap_up = 0.02112765982747078
up = 0.1453733742237091
-------------------------------------------
down = 4
swap_down = 0
swap_up = 0
up = 0
for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  <<<<<< cursor (line 0 )
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  <<<<<< cursor (line 0 )
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  <<<<<< cursor (line 0 )
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  <<<<<< cursor (line 0 )
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1646, -0.0362,  0.0181,  0.1019]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.036198392510414124
swap_down = 0.10193890333175659
-------------------------------------------
down = 6
swap_down = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.149650688
**************************** 13 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1477, 0.0234, 0.0226, 0.1585]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.02336108684539795
swap_down = 0.15853844583034515
swap_up = 0.022565726190805435
up = 0.147720605134964
-------------------------------------------
down = 1
swap_down = 0
swap_up = 7
up = 0
====================================================================
Reward = 35.314544384
Reward = 35.314544384

Flops = 37.464195072
**************************** 14 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[ 0.1575, -0.0088,  0.0099,  0.1272]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.009860006161034107
up = 0.15745386481285095
-------------------------------------------
swap_up = 3
up = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.464195072
**************************** 15 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1751, -0.0426,  0.0233,  0.1253]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.04264071583747864
swap_down = 0.1253281831741333
swap_up = 0.023342033848166466
up = 0.17507794499397278
-------------------------------------------
down = 4
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.464195072
**************************** 16 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.03144785761833191
swap_down = 0.1071430966258049
-------------------------------------------
down = 7
swap_down = 2
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
down = 0.028557702898979187
swap_down = 0.1649007350206375
swap_up = 0.028429822996258736
up = 0.1522517055273056
-------------------------------------------
down = 1
swap_down = 0
swap_up = 8
up = 0
====================================================================
Reward = -0.08202444799999986
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.041348256170749664
swap_down = 0.11872547119855881
-------------------------------------------
down = 5
swap_down = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.011498019099235535
swap_down = 0.1437350958585739
swap_up = 0.026499880477786064
up = 0.14945277571678162
-------------------------------------------
down = 5
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1686, -0.0314,  0.0227,  0.1071]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.03144785761833191
swap_down = 0.1071430966258049
-------------------------------------------
down = 8
swap_down = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.153813504
**************************** 17 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1534, 0.0299, 0.0298, 0.1664]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.029867395758628845
swap_down = 0.1664101630449295
swap_up = 0.029828809201717377
up = 0.1534361094236374
-------------------------------------------
down = 1
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.170691584000004
Reward = 35.170691584000004

Flops = 37.324505088
**************************** 18 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[ 0.1612, -0.0005,  0.0163,  0.1345]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.016305256634950638
up = 0.16121956706047058
-------------------------------------------
swap_up = 4
up = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.324505088
**************************** 19 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1799, -0.0361,  0.0301,  0.1332]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.036083340644836426
swap_down = 0.13321691751480103
swap_up = 0.030061952769756317
up = 0.17992042005062103
-------------------------------------------
down = 5
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.324505088
**************************** 20 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.02648099511861801
swap_down = 0.11293916404247284
-------------------------------------------
down = 9
swap_down = 2
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.03535743057727814
swap_down = 0.17324364185333252
swap_up = 0.03588852286338806
up = 0.15798470377922058
-------------------------------------------
down = 2
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Available_actions = ['swap_up', 'up']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.027130279690027237
up = 0.14825350046157837
-------------------------------------------
swap_up = 1
up = 0
====================================================================
Reward = 35.133989888
Available_actions = ['down', 'swap_down', 'swap_up', 'up']
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.0658324807882309
swap_down = 0.09995798766613007
swap_up = 0.008766891434788704
up = 0.19136878848075867
-------------------------------------------
down = 1
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1728, -0.0265,  0.0279,  0.1129]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.02648099511861801
swap_down = 0.11293916404247284
-------------------------------------------
down = 10
swap_down = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.145810432
**************************** 21 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1592, 0.0368, 0.0375, 0.1750]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.03679901361465454
swap_down = 0.1750103384256363
swap_up = 0.037455372512340546
up = 0.15919288992881775
-------------------------------------------
down = 3
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 34.879522816000005
Reward = 34.879522816000005

Flops = 37.025333248
**************************** 22 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1655, 0.0075, 0.0239, 0.1421]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.023893941193819046
up = 0.16545319557189941
-------------------------------------------
swap_up = 5
up = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.025333248
**************************** 23 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1850, -0.0294,  0.0368,  0.1412]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.029443785548210144
swap_down = 0.14121520519256592
swap_up = 0.036844320595264435
up = 0.1849612146615982
-------------------------------------------
down = 6
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.025333248
**************************** 24 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.021474242210388184
swap_down = 0.11885988712310791
-------------------------------------------
down = 11
swap_down = 2
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.04236076772212982
swap_down = 0.1817990094423294
swap_up = 0.04341857135295868
up = 0.1639733910560608
-------------------------------------------
down = 4
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.035721439868211746
up = 0.15455269813537598
-------------------------------------------
swap_up = 2
up = 0
====================================================================
Reward = 35.225841664
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.057655274868011475
swap_down = 0.10792297124862671
swap_up = 0.0166618712246418
up = 0.19521285593509674
-------------------------------------------
down = 2
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Explore <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_down

_____________________________Actions from network: tensor([[ 0.1771, -0.0215,  0.0334,  0.1189]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.021474242210388184
swap_down = 0.11885988712310791
-------------------------------------------
down = 11
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.145887232
**************************** 25 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1653, 0.0438, 0.0450, 0.1836]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.043841853737831116
swap_down = 0.18361110985279083
swap_up = 0.045010555535554886
up = 0.16525527834892273
-------------------------------------------
down = 5
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.108998144
Reward = 35.108998144

Flops = 37.254885376
**************************** 26 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1697, 0.0161, 0.0319, 0.1497]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.03187552094459534
up = 0.16968348622322083
-------------------------------------------
swap_up = 6
up = 1
for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  <<<<<< cursor (line 0 )
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for n_5625 in 128 : L0  
 for m_5586 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.254885376
**************************** 27 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1901, -0.0227,  0.0438,  0.1493]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.022695839405059814
swap_down = 0.14932304620742798
swap_up = 0.04378538206219673
up = 0.1901438981294632
-------------------------------------------
down = 7
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.254885376
**************************** 28 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.016465336084365845
swap_down = 0.12471415102481842
-------------------------------------------
down = 12
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.04959970712661743
swap_down = 0.19058223068714142
swap_up = 0.051207415759563446
up = 0.17014089226722717
-------------------------------------------
down = 6
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.04459216445684433
up = 0.16119396686553955
-------------------------------------------
swap_up = 3
up = 0
====================================================================
Reward = 35.106387456
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.04959878325462341
swap_down = 0.11599429696798325
swap_up = 0.024494227021932602
up = 0.19935902953147888
-------------------------------------------
down = 3
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1816, -0.0165,  0.0388,  0.1247]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.016465336084365845
swap_down = 0.12471415102481842
-------------------------------------------
down = 13
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.153351168
**************************** 29 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1714, 0.0511, 0.0529, 0.1924]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.05113154649734497
swap_down = 0.19244132936000824
swap_up = 0.052863892167806625
up = 0.17144756019115448
-------------------------------------------
down = 7
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 34.940422144
Reward = 34.940422144

Flops = 37.093773312
**************************** 30 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1740, 0.0252, 0.0402, 0.1574]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.04018944501876831
up = 0.17400121688842773
-------------------------------------------
swap_up = 7
up = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.093773312
**************************** 31 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1954, -0.0158,  0.0509,  0.1576]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.0158090740442276
swap_down = 0.15756826102733612
swap_up = 0.05094382166862488
up = 0.1954367607831955
-------------------------------------------
down = 8
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.093773312
**************************** 32 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.011430561542510986
swap_down = 0.13054125010967255
-------------------------------------------
down = 14
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.05710229277610779
swap_down = 0.1996219903230667
swap_up = 0.0592731349170208
up = 0.17645585536956787
-------------------------------------------
down = 8
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.05374943092465401
up = 0.16814929246902466
-------------------------------------------
swap_up = 4
up = 0
====================================================================
Reward = 35.219310848
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.04189436882734299
swap_down = 0.12440438568592072
swap_up = 0.032205335795879364
up = 0.2039378136396408
-------------------------------------------
down = 4
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1861, -0.0114,  0.0442,  0.1305]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.011430561542510986
swap_down = 0.13054125010967255
-------------------------------------------
down = 15
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.146576896
**************************** 33 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1778, 0.0587, 0.0610, 0.2015]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.05868682265281677
swap_down = 0.201532781124115
swap_up = 0.060983188450336456
up = 0.17779234051704407
-------------------------------------------
down = 9
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.177928192
Reward = 35.177928192

Flops = 37.324505088
**************************** 34 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1789, 0.0339, 0.0485, 0.1659]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.04848138615489006
up = 0.17890730500221252
-------------------------------------------
swap_up = 8
up = 1
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.324505088
**************************** 35 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.2009, -0.0088,  0.0583,  0.1660]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.008779391646385193
swap_down = 0.1659618616104126
swap_up = 0.05829503759741783
up = 0.20086844265460968
-------------------------------------------
down = 9
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.324505088
**************************** 36 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.0063593387603759766
swap_down = 0.13636547327041626
-------------------------------------------
down = 16
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.06488768756389618
swap_down = 0.2089478075504303
swap_up = 0.06758484244346619
up = 0.18294283747673035
-------------------------------------------
down = 10
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.0632021576166153
up = 0.17540989816188812
-------------------------------------------
swap_up = 5
up = 0
====================================================================
Reward = 35.110300544
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.034068211913108826
swap_down = 0.13287094235420227
swap_up = 0.04000302404165268
up = 0.2087016999721527
-------------------------------------------
down = 5
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1907, -0.0064,  0.0497,  0.1364]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.0063593387603759766
swap_down = 0.13636547327041626
-------------------------------------------
down = 17
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.148189696
**************************** 37 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1843, 0.0665, 0.0693, 0.2109]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.06652814149856567
swap_down = 0.21091538667678833
swap_up = 0.06934139877557755
up = 0.18431337177753448
-------------------------------------------
down = 11
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.153078783999995
Reward = 35.153078783999995

Flops = 37.30126848
**************************** 38 ******************************
Explore <<<<<<<<<<<<<<<<<<<<<
Chosen Action = up

_____________________________Actions from network: tensor([[0.1846, 0.0415, 0.0563, 0.1747]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.056316077709198
up = 0.18459630012512207
-------------------------------------------
swap_up = 8
up = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.30126848
**************************** 39 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.2065, -0.0016,  0.0658,  0.1745]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.0016075074672698975
swap_down = 0.17450843751430511
swap_up = 0.06581869721412659
up = 0.2064589262008667
-------------------------------------------
down = 10
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.30126848
**************************** 40 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.0012458860874176025
swap_down = 0.1422019600868225
-------------------------------------------
down = 18
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.07297758758068085
swap_down = 0.21858873963356018
swap_up = 0.07612299174070358
up = 0.18962320685386658
-------------------------------------------
down = 12
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.07295949012041092
up = 0.18297328054904938
-------------------------------------------
swap_up = 6
up = 0
====================================================================
for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

Reward = 35.226990592
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.026111513376235962
swap_down = 0.14141973853111267
swap_up = 0.04790927469730377
up = 0.2136535942554474
-------------------------------------------
down = 6
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[ 0.1953, -0.0012,  0.0551,  0.1422]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = -0.0012458860874176025
swap_down = 0.1422019600868225
-------------------------------------------
down = 19
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.14167744
**************************** 41 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1910, 0.0747, 0.0779, 0.2206]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.07467789947986603
swap_down = 0.2206185758113861
swap_up = 0.07792160660028458
up = 0.191031813621521
-------------------------------------------
down = 13
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.136055424
Reward = 35.136055424

Flops = 37.277732864
**************************** 42 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1907, 0.0487, 0.0640, 0.1836]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.06400193274021149
up = 0.19066038727760315
-------------------------------------------
swap_up = 9
up = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.277732864
**************************** 43 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.2122, 0.0057, 0.0735, 0.1832]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.005704954266548157
swap_down = 0.18321041762828827
swap_up = 0.07349935173988342
up = 0.21222451329231262
-------------------------------------------
down = 11
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.277732864
**************************** 44 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.003912374377250671
swap_down = 0.14805957674980164
-------------------------------------------
down = 20
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.08139492571353912
swap_down = 0.22857356071472168
swap_up = 0.08487752079963684
up = 0.1965174376964569
-------------------------------------------
down = 14
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.08303504437208176
up = 0.1908666044473648
-------------------------------------------
swap_up = 7
up = 0
====================================================================
Reward = 35.154666240000005
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.018020570278167725
swap_down = 0.15006649494171143
swap_up = 0.055937863886356354
up = 0.2187952995300293
-------------------------------------------
down = 7
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.2001, 0.0039, 0.0606, 0.1481]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.003912374377250671
swap_down = 0.14805957674980164
-------------------------------------------
down = 21
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.14742208
**************************** 45 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.1980, 0.0832, 0.0867, 0.2307]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.08315950632095337
swap_down = 0.230671226978302
swap_up = 0.08671589940786362
up = 0.19796815514564514
-------------------------------------------
down = 15
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.107463296
Reward = 35.107463296

Flops = 37.254885376
**************************** 46 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

_____________________________Actions from network: tensor([[0.1969, 0.0562, 0.0719, 0.1928]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
swap_up = 0.07187170535326004
up = 0.19688624143600464
-------------------------------------------
swap_up = 10
up = 2
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.254885376
**************************** 47 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.2182, 0.0132, 0.0813, 0.1921]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.013156771659851074
swap_down = 0.19206993281841278
swap_up = 0.08132626116275787
up = 0.21817976236343384
-------------------------------------------
down = 12
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 37.254885376
**************************** 48 ******************************
<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  <<<<<< cursor (line 0 )
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  <<<<<< cursor (line 1 )
  for k_5587 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for n_5625 in 128 : L1  
  for k_5587 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  <<<<<< cursor (line 1 )
  for n_5625 in 128 : L2  
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

for m_5586 in 128 : L0  
 for k_5587 in 128 : L1  
  for n_5625 in 128 : L2  <<<<<< cursor (line 2 )
   %2[m_5586, k_5587, n_5625] <- multiply(%0, %1)  
   %3[m_5586, n_5625] <- add(%2)  
 for n_5625 in 128 : L5  
  %4[m_5586, n_5625] <- write(%3)  

====================================================================
====================================================================
Action Preference:
down = 0.00913006067276001
swap_down = 0.15394951403141022
-------------------------------------------
down = 22
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.0901627242565155
swap_down = 0.23892973363399506
swap_up = 0.09384702146053314
up = 0.203646719455719
-------------------------------------------
down = 16
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.0934324860572815
up = 0.19909651577472687
-------------------------------------------
swap_up = 8
up = 0
====================================================================
Reward = 35.080621056
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.00957992672920227
swap_down = 0.15884359180927277
swap_up = 0.06434152275323868
up = 0.22432130575180054
-------------------------------------------
down = 8
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################

^^^^ New Epoch ^^^^^^

Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.2049, 0.0091, 0.0662, 0.1539]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.00913006067276001
swap_down = 0.15394951403141022
-------------------------------------------
down = 23
swap_down = 3
====================================================================
Reward = 0.0
Reward = 0.0

Flops = 2.150112512
**************************** 49 ******************************
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

_____________________________Actions from network: tensor([[0.2051, 0.0920, 0.0957, 0.2411]], grad_fn=<AddmmBackward0>)
====================================================================
====================================================================
Action Preference:
down = 0.0919964611530304
swap_down = 0.24110063910484314
swap_up = 0.09572435915470123
up = 0.20514333248138428
-------------------------------------------
down = 17
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 35.45482112
Reward = 35.45482112

Flops = 37.604933632
====================================================================
============================ END ==================================

============================ TESTING ==================================

<<<<< Evaluate Policy >>>>>>>
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.011748015880584717
swap_down = 0.15683121979236603
-------------------------------------------
down = 24
swap_down = 3
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = 0.09464919567108154
swap_down = 0.24421769380569458
swap_up = 0.09837798774242401
up = 0.20727939903736115
-------------------------------------------
down = 18
swap_down = 0
swap_up = 9
up = 0
====================================================================
Reward = 0.0
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = swap_up

====================================================================
====================================================================
Action Preference:
swap_up = 0.09870633482933044
up = 0.2032928317785263
-------------------------------------------
swap_up = 9
up = 0
====================================================================
Reward = 34.228284927999994
Policy <<<<<<<<<<<<<<<<<<<<<
Chosen Action = down

====================================================================
====================================================================
Action Preference:
down = -0.005280852317810059
swap_down = 0.16319359838962555
swap_up = 0.06864979863166809
up = 0.22722089290618896
-------------------------------------------
down = 9
swap_down = 0
swap_up = 0
up = 0
====================================================================
Reward = 0.0
#################################################
Current best policy:
#################################################
